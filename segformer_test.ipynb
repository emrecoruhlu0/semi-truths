{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e2cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emrec\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Cihaz: cuda\n",
      "ğŸ§  Ekran KartÄ±: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "======================================================================\n",
      "âš™ï¸ YAPILANDIRMA KONTROLÃœ\n",
      "======================================================================\n",
      "âœ… Dengeleme Modu: fixed_number\n",
      "   ğŸ“Š Hedef: Her magnitude sÄ±nÄ±fÄ±ndan 50 gÃ¶rsel\n",
      "   ğŸ“‚ Veri dizini: C:\\AI_DATA\\SEMI_TRUTHS_extracted\n",
      "   ğŸ’¾ SonuÃ§larÄ± kaydet: True\n",
      "   ğŸ“Š GÃ¶rselleÅŸtirme: True\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# GPU KontrolÃ¼\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ”¥ Cihaz: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"ğŸ§  Ekran KartÄ±: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# GELIÅMIÅ AYARLAR (EÄŸitim filtreleme ve dengeli test seti iÃ§in)\n",
    "CONFIG = {\n",
    "    # Temel Ayarlar\n",
    "    'root_dir': r\"C:\\AI_DATA\\SEMI_TRUTHS_extracted\",\n",
    "    'model_path': \"./segformer_b4_stable/final_best_model\",\n",
    "    'batch_size': 16,\n",
    "    \n",
    "    # YENI: EÄŸitim verileri (filtreleme iÃ§in)\n",
    "    'train_csv_path': r\"segformer_b4_stable\\segmentation_dataset_balanced.csv\",\n",
    "    \n",
    "    # YENI: Test CSV dosyalarÄ± (tÃ¼mÃ¼ kullanÄ±lacak)\n",
    "    'test_csv_paths': [\n",
    "        r\"dataset_splits\\fake_only_split\\fake_test.csv\",\n",
    "        r\"dataset_splits\\fake_only_split\\fake_train.csv\",\n",
    "        r\"dataset_splits\\fake_only_split\\fake_val.csv\"\n",
    "    ],\n",
    "    \n",
    "    # YENI: Dengeleme Modu\n",
    "    'balancing_mode': 'fixed_number',  # SeÃ§enekler: 'fixed_number' veya 'percentage'\n",
    "    'samples_per_magnitude_class': 50,  # balancing_mode='fixed_number' iÃ§in\n",
    "    'percentage_to_keep': 70,  # balancing_mode='percentage' iÃ§in (0-100 arasÄ±)\n",
    "    \n",
    "    # YENI: Yol DÃ¶nÃ¼ÅŸtÃ¼rme AyarlarÄ±\n",
    "    'training_path_prefix': r\"C:\\Users\\DeepLab\\Desktop\\Grup-17\\SEMI_TRUTHS-extracted\",\n",
    "    'test_path_prefix': r\"C:\\AI_DATA\\SEMI_TRUTHS_extracted\",\n",
    "    \n",
    "    # YENI: CSV Export & Visualization Settings\n",
    "    'save_results': True,  # Enable/disable CSV export\n",
    "    'output_dir': 'segformer_test_results',  # Base output directory\n",
    "    'generate_visualizations': True,  # Enable/disable plots\n",
    "}\n",
    "\n",
    "# YapÄ±landÄ±rma DoÄŸrulama\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš™ï¸ YAPILANDIRMA KONTROLÃœ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if CONFIG['balancing_mode'] not in ['fixed_number', 'percentage']:\n",
    "    raise ValueError(\"âŒ balancing_mode 'fixed_number' veya 'percentage' olmalÄ±!\")\n",
    "\n",
    "if CONFIG['balancing_mode'] == 'fixed_number' and CONFIG['samples_per_magnitude_class'] <= 0:\n",
    "    raise ValueError(\"âŒ samples_per_magnitude_class pozitif bir sayÄ± olmalÄ±!\")\n",
    "\n",
    "if CONFIG['balancing_mode'] == 'percentage' and not (0 < CONFIG['percentage_to_keep'] <= 100):\n",
    "    raise ValueError(\"âŒ percentage_to_keep 0-100 arasÄ±nda olmalÄ±!\")\n",
    "\n",
    "print(f\"âœ… Dengeleme Modu: {CONFIG['balancing_mode']}\")\n",
    "if CONFIG['balancing_mode'] == 'fixed_number':\n",
    "    print(f\"   ğŸ“Š Hedef: Her magnitude sÄ±nÄ±fÄ±ndan {CONFIG['samples_per_magnitude_class']:,} gÃ¶rsel\")\n",
    "else:\n",
    "    print(f\"   ğŸ“Š Hedef: Kalan verilerin %{CONFIG['percentage_to_keep']}'i\")\n",
    "print(f\"   ğŸ“‚ Veri dizini: {CONFIG['root_dir']}\")\n",
    "print(f\"   ğŸ’¾ SonuÃ§larÄ± kaydet: {CONFIG['save_results']}\")\n",
    "print(f\"   ğŸ“Š GÃ¶rselleÅŸtirme: {CONFIG['generate_visualizations']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab64e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ GELÄ°ÅMÄ°Å TEST VERÄ°SÄ° HAZIRLAMA (FÄ°LTRELEME & DENGELEME)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ FAZ 1: Test CSV dosyalarÄ± yÃ¼kleniyor ve birleÅŸtiriliyor...\n",
      "   âœ“ YÃ¼klendi: fake_test.csv (16,216 Ã¶rnek)\n",
      "   âœ“ YÃ¼klendi: fake_train.csv (75,154 Ã¶rnek)\n",
      "   âœ“ YÃ¼klendi: fake_val.csv (16,291 Ã¶rnek)\n",
      "\n",
      "âœ… BirleÅŸtirilmiÅŸ test verisi: 107,661 Ã¶rnek\n",
      "\n",
      "ğŸ“‚ FAZ 2: EÄŸitim verileri yÃ¼kleniyor (filtreleme iÃ§in)...\n",
      "   âœ“ EÄŸitim CSV yÃ¼klendi: 60,000 Ã¶rnek\n",
      "   ğŸ”„ Yol dÃ¶nÃ¼ÅŸÃ¼m Ã¶rneÄŸi:\n",
      "      EÄŸitim: C:\\Users\\DeepLab\\Desktop\\Grup-17\\SEMI_TRUTHS-extracted\\inpainting\\CelebAHQ\\StableDiffusion_v5\\11709_nose_CelebAHQ_StableDiffusion_v5.png\n",
      "      DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ: C:\\AI_DATA\\SEMI_TRUTHS_extracted\\inpainting\\CelebAHQ\\StableDiffusion_v5\\11709_nose_CelebAHQ_StableDiffusion_v5.png\n",
      "      Dosya adÄ±: 11709_nose_CelebAHQ_StableDiffusion_v5.png\n",
      "   ğŸ”„ Yol dÃ¶nÃ¼ÅŸÃ¼m Ã¶rneÄŸi:\n",
      "      EÄŸitim: C:\\Users\\DeepLab\\Desktop\\Grup-17\\SEMI_TRUTHS-extracted\\inpainting\\CityScapes\\Kandinsky_2_2\\hamburg_000000_066988_instance038_CityScapes_Kandinsky_2_2.png\n",
      "      DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ: C:\\AI_DATA\\SEMI_TRUTHS_extracted\\inpainting\\CityScapes\\Kandinsky_2_2\\hamburg_000000_066988_instance038_CityScapes_Kandinsky_2_2.png\n",
      "      Dosya adÄ±: hamburg_000000_066988_instance038_CityScapes_Kandinsky_2_2.png\n",
      "   ğŸ”„ Yol dÃ¶nÃ¼ÅŸÃ¼m Ã¶rneÄŸi:\n",
      "      EÄŸitim: C:\\Users\\DeepLab\\Desktop\\Grup-17\\SEMI_TRUTHS-extracted\\inpainting\\CityScapes\\Kandinsky_2_2\\bremen_000102_000019_instance028_CityScapes_Kandinsky_2_2.png\n",
      "      DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ: C:\\AI_DATA\\SEMI_TRUTHS_extracted\\inpainting\\CityScapes\\Kandinsky_2_2\\bremen_000102_000019_instance028_CityScapes_Kandinsky_2_2.png\n",
      "      Dosya adÄ±: bremen_000102_000019_instance028_CityScapes_Kandinsky_2_2.png\n",
      "\n",
      "âœ… EÄŸitim dÄ±ÅŸlama seti: 60,000 benzersiz dosya adÄ±\n",
      "\n",
      "ğŸ” FAZ 3: EÄŸitim gÃ¶rselleri filtreleniyor...\n",
      "   âœ— Ã‡Ä±karÄ±ldÄ±: 3,598 eÄŸitim gÃ¶rseli\n",
      "   âœ“ Kalan: 104,063 test iÃ§in Ã¶rnek\n",
      "\n",
      "ğŸ“Š FAZ 4: sem_magnitude daÄŸÄ±lÄ±mÄ± analiz ediliyor...\n",
      "\n",
      "   Dengeleme Ã–NCESÄ° daÄŸÄ±lÄ±m:\n",
      "   â€¢ small   : 34,172 Ã¶rnek (32.84%)\n",
      "   â€¢ medium  : 34,158 Ã¶rnek (32.82%)\n",
      "   â€¢ large   : 35,733 Ã¶rnek (34.34%)\n",
      "\n",
      "âš–ï¸  FAZ 5: SÄ±nÄ±flar dengeleniyor (mod: fixed_number)...\n",
      "   Hedef: Her sÄ±nÄ±ftan 50 Ã¶rnek\n",
      "\n",
      "   âœ“ small   : 34,172 mevcut Ã¶rnekten 50 seÃ§ildi\n",
      "   âœ“ medium  : 34,158 mevcut Ã¶rnekten 50 seÃ§ildi\n",
      "   âœ“ large   : 35,733 mevcut Ã¶rnekten 50 seÃ§ildi\n",
      "\n",
      "ğŸ¯ FAZ 6: Son test veri seti oluÅŸturuluyor...\n",
      "\n",
      "âœ… SON TEST VERÄ° SETÄ°: 150 Ã¶rnek\n",
      "\n",
      "   Dengeleme SONRASI daÄŸÄ±lÄ±m:\n",
      "   â€¢ small   :     50 Ã¶rnek (33.33%)\n",
      "   â€¢ medium  :     50 Ã¶rnek (33.33%)\n",
      "   â€¢ large   :     50 Ã¶rnek (33.33%)\n",
      "\n",
      "ğŸ—‚ï¸  FAZ 7: Maske veritabanÄ± oluÅŸturuluyor...\n",
      "   âœ“ Maske veritabanÄ±: 158,750 maske indekslendi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95460483aa34075b3799e33b5dc553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Maskeler eÅŸleÅŸtiriliyor:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš ï¸  Eksik maskeler: 21, bu Ã¶rnekler Ã§Ä±karÄ±lÄ±yor...\n",
      "   âœ“ Maske doÄŸrulamasÄ±ndan sonra son sayÄ±: 129 Ã¶rnek\n",
      "\n",
      "ğŸ” FAZ 8: Yollar doÄŸrulanÄ±yor...\n",
      "   âœ“ Ã–rnek doÄŸrulama: 5/5 baÅŸarÄ±lÄ±\n",
      "\n",
      "======================================================================\n",
      "âœ… TEST VERÄ°SÄ° HAZIRLAMA TAMAMLANDI!\n",
      "======================================================================\n",
      "ğŸ“Š 129 dengeli Ã¶rnek ile test iÃ§in hazÄ±r\n",
      "   Test fonksiyonunu Ã§alÄ±ÅŸtÄ±rmak iÃ§in HÃ¼cre 4'Ã¼ Ã§alÄ±ÅŸtÄ±rabilirsiniz.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸš€ GELÄ°ÅMÄ°Å TEST VERÄ°SÄ° HAZIRLAMA (FÄ°LTRELEME & DENGELEME)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 1: TEST CSV DOSYALARINI YÃœKLE VE BÄ°RLEÅTÄ°R\n",
    "# ==============================================================================\n",
    "print(\"\\nğŸ“‚ FAZ 1: Test CSV dosyalarÄ± yÃ¼kleniyor ve birleÅŸtiriliyor...\")\n",
    "\n",
    "combined_test_dfs = []\n",
    "total_original_samples = 0\n",
    "\n",
    "for csv_path in CONFIG['test_csv_paths']:\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"âš ï¸  UyarÄ±: {csv_path} bulunamadÄ±, atlanÄ±yor...\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    combined_test_dfs.append(df)\n",
    "    total_original_samples += len(df)\n",
    "    print(f\"   âœ“ YÃ¼klendi: {os.path.basename(csv_path)} ({len(df):,} Ã¶rnek)\")\n",
    "\n",
    "if not combined_test_dfs:\n",
    "    raise FileNotFoundError(\"âŒ Test CSV dosyasÄ± bulunamadÄ±!\")\n",
    "\n",
    "df_combined = pd.concat(combined_test_dfs, ignore_index=True)\n",
    "print(f\"\\nâœ… BirleÅŸtirilmiÅŸ test verisi: {len(df_combined):,} Ã¶rnek\")\n",
    "\n",
    "# Gerekli sÃ¼tunlarÄ± kontrol et\n",
    "required_cols = ['perturbed_img_id', 'fake_img_path', 'sem_magnitude', 'mask_id']\n",
    "missing_cols = [col for col in required_cols if col not in df_combined.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"âŒ Eksik sÃ¼tunlar: {missing_cols}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 2: EÄÄ°TÄ°M VERÄ°LERÄ°NÄ° YÃœKLE VE DIÅLAMA SETÄ° OLUÅTUR\n",
    "# ==============================================================================\n",
    "print(\"\\nğŸ“‚ FAZ 2: EÄŸitim verileri yÃ¼kleniyor (filtreleme iÃ§in)...\")\n",
    "\n",
    "if not os.path.exists(CONFIG['train_csv_path']):\n",
    "    raise FileNotFoundError(f\"âŒ EÄŸitim CSV bulunamadÄ±: {CONFIG['train_csv_path']}\")\n",
    "\n",
    "df_train = pd.read_csv(CONFIG['train_csv_path'])\n",
    "print(f\"   âœ“ EÄŸitim CSV yÃ¼klendi: {len(df_train):,} Ã¶rnek\")\n",
    "\n",
    "# EÄŸitim yollarÄ±nÄ± dÃ¶nÃ¼ÅŸtÃ¼r ve dosya adlarÄ±nÄ± Ã§Ä±kar\n",
    "training_filenames = set()\n",
    "path_conversions_checked = 0\n",
    "\n",
    "for train_path in df_train['image_path']:\n",
    "    # Yol dÃ¶nÃ¼ÅŸÃ¼mÃ¼: eÄŸitim prefix'ini test prefix'i ile deÄŸiÅŸtir\n",
    "    converted_path = str(train_path).replace(\n",
    "        CONFIG['training_path_prefix'], \n",
    "        CONFIG['test_path_prefix']\n",
    "    )\n",
    "    \n",
    "    # Sadece dosya adÄ±nÄ± al (Ã¶rn: \"image.png\")\n",
    "    filename = os.path.basename(converted_path)\n",
    "    training_filenames.add(filename)\n",
    "    \n",
    "    # Ä°lk birkaÃ§ dÃ¶nÃ¼ÅŸÃ¼mÃ¼ gÃ¶ster (debug iÃ§in)\n",
    "    if path_conversions_checked < 3:\n",
    "        print(f\"   ğŸ”„ Yol dÃ¶nÃ¼ÅŸÃ¼m Ã¶rneÄŸi:\")\n",
    "        print(f\"      EÄŸitim: {train_path}\")\n",
    "        print(f\"      DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ: {converted_path}\")\n",
    "        print(f\"      Dosya adÄ±: {filename}\")\n",
    "        path_conversions_checked += 1\n",
    "\n",
    "print(f\"\\nâœ… EÄŸitim dÄ±ÅŸlama seti: {len(training_filenames):,} benzersiz dosya adÄ±\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 3: EÄÄ°TÄ°M GÃ–RSELLERÄ°NÄ° FÄ°LTRELE\n",
    "# ==============================================================================\n",
    "print(\"\\nğŸ” FAZ 3: EÄŸitim gÃ¶rselleri filtreleniyor...\")\n",
    "\n",
    "# Test verisinden dosya adlarÄ±nÄ± Ã§Ä±kar\n",
    "df_combined['filename'] = df_combined['fake_img_path'].apply(lambda x: os.path.basename(str(x)))\n",
    "\n",
    "# Filtrele: eÄŸitim setinde OLMAYAN gÃ¶rselleri tut\n",
    "df_filtered = df_combined[~df_combined['filename'].isin(training_filenames)].copy()\n",
    "\n",
    "removed_count = len(df_combined) - len(df_filtered)\n",
    "print(f\"   âœ— Ã‡Ä±karÄ±ldÄ±: {removed_count:,} eÄŸitim gÃ¶rseli\")\n",
    "print(f\"   âœ“ Kalan: {len(df_filtered):,} test iÃ§in Ã¶rnek\")\n",
    "\n",
    "# GeÃ§ici sÃ¼tunu temizle\n",
    "df_filtered = df_filtered.drop(columns=['filename'])\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 4: SEM_MAGNITUDE DAÄILIMINI ANALÄ°Z ET\n",
    "# ==============================================================================\n",
    "print(\"\\nğŸ“Š FAZ 4: sem_magnitude daÄŸÄ±lÄ±mÄ± analiz ediliyor...\")\n",
    "\n",
    "magnitude_groups = df_filtered.groupby('sem_magnitude')\n",
    "magnitude_counts = df_filtered['sem_magnitude'].value_counts().to_dict()\n",
    "\n",
    "print(\"\\n   Dengeleme Ã–NCESÄ° daÄŸÄ±lÄ±m:\")\n",
    "for mag in ['small', 'medium', 'large']:\n",
    "    count = magnitude_counts.get(mag, 0)\n",
    "    percentage = (count / len(df_filtered) * 100) if len(df_filtered) > 0 else 0\n",
    "    print(f\"   â€¢ {mag:8s}: {count:6,} Ã¶rnek ({percentage:5.2f}%)\")\n",
    "\n",
    "# ÃœÃ§ sÄ±nÄ±fÄ±n da var olduÄŸunu doÄŸrula\n",
    "missing_classes = [mag for mag in ['small', 'medium', 'large'] if mag not in magnitude_counts]\n",
    "if missing_classes:\n",
    "    raise ValueError(f\"âŒ Eksik sem_magnitude sÄ±nÄ±flarÄ±: {missing_classes}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 5: SINIFLARI DENGELE\n",
    "# ==============================================================================\n",
    "print(f\"\\nâš–ï¸  FAZ 5: SÄ±nÄ±flar dengeleniyor (mod: {CONFIG['balancing_mode']})...\")\n",
    "\n",
    "random.seed(42)  # Tekrarlanabilirlik iÃ§in\n",
    "balanced_samples = []\n",
    "\n",
    "if CONFIG['balancing_mode'] == 'fixed_number':\n",
    "    # SEÃ‡ENEK 1: Her sÄ±nÄ±ftan sabit sayÄ±\n",
    "    target_per_class = CONFIG['samples_per_magnitude_class']\n",
    "    print(f\"   Hedef: Her sÄ±nÄ±ftan {target_per_class:,} Ã¶rnek\\n\")\n",
    "    \n",
    "    for mag in ['small', 'medium', 'large']:\n",
    "        class_data = magnitude_groups.get_group(mag)\n",
    "        available = len(class_data)\n",
    "        \n",
    "        if available >= target_per_class:\n",
    "            # Yeterli Ã¶rnek var: hedef kadar rastgele seÃ§\n",
    "            sampled = class_data.sample(n=target_per_class, random_state=42)\n",
    "            balanced_samples.append(sampled)\n",
    "            print(f\"   âœ“ {mag:8s}: {available:,} mevcut Ã¶rnekten {target_per_class:,} seÃ§ildi\")\n",
    "        else:\n",
    "            # Yeterli Ã¶rnek yok: hepsini al ve uyar\n",
    "            balanced_samples.append(class_data)\n",
    "            print(f\"   âš ï¸  {mag:8s}: sadece {available:,} mevcut (hedef {target_per_class:,}), HEPSÄ° alÄ±nÄ±yor\")\n",
    "\n",
    "elif CONFIG['balancing_mode'] == 'percentage':\n",
    "    # SEÃ‡ENEK 2: Her sÄ±nÄ±ftan yÃ¼zde olarak\n",
    "    percentage = CONFIG['percentage_to_keep']\n",
    "    print(f\"   Hedef: Her sÄ±nÄ±fÄ±n %{percentage}'i\\n\")\n",
    "    \n",
    "    for mag in ['small', 'medium', 'large']:\n",
    "        class_data = magnitude_groups.get_group(mag)\n",
    "        available = len(class_data)\n",
    "        target = int(available * percentage / 100)\n",
    "        target = max(1, target)  # En az 1 Ã¶rnek\n",
    "        \n",
    "        sampled = class_data.sample(n=target, random_state=42)\n",
    "        balanced_samples.append(sampled)\n",
    "        print(f\"   âœ“ {mag:8s}: {available:,} Ã¶rnekten {target:,} seÃ§ildi (%{percentage})\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 6: SON DATAFRAME'Ä° OLUÅTUR\n",
    "# ==============================================================================\n",
    "print(\"\\nğŸ¯ FAZ 6: Son test veri seti oluÅŸturuluyor...\")\n",
    "\n",
    "df_test = pd.concat(balanced_samples, ignore_index=True)\n",
    "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)  # KarÄ±ÅŸtÄ±r\n",
    "\n",
    "final_magnitude_counts = df_test['sem_magnitude'].value_counts().to_dict()\n",
    "\n",
    "print(f\"\\nâœ… SON TEST VERÄ° SETÄ°: {len(df_test):,} Ã¶rnek\")\n",
    "print(\"\\n   Dengeleme SONRASI daÄŸÄ±lÄ±m:\")\n",
    "for mag in ['small', 'medium', 'large']:\n",
    "    count = final_magnitude_counts.get(mag, 0)\n",
    "    percentage = (count / len(df_test) * 100) if len(df_test) > 0 else 0\n",
    "    print(f\"   â€¢ {mag:8s}: {count:6,} Ã¶rnek ({percentage:5.2f}%)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 7: MASKE VERÄ°TABANI OLUÅTUR VE MASKE YOLLARINI EKLE\n",
    "# ==============================================================================\n",
    "print(\"\\nğŸ—‚ï¸  FAZ 7: Maske veritabanÄ± oluÅŸturuluyor...\")\n",
    "\n",
    "# Root dizinden maskeleri indeksle\n",
    "mask_db = {}\n",
    "search_patterns = [\n",
    "    os.path.join(CONFIG['root_dir'], \"**\", \"masks\", \"*.png\"),\n",
    "    os.path.join(CONFIG['root_dir'], \"**\", \"masks\", \"*.jpg\"),\n",
    "    os.path.join(CONFIG['root_dir'], \"masks\", \"**\", \"*.png\"),\n",
    "    os.path.join(CONFIG['root_dir'], \"original\", \"**\", \"*.png\"),\n",
    "]\n",
    "\n",
    "for pattern in search_patterns:\n",
    "    for m_path in glob.glob(pattern, recursive=True):\n",
    "        filename = os.path.basename(m_path)\n",
    "        name_no_ext = os.path.splitext(filename)[0]\n",
    "        mask_db[name_no_ext] = m_path\n",
    "\n",
    "print(f\"   âœ“ Maske veritabanÄ±: {len(mask_db):,} maske indekslendi\")\n",
    "\n",
    "# Maske yollarÄ±nÄ± ekle\n",
    "mask_paths = []\n",
    "missing_masks = 0\n",
    "\n",
    "for idx, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Maskeler eÅŸleÅŸtiriliyor\"):\n",
    "    mask_id = str(row['mask_id']).strip()\n",
    "    \n",
    "    if mask_id in mask_db:\n",
    "        mask_paths.append(mask_db[mask_id])\n",
    "    else:\n",
    "        # Yedek: perturbed_img_id kullanmayÄ± dene\n",
    "        img_id = os.path.splitext(str(row['perturbed_img_id']))[0]\n",
    "        if img_id in mask_db:\n",
    "            mask_paths.append(mask_db[img_id])\n",
    "        else:\n",
    "            mask_paths.append(None)\n",
    "            missing_masks += 1\n",
    "\n",
    "df_test['mask_path'] = mask_paths\n",
    "\n",
    "# Eksik maskeli satÄ±rlarÄ± kaldÄ±r\n",
    "if missing_masks > 0:\n",
    "    print(f\"   âš ï¸  Eksik maskeler: {missing_masks}, bu Ã¶rnekler Ã§Ä±karÄ±lÄ±yor...\")\n",
    "    df_test = df_test.dropna(subset=['mask_path']).reset_index(drop=True)\n",
    "    print(f\"   âœ“ Maske doÄŸrulamasÄ±ndan sonra son sayÄ±: {len(df_test):,} Ã¶rnek\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FAZ 8: SON DOÄRULAMA\n",
    "# ==============================================================================\n",
    "print(\"\\nğŸ” FAZ 8: Yollar doÄŸrulanÄ±yor...\")\n",
    "\n",
    "# Mevcut kodla uyumluluk iÃ§in sÃ¼tun adÄ±nÄ± deÄŸiÅŸtir\n",
    "df_test = df_test.rename(columns={'fake_img_path': 'image_path'})\n",
    "\n",
    "# BirkaÃ§ Ã¶rneÄŸi doÄŸrula\n",
    "sample_size = min(5, len(df_test))\n",
    "validation_passed = 0\n",
    "validation_failed = 0\n",
    "\n",
    "for idx in range(sample_size):\n",
    "    row = df_test.iloc[idx]\n",
    "    img_exists = os.path.exists(row['image_path'])\n",
    "    mask_exists = os.path.exists(row['mask_path'])\n",
    "    \n",
    "    if img_exists and mask_exists:\n",
    "        validation_passed += 1\n",
    "    else:\n",
    "        validation_failed += 1\n",
    "        print(f\"   âš ï¸  Ã–rnek {idx}: GÃ¶rsel var: {img_exists}, Maske var: {mask_exists}\")\n",
    "\n",
    "print(f\"   âœ“ Ã–rnek doÄŸrulama: {validation_passed}/{sample_size} baÅŸarÄ±lÄ±\")\n",
    "\n",
    "if validation_failed > 0:\n",
    "    print(f\"   âš ï¸  UyarÄ±: {validation_failed} Ã¶rnekte eksik dosya var\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… TEST VERÄ°SÄ° HAZIRLAMA TAMAMLANDI!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ“Š {len(df_test):,} dengeli Ã¶rnek ile test iÃ§in hazÄ±r\")\n",
    "print(\"   Test fonksiyonunu Ã§alÄ±ÅŸtÄ±rmak iÃ§in HÃ¼cre 4'Ã¼ Ã§alÄ±ÅŸtÄ±rabilirsiniz.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b05a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.df.iloc[idx]\n",
    "            image = Image.open(row['image_path']).convert(\"RGB\")\n",
    "            mask = Image.open(row['mask_path']).convert(\"L\")\n",
    "            \n",
    "            # Maske Binary (0-1)\n",
    "            mask_np = np.array(mask)\n",
    "            mask_np = np.where(mask_np > 0, 1, 0).astype(np.uint8)\n",
    "            \n",
    "            encoded = self.processor(images=image, segmentation_maps=mask_np, return_tensors=\"pt\")\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": encoded[\"pixel_values\"].squeeze(),\n",
    "                \"labels\": encoded[\"labels\"].squeeze(),\n",
    "                \"sem_magnitude\": str(row.get('sem_magnitude', 'Unknown'))\n",
    "            }\n",
    "        except:\n",
    "            return self.__getitem__((idx + 1) % len(self.df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "q9nomh9tn0g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper function tanÄ±mlandÄ±: calculate_per_image_iou()\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PER-IMAGE IoU CALCULATION HELPER\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_per_image_iou(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculate IoU metrics for a single image\n",
    "    \n",
    "    Args:\n",
    "        labels: 1D numpy array of ground truth labels\n",
    "        preds: 1D numpy array of predictions\n",
    "    \n",
    "    Returns:\n",
    "        fake_iou, bg_iou, mean_iou, pixel_accuracy\n",
    "    \"\"\"\n",
    "    n_classes = 2\n",
    "    \n",
    "    # Build confusion matrix for this image\n",
    "    hist = np.bincount(\n",
    "        n_classes * labels.astype(int) + preds.astype(int),\n",
    "        minlength=n_classes ** 2\n",
    "    ).reshape(n_classes, n_classes)\n",
    "    \n",
    "    # Calculate IoU per class\n",
    "    ious = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist) + 1e-10)\n",
    "    \n",
    "    bg_iou = ious[0] if len(ious) > 0 else 0.0\n",
    "    fake_iou = ious[1] if len(ious) > 1 else 0.0\n",
    "    mean_iou = np.nanmean(ious)\n",
    "    \n",
    "    # Calculate pixel accuracy\n",
    "    pixel_accuracy = (labels == preds).mean()\n",
    "    \n",
    "    return fake_iou, bg_iou, mean_iou, pixel_accuracy\n",
    "\n",
    "print(\"âœ… Helper function tanÄ±mlandÄ±: calculate_per_image_iou()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce39d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test fonksiyonu tanÄ±mlandÄ±. Ã‡alÄ±ÅŸtÄ±rmak iÃ§in: results = run_safe_test()\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# GELIÅMIÅ TEST FONKSIYONU (CSV & VÄ°ZÃœALÄ°ZASYON DESTEKLÄ°)\n",
    "# ==============================================================================\n",
    "\n",
    "class StreamSegMetrics:\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "        self.total_samples = 0\n",
    "\n",
    "    def update(self, label_trues, label_preds):\n",
    "        if torch.is_tensor(label_trues):\n",
    "            label_trues = label_trues.cpu().numpy()\n",
    "        if torch.is_tensor(label_preds):\n",
    "            label_preds = label_preds.cpu().numpy()\n",
    "            \n",
    "        mask = (label_trues >= 0) & (label_trues < self.n_classes)\n",
    "        label_trues = label_trues[mask].astype(np.int32)\n",
    "        label_preds = label_preds[mask].astype(np.int32)\n",
    "        \n",
    "        self.confusion_matrix += np.bincount(\n",
    "            self.n_classes * label_trues + label_preds,\n",
    "            minlength=self.n_classes ** 2\n",
    "        ).reshape(self.n_classes, self.n_classes)\n",
    "        \n",
    "        self.total_samples += 1\n",
    "\n",
    "    def get_results(self):\n",
    "        hist = self.confusion_matrix\n",
    "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist) + 1e-10)\n",
    "        mean_iou = np.nanmean(iu)\n",
    "        acc = np.diag(hist).sum() / (hist.sum() + 1e-10)\n",
    "        \n",
    "        return {\n",
    "            \"mean_iou\": mean_iou,\n",
    "            \"fake_iou\": iu[1] if len(iu) > 1 else 0.0,\n",
    "            \"bg_iou\": iu[0] if len(iu) > 0 else 0.0,\n",
    "            \"accuracy\": acc\n",
    "        }\n",
    "\n",
    "def run_safe_test():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ GELIÅMIÅ TEST (CSV & VÄ°ZÃœALÄ°ZASYON Ä°LE)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if 'df_test' not in globals() or len(df_test) == 0:\n",
    "        print(\"âŒ HATA: df_test bulunamadÄ±! LÃ¼tfen veri hazÄ±rlama hÃ¼cresini Ã§alÄ±ÅŸtÄ±r.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ”„ Model: {CONFIG['model_path']}\")\n",
    "    try:\n",
    "        processor = SegformerImageProcessor.from_pretrained(CONFIG['model_path'])\n",
    "        model = SegformerForSemanticSegmentation.from_pretrained(CONFIG['model_path'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model HatasÄ±: {e}\")\n",
    "        return\n",
    "\n",
    "    test_ds = TestDataset(df_test, processor)\n",
    "    loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    global_metric = StreamSegMetrics(n_classes=2)\n",
    "    mag_metrics = defaultdict(lambda: StreamSegMetrics(n_classes=2))\n",
    "    \n",
    "    # âœ¨ NEW: Per-image results collection\n",
    "    per_image_results = []\n",
    "    current_idx = 0\n",
    "    \n",
    "    print(f\"ğŸï¸  {len(test_ds)} gÃ¶rsel test ediliyor...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Analiz\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            mags = batch[\"sem_magnitude\"]\n",
    "            \n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            \n",
    "            logits = torch.nn.functional.interpolate(\n",
    "                outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            # âœ¨ NEW: Calculate confidence scores\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)  # Shape: [B, 2, H, W]\n",
    "            \n",
    "            # Global metrics (existing)\n",
    "            global_metric.update(labels, preds)\n",
    "            \n",
    "            # Per-image metrics (NEW)\n",
    "            for i, mag in enumerate(mags):\n",
    "                single_label = labels[i]\n",
    "                single_pred = preds[i]\n",
    "                mag_metrics[mag].update(single_label, single_pred)\n",
    "                \n",
    "                # âœ¨ Calculate per-image metrics\n",
    "                label_np = single_label.cpu().numpy()\n",
    "                pred_np = single_pred.cpu().numpy()\n",
    "                prob_np = probs[i].cpu().numpy()  # Shape: [2, H, W]\n",
    "                \n",
    "                # Filter valid pixels\n",
    "                valid_mask = (label_np >= 0) & (label_np < 2)\n",
    "                label_valid = label_np[valid_mask]\n",
    "                pred_valid = pred_np[valid_mask]\n",
    "                \n",
    "                if len(label_valid) > 0:\n",
    "                    # Calculate IoU\n",
    "                    fake_iou, bg_iou, mean_iou, pixel_acc = calculate_per_image_iou(\n",
    "                        label_valid, pred_valid\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate confidence\n",
    "                    max_probs = prob_np.max(axis=0)  # Max prob per pixel\n",
    "                    conf_mean = float(max_probs[valid_mask].mean())\n",
    "                    conf_max = float(max_probs[valid_mask].max())\n",
    "                    conf_min = float(max_probs[valid_mask].min())\n",
    "                    \n",
    "                    # Get metadata\n",
    "                    row_data = df_test.iloc[current_idx + i]\n",
    "                    \n",
    "                    # Store results\n",
    "                    per_image_results.append({\n",
    "                        'image_path': row_data['image_path'],\n",
    "                        'perturbed_img_id': row_data.get('perturbed_img_id', ''),\n",
    "                        'dataset': row_data.get('dataset', ''),\n",
    "                        'sem_magnitude': mag,\n",
    "                        'area_ratio': float(row_data.get('area_ratio', np.nan)),\n",
    "                        'ssim': float(row_data.get('ssim', np.nan)),\n",
    "                        'lpips_score': float(row_data.get('lpips_score', np.nan)),\n",
    "                        'mse': float(row_data.get('mse', np.nan)),\n",
    "                        'fake_iou': float(fake_iou),\n",
    "                        'bg_iou': float(bg_iou),\n",
    "                        'mean_iou': float(mean_iou),\n",
    "                        'pixel_accuracy': float(pixel_acc),\n",
    "                        'confidence_mean': conf_mean,\n",
    "                        'confidence_max': conf_max,\n",
    "                        'confidence_min': conf_min\n",
    "                    })\n",
    "            \n",
    "            current_idx += len(mags)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ† GENEL SONUÃ‡LAR\")\n",
    "    print(\"=\"*70)\n",
    "    res = global_metric.get_results()\n",
    "    print(f\"ğŸ”¥ Mean IoU:       {res['mean_iou']:.4f}\")\n",
    "    print(f\"ğŸ¦  Fake IoU (1):   {res['fake_iou']:.4f}\")\n",
    "    print(f\"ğŸï¸ Background IoU: {res['bg_iou']:.4f}\")\n",
    "    print(f\"ğŸ¯ Accuracy:       {res['accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š ZORLUK SEVÄ°YESÄ°NE (MAGNITUDE) GÃ–RE ANALÄ°Z\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'MAGNITUDE':<20} | {'mIoU':<10} | {'Fake IoU':<10} | {'Adet':<5}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for mag in sorted(mag_metrics.keys()):\n",
    "        res = mag_metrics[mag].get_results()\n",
    "        count = mag_metrics[mag].total_samples\n",
    "        print(f\"{mag:<20} | {res['mean_iou']:.4f}     | {res['fake_iou']:.4f}     | {count}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # âœ¨ Save results if configured\n",
    "    if CONFIG.get('save_results', True):\n",
    "        df_results = pd.DataFrame(per_image_results)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_dir = os.path.join(CONFIG.get('output_dir', 'segformer_test_results'), timestamp)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CSV (function will be defined in next cell)\n",
    "        save_results_to_csv(df_results, output_dir, global_metric, mag_metrics)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        if CONFIG.get('generate_visualizations', True):\n",
    "            generate_all_visualizations(df_results, output_dir)\n",
    "        \n",
    "        print(f\"\\nâœ… SonuÃ§lar kaydedildi: {output_dir}\")\n",
    "    \n",
    "    return per_image_results, global_metric, mag_metrics\n",
    "\n",
    "print(\"âœ… Test fonksiyonu tanÄ±mlandÄ±. Ã‡alÄ±ÅŸtÄ±rmak iÃ§in: results = run_safe_test()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "umcoqc1uea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CSV export fonksiyonlarÄ± hazÄ±r\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CSV EXPORT FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def save_results_to_csv(df_results, output_dir, global_metric, mag_metrics):\n",
    "    \"\"\"\n",
    "    Save test results to CSV files\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“ CSV dosyalarÄ± kaydediliyor...\")\n",
    "    \n",
    "    # 1. Detailed results CSV (per-image)\n",
    "    detailed_csv_path = os.path.join(output_dir, 'detailed_results.csv')\n",
    "    df_results.to_csv(detailed_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"   âœ… DetaylÄ± sonuÃ§lar: {detailed_csv_path}\")\n",
    "    print(f\"      {len(df_results):,} Ã¶rnek kaydedildi\")\n",
    "    \n",
    "    # 2. Summary metrics CSV (aggregate)\n",
    "    summary_data = []\n",
    "    \n",
    "    # Global metrics\n",
    "    global_res = global_metric.get_results()\n",
    "    summary_data.append({\n",
    "        'category': 'Global',\n",
    "        'subset': 'All',\n",
    "        'sample_count': global_metric.total_samples,\n",
    "        'mean_iou': global_res['mean_iou'],\n",
    "        'fake_iou': global_res['fake_iou'],\n",
    "        'bg_iou': global_res['bg_iou'],\n",
    "        'pixel_accuracy': global_res['accuracy']\n",
    "    })\n",
    "    \n",
    "    # Magnitude-based metrics\n",
    "    for mag_name in sorted(mag_metrics.keys()):\n",
    "        mag_res = mag_metrics[mag_name].get_results()\n",
    "        summary_data.append({\n",
    "            'category': 'Sem_Magnitude',\n",
    "            'subset': mag_name,\n",
    "            'sample_count': mag_metrics[mag_name].total_samples,\n",
    "            'mean_iou': mag_res['mean_iou'],\n",
    "            'fake_iou': mag_res['fake_iou'],\n",
    "            'bg_iou': mag_res['bg_iou'],\n",
    "            'pixel_accuracy': mag_res['accuracy']\n",
    "        })\n",
    "    \n",
    "    # Dataset-based metrics (if available)\n",
    "    if 'dataset' in df_results.columns:\n",
    "        for dataset_name in df_results['dataset'].unique():\n",
    "            if pd.notna(dataset_name):\n",
    "                dataset_subset = df_results[df_results['dataset'] == dataset_name]\n",
    "                summary_data.append({\n",
    "                    'category': 'Dataset',\n",
    "                    'subset': dataset_name,\n",
    "                    'sample_count': len(dataset_subset),\n",
    "                    'mean_iou': dataset_subset['mean_iou'].mean(),\n",
    "                    'fake_iou': dataset_subset['fake_iou'].mean(),\n",
    "                    'bg_iou': dataset_subset['bg_iou'].mean(),\n",
    "                    'pixel_accuracy': dataset_subset['pixel_accuracy'].mean()\n",
    "                })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_csv_path = os.path.join(output_dir, 'summary_metrics.csv')\n",
    "    df_summary.to_csv(summary_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"   âœ… Ã–zet metrikler: {summary_csv_path}\")\n",
    "    \n",
    "    # 3. Statistical summary\n",
    "    stats_data = []\n",
    "    for column in ['fake_iou', 'bg_iou', 'mean_iou', 'pixel_accuracy', \n",
    "                   'confidence_mean', 'confidence_max', 'confidence_min']:\n",
    "        if column in df_results.columns:\n",
    "            stats_data.append({\n",
    "                'metric': column,\n",
    "                'mean': df_results[column].mean(),\n",
    "                'std': df_results[column].std(),\n",
    "                'min': df_results[column].min(),\n",
    "                'q25': df_results[column].quantile(0.25),\n",
    "                'median': df_results[column].median(),\n",
    "                'q75': df_results[column].quantile(0.75),\n",
    "                'max': df_results[column].max()\n",
    "            })\n",
    "    \n",
    "    df_stats = pd.DataFrame(stats_data)\n",
    "    stats_csv_path = os.path.join(output_dir, 'statistical_summary.csv')\n",
    "    df_stats.to_csv(stats_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"   âœ… Ä°statistiksel Ã¶zet: {stats_csv_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Toplam {len(df_results):,} Ã¶rnek iÃ§in 3 CSV dosyasÄ± oluÅŸturuldu\")\n",
    "\n",
    "print(\"âœ… CSV export fonksiyonlarÄ± hazÄ±r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hlhkit9auf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Visualization fonksiyonlarÄ± (1/2) hazÄ±r\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZATION FUNCTIONS - PART 1/2\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_all_visualizations(df_results, output_dir):\n",
    "    \"\"\"Generate all visualization plots\"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    viz_dir = os.path.join(output_dir, 'visualizations')\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\")\n",
    "    \n",
    "    print(\"   ğŸ“Š 1/5: Metrik daÄŸÄ±lÄ±mlarÄ±...\")\n",
    "    plot_metric_distributions(df_results, viz_dir)\n",
    "    \n",
    "    print(\"   ğŸ“Š 2/5: Magnitude bazlÄ± performans...\")\n",
    "    plot_magnitude_performance(df_results, viz_dir)\n",
    "    \n",
    "    print(\"   ğŸ“Š 3/5: Dataset karÅŸÄ±laÅŸtÄ±rmasÄ±...\")\n",
    "    plot_dataset_comparison(df_results, viz_dir)\n",
    "    \n",
    "    print(\"   ğŸ“Š 4/5: Korelasyon grafikleri...\")\n",
    "    plot_correlation_scatters(df_results, viz_dir)\n",
    "    \n",
    "    print(\"   ğŸ“Š 5/5: KapsamlÄ± dashboard...\")\n",
    "    plot_comprehensive_dashboard(df_results, viz_dir)\n",
    "    \n",
    "    print(f\"\\nâœ… 5 gÃ¶rselleÅŸtirme kaydedildi: {viz_dir}/\")\n",
    "\n",
    "def plot_metric_distributions(df, viz_dir):\n",
    "    \"\"\"2x3 dashboard of metric distributions\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Metric Distributions & Box Plots by Sem_Magnitude', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    metrics = [\n",
    "        ('fake_iou', 'Fake IoU', '#e74c3c'),\n",
    "        ('pixel_accuracy', 'Pixel Accuracy', '#2ecc71'),\n",
    "        ('confidence_mean', 'Mean Confidence', '#3498db')\n",
    "    ]\n",
    "    \n",
    "    for idx, (col, title, color) in enumerate(metrics):\n",
    "        # Top row: Histograms\n",
    "        ax_hist = axes[0, idx]\n",
    "        ax_hist.hist(df[col].dropna(), bins=40, color=color, \n",
    "                     alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "        ax_hist.axvline(df[col].mean(), color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Mean: {df[col].mean():.3f}')\n",
    "        ax_hist.axvline(df[col].median(), color='blue', linestyle='--', \n",
    "                       linewidth=2, label=f'Median: {df[col].median():.3f}')\n",
    "        ax_hist.set_xlabel(title, fontsize=12, fontweight='bold')\n",
    "        ax_hist.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "        ax_hist.set_title(f'{title} Distribution', fontsize=13, fontweight='bold')\n",
    "        ax_hist.legend(fontsize=9)\n",
    "        ax_hist.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Bottom row: Box plots\n",
    "        ax_box = axes[1, idx]\n",
    "        if 'sem_magnitude' in df.columns:\n",
    "            mag_order = ['small', 'medium', 'large']\n",
    "            available_mags = [m for m in mag_order if m in df['sem_magnitude'].unique()]\n",
    "            \n",
    "            box_data = [df[df['sem_magnitude'] == mag][col].dropna() \n",
    "                       for mag in available_mags]\n",
    "            \n",
    "            bp = ax_box.boxplot(box_data, labels=available_mags, \n",
    "                               patch_artist=True, showmeans=True,\n",
    "                               meanprops=dict(marker='D', markerfacecolor='red', \n",
    "                                            markersize=8))\n",
    "            \n",
    "            for patch in bp['boxes']:\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.6)\n",
    "            \n",
    "            ax_box.set_xlabel('Sem_Magnitude', fontsize=11, fontweight='bold')\n",
    "            ax_box.set_ylabel(title, fontsize=11, fontweight='bold')\n",
    "            ax_box.set_title(f'{title} by Magnitude', fontsize=13, fontweight='bold')\n",
    "            ax_box.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, '01_metric_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_magnitude_performance(df, viz_dir):\n",
    "    \"\"\"Bar charts with error bars for performance by magnitude\"\"\"\n",
    "    if 'sem_magnitude' not in df.columns:\n",
    "        print(\"      âš ï¸ sem_magnitude sÃ¼tunu yok, atlanÄ±yor...\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Performance Metrics by Sem_Magnitude', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = [\n",
    "        ('mean_iou', 'Mean IoU', '#9b59b6'),\n",
    "        ('fake_iou', 'Fake IoU', '#e74c3c'),\n",
    "        ('pixel_accuracy', 'Pixel Accuracy', '#2ecc71'),\n",
    "        ('confidence_mean', 'Mean Confidence', '#3498db')\n",
    "    ]\n",
    "    \n",
    "    mag_order = ['small', 'medium', 'large']\n",
    "    \n",
    "    for idx, (col, title, color) in enumerate(metrics):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        means = []\n",
    "        stds = []\n",
    "        counts = []\n",
    "        \n",
    "        for mag in mag_order:\n",
    "            mag_data = df[df['sem_magnitude'] == mag][col].dropna()\n",
    "            if len(mag_data) > 0:\n",
    "                means.append(mag_data.mean())\n",
    "                stds.append(mag_data.std())\n",
    "                counts.append(len(mag_data))\n",
    "            else:\n",
    "                means.append(0)\n",
    "                stds.append(0)\n",
    "                counts.append(0)\n",
    "        \n",
    "        x = np.arange(len(mag_order))\n",
    "        bars = ax.bar(x, means, yerr=stds, capsize=10, \n",
    "                      color=color, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        for i, (bar, mean, count) in enumerate(zip(bars, means, counts)):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{mean:.3f}\\n(n={count:,})',\n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(mag_order, fontsize=11)\n",
    "        ax.set_xlabel('Sem_Magnitude', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{title} by Magnitude', fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_ylim([0, min(1.1, max(means) * 1.2) if max(means) > 0 else 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, '02_magnitude_performance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(\"âœ… Visualization fonksiyonlarÄ± (1/2) hazÄ±r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "x532z365pw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Visualization fonksiyonlarÄ± (2/2) hazÄ±r\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZATION FUNCTIONS - PART 2/2\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_dataset_comparison(df, viz_dir):\n",
    "    \"\"\"Grouped bar chart comparing performance across datasets\"\"\"\n",
    "    if 'dataset' not in df.columns or df['dataset'].isna().all():\n",
    "        print(\"      âš ï¸ dataset sÃ¼tunu yok veya boÅŸ, atlanÄ±yor...\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    metrics = ['fake_iou', 'mean_iou', 'pixel_accuracy']\n",
    "    metric_labels = ['Fake IoU', 'Mean IoU', 'Pixel Accuracy']\n",
    "    \n",
    "    datasets = df['dataset'].dropna().unique()\n",
    "    datasets = sorted([d for d in datasets if d])[:10]\n",
    "    \n",
    "    if len(datasets) == 0:\n",
    "        print(\"      âš ï¸ GeÃ§erli dataset yok, atlanÄ±yor...\")\n",
    "        return\n",
    "    \n",
    "    data_matrix = []\n",
    "    for dataset in datasets:\n",
    "        dataset_data = df[df['dataset'] == dataset]\n",
    "        row = [dataset_data[col].mean() for col in metrics]\n",
    "        data_matrix.append(row)\n",
    "    \n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.25\n",
    "    \n",
    "    colors = ['#e74c3c', '#9b59b6', '#2ecc71']\n",
    "    \n",
    "    for i, (metric, label, color) in enumerate(zip(metrics, metric_labels, colors)):\n",
    "        values = [row[i] for row in data_matrix]\n",
    "        offset = width * (i - 1)\n",
    "        bars = ax.bar(x + offset, values, width, label=label, \n",
    "                     color=color, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "        \n",
    "        for bar, val in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            if height > 0.05:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{val:.2f}',\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Dataset', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Performance Score', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Performance Comparison Across Datasets', \n",
    "                fontsize=15, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets, rotation=45, ha='right', fontsize=10)\n",
    "    ax.legend(fontsize=11, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, '03_dataset_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_correlation_scatters(df, viz_dir):\n",
    "    \"\"\"2x2 grid of scatter plots showing correlations\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    fig.suptitle('Correlation Analysis: Scatter Plots', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plots = [\n",
    "        ('confidence_mean', 'fake_iou', 'Mean Confidence vs Fake IoU'),\n",
    "        ('confidence_mean', 'pixel_accuracy', 'Mean Confidence vs Pixel Accuracy'),\n",
    "        ('area_ratio', 'fake_iou', 'Area Ratio vs Fake IoU'),\n",
    "        ('ssim', 'fake_iou', 'SSIM vs Fake IoU')\n",
    "    ]\n",
    "    \n",
    "    for idx, (x_col, y_col, title) in enumerate(plots):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        if x_col not in df.columns or y_col not in df.columns:\n",
    "            ax.text(0.5, 0.5, f'Data not available\\n({x_col}, {y_col})',\n",
    "                   ha='center', va='center', fontsize=12)\n",
    "            ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        valid_data = df[[x_col, y_col]].dropna()\n",
    "        \n",
    "        if len(valid_data) == 0:\n",
    "            ax.text(0.5, 0.5, 'No valid data',\n",
    "                   ha='center', va='center', fontsize=12)\n",
    "            ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        x = valid_data[x_col]\n",
    "        y = valid_data[y_col]\n",
    "        \n",
    "        if 'sem_magnitude' in df.columns:\n",
    "            mag_colors = {'small': '#3498db', 'medium': '#f39c12', 'large': '#e74c3c'}\n",
    "            \n",
    "            for mag, color in mag_colors.items():\n",
    "                mag_mask = df.loc[valid_data.index, 'sem_magnitude'] == mag\n",
    "                if mag_mask.sum() > 0:\n",
    "                    ax.scatter(x[mag_mask], y[mag_mask], \n",
    "                             c=color, label=mag, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)\n",
    "            ax.legend(fontsize=9, title='Sem_Magnitude')\n",
    "        else:\n",
    "            ax.scatter(x, y, alpha=0.6, s=30, c='#3498db', edgecolors='black', linewidth=0.5)\n",
    "        \n",
    "        correlation = x.corr(y)\n",
    "        \n",
    "        if len(x) > 1:\n",
    "            z = np.polyfit(x, y, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_line = np.linspace(x.min(), x.max(), 100)\n",
    "            ax.plot(x_line, p(x_line), \"r--\", linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel(x_col.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel(y_col.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        textstr = f'Pearson r = {correlation:.3f}\\nN = {len(x):,}'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "               verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, '04_correlation_plots.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_comprehensive_dashboard(df, viz_dir):\n",
    "    \"\"\"Professional comprehensive dashboard with statistics\"\"\"\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "    fig.suptitle('SegFormer Test Results - Comprehensive Dashboard', \n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Statistics text\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                            OVERALL STATISTICS                                  \n",
    "    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \n",
    "    Total Images: {len(df):,}\n",
    "    \n",
    "    â”Œâ”€ Performance Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Mean IoU:       {df['mean_iou'].mean():.4f} Â± {df['mean_iou'].std():.4f}\n",
    "    â”‚  Fake IoU:       {df['fake_iou'].mean():.4f} Â± {df['fake_iou'].std():.4f}\n",
    "    â”‚  Pixel Accuracy: {df['pixel_accuracy'].mean():.4f} Â± {df['pixel_accuracy'].std():.4f}\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    â”Œâ”€ Confidence Scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  Mean Confidence:  {df['confidence_mean'].mean():.4f} Â± {df['confidence_mean'].std():.4f}\n",
    "    â”‚  Max Confidence:   {df['confidence_max'].mean():.4f}\n",
    "    â”‚  Min Confidence:   {df['confidence_min'].mean():.4f}\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \"\"\"\n",
    "    \n",
    "    ax1.text(0.05, 0.5, stats_text, fontsize=10, family='monospace',\n",
    "            verticalalignment='center', bbox=dict(boxstyle='round', \n",
    "            facecolor='lightblue', alpha=0.3))\n",
    "    \n",
    "    # Heatmap\n",
    "    ax2 = fig.add_subplot(gs[1, :2])\n",
    "    if 'sem_magnitude' in df.columns:\n",
    "        mag_order = ['small', 'medium', 'large']\n",
    "        metrics = ['fake_iou', 'mean_iou', 'pixel_accuracy']\n",
    "        \n",
    "        heatmap_data = []\n",
    "        for mag in mag_order:\n",
    "            mag_data = df[df['sem_magnitude'] == mag]\n",
    "            if len(mag_data) > 0:\n",
    "                row = [mag_data[col].mean() for col in metrics]\n",
    "                heatmap_data.append(row)\n",
    "        \n",
    "        if heatmap_data:\n",
    "            heatmap_array = np.array(heatmap_data)\n",
    "            sns.heatmap(heatmap_array, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                       xticklabels=['Fake IoU', 'Mean IoU', 'Pixel Acc'],\n",
    "                       yticklabels=mag_order, ax=ax2, cbar_kws={'label': 'Score'},\n",
    "                       linewidths=1, linecolor='white', vmin=0, vmax=1)\n",
    "            ax2.set_title('Performance Heatmap by Sem_Magnitude', \n",
    "                         fontsize=13, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Top performers\n",
    "    ax3 = fig.add_subplot(gs[1, 2])\n",
    "    top_10 = df.nlargest(10, 'mean_iou')[['sem_magnitude', 'mean_iou']]\n",
    "    ax3.barh(range(10), top_10['mean_iou'], color='#2ecc71', edgecolor='black')\n",
    "    ax3.set_yticks(range(10))\n",
    "    ax3.set_yticklabels([row['sem_magnitude'][:3] for _, row in top_10.iterrows()], fontsize=8)\n",
    "    ax3.set_xlabel('Mean IoU', fontsize=10, fontweight='bold')\n",
    "    ax3.set_title('Top 10 Best Performers', fontsize=11, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='x')\n",
    "    ax3.invert_yaxis()\n",
    "    \n",
    "    # Violin plot\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    if 'sem_magnitude' in df.columns:\n",
    "        mag_order = ['small', 'medium', 'large']\n",
    "        violin_data = [df[df['sem_magnitude'] == mag]['fake_iou'].dropna() \n",
    "                      for mag in mag_order]\n",
    "        ax4.violinplot(violin_data, positions=range(len(mag_order)),\n",
    "                      showmeans=True, showmedians=True)\n",
    "        ax4.set_xticks(range(len(mag_order)))\n",
    "        ax4.set_xticklabels(mag_order)\n",
    "        ax4.set_ylabel('Fake IoU', fontsize=10, fontweight='bold')\n",
    "        ax4.set_title('Fake IoU Distribution', fontsize=11, fontweight='bold')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Confidence histogram\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    ax5.hist(df['confidence_mean'], bins=30, color='#3498db', \n",
    "            alpha=0.7, edgecolor='black')\n",
    "    ax5.axvline(df['confidence_mean'].mean(), color='red', \n",
    "               linestyle='--', linewidth=2, label='Mean')\n",
    "    ax5.set_xlabel('Mean Confidence', fontsize=10, fontweight='bold')\n",
    "    ax5.set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "    ax5.set_title('Confidence Distribution', fontsize=11, fontweight='bold')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pie chart\n",
    "    ax6 = fig.add_subplot(gs[2, 2])\n",
    "    if 'sem_magnitude' in df.columns:\n",
    "        mag_counts = df['sem_magnitude'].value_counts()\n",
    "        colors_pie = ['#3498db', '#f39c12', '#e74c3c']\n",
    "        ax6.pie(mag_counts.values, labels=mag_counts.index, autopct='%1.1f%%',\n",
    "               colors=colors_pie, startangle=90, \n",
    "               textprops={'fontsize': 10, 'weight': 'bold'})\n",
    "        ax6.set_title('Sample Distribution', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, '05_comprehensive_dashboard.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(\"âœ… Visualization fonksiyonlarÄ± (2/2) hazÄ±r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f21bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ¯ TÃœM FONKSÄ°YONLAR TANIMLANDI - TEST BAÅLIYOR\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ GELIÅMIÅ TEST (CSV & VÄ°ZÃœALÄ°ZASYON Ä°LE)\n",
      "======================================================================\n",
      "ğŸ”„ Model: ./segformer_b4_stable/final_best_model\n",
      "ğŸï¸  129 gÃ¶rsel test ediliyor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f43b5e6e44b4be38bcc0bebc84e4236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analiz:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ† GENEL SONUÃ‡LAR\n",
      "======================================================================\n",
      "ğŸ”¥ Mean IoU:       0.8742\n",
      "ğŸ¦  Fake IoU (1):   0.7731\n",
      "ğŸï¸ Background IoU: 0.9752\n",
      "ğŸ¯ Accuracy:       0.9772\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ZORLUK SEVÄ°YESÄ°NE (MAGNITUDE) GÃ–RE ANALÄ°Z\n",
      "======================================================================\n",
      "MAGNITUDE            | mIoU       | Fake IoU   | Adet \n",
      "----------------------------------------------------------------------\n",
      "large                | 0.8597     | 0.7429     | 43\n",
      "medium               | 0.8731     | 0.7689     | 43\n",
      "small                | 0.8844     | 0.7970     | 43\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ğŸ“ CSV dosyalarÄ± kaydediliyor...\n",
      "   âœ… DetaylÄ± sonuÃ§lar: segformer_test_results\\20251229_205934\\detailed_results.csv\n",
      "      129 Ã¶rnek kaydedildi\n",
      "   âœ… Ã–zet metrikler: segformer_test_results\\20251229_205934\\summary_metrics.csv\n",
      "   âœ… Ä°statistiksel Ã¶zet: segformer_test_results\\20251229_205934\\statistical_summary.csv\n",
      "\n",
      "âœ… Toplam 129 Ã¶rnek iÃ§in 3 CSV dosyasÄ± oluÅŸturuldu\n",
      "\n",
      "ğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\n",
      "   ğŸ“Š 1/5: Metrik daÄŸÄ±lÄ±mlarÄ±...\n",
      "   ğŸ“Š 2/5: Magnitude bazlÄ± performans...\n",
      "   ğŸ“Š 3/5: Dataset karÅŸÄ±laÅŸtÄ±rmasÄ±...\n",
      "   ğŸ“Š 4/5: Korelasyon grafikleri...\n",
      "   ğŸ“Š 5/5: KapsamlÄ± dashboard...\n",
      "\n",
      "âœ… 5 gÃ¶rselleÅŸtirme kaydedildi: segformer_test_results\\20251229_205934\\visualizations/\n",
      "\n",
      "âœ… SonuÃ§lar kaydedildi: segformer_test_results\\20251229_205934\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TEST Ã‡ALIÅTIRMA - TÃœM FONKSÄ°YONLAR TANIMLANDIKTAN SONRA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ TÃœM FONKSÄ°YONLAR TANIMLANDI - TEST BAÅLIYOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test fonksiyonunu Ã§alÄ±ÅŸtÄ±r\n",
    "results = run_safe_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
