{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ KÃœTÃœPHANE KURULUMU\n",
    "# TÃ¼m gerekli kÃ¼tÃ¼phaneleri kur (ilk Ã§alÄ±ÅŸtÄ±rmada Ã§alÄ±ÅŸtÄ±r)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ“¦ Gerekli kÃ¼tÃ¼phaneler yÃ¼kleniyor...\\n\")\n",
    "\n",
    "libraries = {\n",
    "    \"torch\": \"PyTorch (Deep Learning)\",\n",
    "    \"torchvision\": \"TorchVision (Image Models)\",\n",
    "    \"pandas\": \"Pandas (Data Processing)\",\n",
    "    \"numpy\": \"NumPy (Array Operations)\",\n",
    "    \"pillow\": \"Pillow (Image Loading)\",\n",
    "    \"tqdm\": \"TQDM (Progress Bar)\",\n",
    "    \"matplotlib\": \"Matplotlib (Plotting)\",\n",
    "    \"seaborn\": \"Seaborn (Advanced Plots)\",\n",
    "    \"scikit-learn\": \"Scikit-Learn (ML Utils)\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¯ KURULACAK KÃœTÃœPHANELER:\")\n",
    "print(\"=\" * 70)\n",
    "for i, (lib, desc) in enumerate(libraries.items(), 1):\n",
    "    print(f\"   {i}. {lib:<20} â†’ {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âš¡ KURULUM BAÅLIYOR...\\n\")\n",
    "\n",
    "# Pip upgrade\n",
    "print(\"1ï¸âƒ£ Pip yÃ¼kseltiliyor...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"])\n",
    "print(\"   âœ… Pip gÃ¼ncelleÅŸtirildi\\n\")\n",
    "\n",
    "# Standart libraries\n",
    "standard_libs = [\"pandas\", \"pillow\", \"numpy\", \"tqdm\", \"matplotlib\", \"seaborn\", \"scikit-learn\"]\n",
    "print(\"2ï¸âƒ£ Standart kÃ¼tÃ¼phaneler yÃ¼kleniyor...\")\n",
    "for lib in standard_libs:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib, \"-q\"])\n",
    "        print(f\"   âœ… {lib}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ {lib} kurulumunda hata: {e}\")\n",
    "\n",
    "# PyTorch NIGHTLY (RTX 5080 DesteÄŸi - CUDA 12.8)\n",
    "print(\"\\n3ï¸âƒ£ PyTorch NIGHTLY (RTX 5080 + CUDA 12.8) yÃ¼kleniyor...\")\n",
    "print(\"   âš ï¸  Bu biraz uzun sÃ¼rebilir (~2-3 dakika)\")\n",
    "\n",
    "try:\n",
    "    # Ã–nce eski PyTorch'u kaldÄ±r\n",
    "    print(\"   â†’ Eski PyTorch kaldÄ±rÄ±lÄ±yor...\")\n",
    "    subprocess.call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"torch\", \"torchvision\", \"-y\"], \n",
    "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    \n",
    "    # PyTorch Nightly + CUDA 12.8 kur (RTX 5080 desteÄŸi!)\n",
    "    print(\"   â†’ PyTorch Nightly (CUDA 12.8) kuruluyor...\")\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\",\n",
    "        \"--pre\", \"torch\", \"torchvision\",\n",
    "        \"--index-url\", \"https://download.pytorch.org/whl/nightly/cu128\"\n",
    "    ])\n",
    "    print(\"   âœ… PyTorch Nightly + CUDA 12.8 yÃ¼klendi (RTX 5080 destekli!)\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ PyTorch Nightly kurulumunda hata: {e}\")\n",
    "    print(\"   â†’ Fallback: CUDA 12.6 deneniyor...\")\n",
    "    try:\n",
    "        subprocess.check_call([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\",\n",
    "            \"torch\", \"torchvision\",\n",
    "            \"--index-url\", \"https://download.pytorch.org/whl/cu126\"\n",
    "        ])\n",
    "        print(\"   âš ï¸  CUDA 12.6 yÃ¼klendi (RTX 5080 tam desteklenmeyebilir)\")\n",
    "    except:\n",
    "        print(\"   âŒ CUDA 12.6 de baÅŸarÄ±sÄ±z!\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\"])\n",
    "        print(\"   âš ï¸  Standart PyTorch yÃ¼klendi\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… KURULUM TAMAMLANDI!\\n\")\n",
    "\n",
    "# Kontrol\n",
    "print(\"4ï¸âƒ£ Kontrol ediliyor...\\n\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"   âœ… PyTorch: {torch.__version__}\")\n",
    "    print(f\"   âœ… CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    print(f\"   âœ… Pandas: {pd.__version__}\")\n",
    "    \n",
    "    import numpy as np\n",
    "    print(f\"   âœ… NumPy: {np.__version__}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ‰ TÃœM KÃœTÃœPHANELER HAZIR!\")\n",
    "    print(\"=\" * 70)\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Hata: {e}\")\n",
    "    print(\"   â†’ LÃ¼tfen hata mesajÄ±nÄ± kontrol et ve tekrar Ã§alÄ±ÅŸtÄ±r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ğŸ—ï¸ HiyerarÅŸik Mask Detection - FULL VERSION (OPTIMIZED)\n",
    "\n",
    "## ğŸš€ YENÄ° Ã–ZELLÄ°KLER:\n",
    "- âœ… Mixed Precision Training (AMP) - %40 daha az VRAM, 2-3x daha hÄ±zlÄ±\n",
    "- âœ… Gradient Accumulation - BÃ¼yÃ¼k batch size simÃ¼lasyonu\n",
    "- âœ… Optimize DataLoader - Multi-worker, persistent workers, prefetch\n",
    "- âœ… BÃ¼yÃ¼k Batch Size - GPU gÃ¼cÃ¼nÃ¼ maksimum kullanma\n",
    "- âœ… CSV ve Extract aynÄ± konumda - Tek base path\n",
    "- âœ… PyTorch 2.0 Compile - %20-30 hÄ±z artÄ±ÅŸÄ±\n",
    "- âœ… Checkpoint Resume - EÄŸitim devam ettirme\n",
    "\n",
    "## Konsept:\n",
    "```\n",
    "Stage 1: Domain Classification (5 sÄ±nÄ±f)\n",
    "  â”œâ”€ URBAN (15 masks)\n",
    "  â”œâ”€ HUMAN_BODY (14 masks)\n",
    "  â”œâ”€ CLOTHING (7 masks)\n",
    "  â”œâ”€ INDOOR (8 masks)\n",
    "  â””â”€ BACKGROUND (5 masks)\n",
    "\n",
    "Stage 2: Fine-Grained Mask Detection\n",
    "  â”œâ”€ URBAN Model â†’ building, car, road, ...\n",
    "  â”œâ”€ HUMAN_BODY Model â†’ hair, face, skin, ...\n",
    "  â”œâ”€ CLOTHING Model â†’ Upper-clothes, Pants, ...\n",
    "  â”œâ”€ INDOOR Model â†’ floor, ceiling, chair, ...\n",
    "  â””â”€ BACKGROUND Model â†’ background, ego vehicle, ...\n",
    "```\n",
    "\n",
    "## Coverage: ~77% (Top-50 eÅŸdeÄŸer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”§ HÃœCRE 1 - SETUP VE IMPORTS (OPTIMIZED)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.cuda.amp import autocast, GradScaler  # âœ¨ YENÄ°: Mixed Precision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from utils import *\n",
    "\n",
    "# âœ¨ YENÄ°: GPU OptimizasyonlarÄ±\n",
    "cudnn.benchmark = True  # En hÄ±zlÄ± conv algoritmasÄ±nÄ± otomatik seÃ§\n",
    "cudnn.deterministic = False  # Deterministiklik yerine hÄ±z Ã¶ncelikli\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # TF32 precision (A100+ iÃ§in)\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"ğŸ”¥ GPU Ä±sÄ±ndÄ±rÄ±lÄ±yor ve optimize ediliyor...\")\n",
    "dummy = torch.randn(2000, 2000).cuda()  # Daha bÃ¼yÃ¼k warm-up\n",
    "for _ in range(20):  # Daha fazla iterasyon\n",
    "    _ = torch.matmul(dummy, dummy)\n",
    "torch.cuda.synchronize()\n",
    "del dummy\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ… GPU hazÄ±r ve optimize edildi!\")\n",
    "\n",
    "# =============================================\n",
    "# âš™ï¸ AYARLAR - OPTIMIZED\n",
    "# =============================================\n",
    "\n",
    "NOTEBOOK_NAME = \"hierarchical_mask_detection\"\n",
    "\n",
    "# âœ¨ RTX 5080 Ä°Ã‡Ä°N OPTÄ°MÄ°ZE AYARLAR (16GB VRAM, 24C/32T CPU, 64GB RAM)\n",
    "BATCH_SIZE = 64  # RTX 5080 16GB VRAM iÃ§in optimal\n",
    "\n",
    "# âœ¨ Gradient Accumulation (Effective batch = BATCH_SIZE * ACCUMULATION_STEPS)\n",
    "ACCUMULATION_STEPS = 2  # Effective batch = 64 * 2 = 128\n",
    "\n",
    "# âœ¨ DataLoader Ä°ÅŸÃ§i SayÄ±sÄ± (Windows iÃ§in optimize)\n",
    "# Windows'ta multi-worker sorunlu olabilir, 0 kullan (CPU-GPU dengesi iÃ§in)\n",
    "NUM_WORKERS = 0  # Windows'ta 0 Ã¶nerilir (CPU'yu meÅŸgul etmez)\n",
    "\n",
    "# EÄŸitim ayarlarÄ±\n",
    "NUM_EPOCHS_STAGE1 = 6  # Domain classifier\n",
    "NUM_EPOCHS_STAGE2 = 5  # Domain-specific models\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOP_PATIENCE = 8\n",
    "IMG_SIZE = 224\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# âœ¨ YENÄ°: CSV ve Extract aynÄ± konumda\n",
    "CSV_BASE = r\"C:\\AI_DATA\\SEMI_TRUTHS\\inpainting\"  # âš ï¸ CSV dosyalarÄ±nÄ±n konumu\n",
    "EXTRACTED_BASE = r\"C:\\AI_DATA\\SEMI_TRUTHS_extracted\\inpainting\"  # âš ï¸ Extract edilmiÅŸ gÃ¶rsellerin konumu\n",
    "\n",
    "# âœ¨ Mixed Precision AyarlarÄ±\n",
    "USE_AMP = True  # Automatic Mixed Precision (FP16) - MUTLAKA True (VRAM %40 azaltÄ±r)\n",
    "USE_COMPILE = False  # PyTorch 2.0+ compile - RTX 5080 tam desteklenene kadar False\n",
    "\n",
    "# âœ¨ YENÄ°: Checkpoint/Resume\n",
    "RESUME_TRAINING = False  # EÄŸitimi devam ettir\n",
    "SAVE_EVERY_N_EPOCHS = 1  # Her N epoch'ta checkpoint kaydet\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ§ª TEST AYARLARI - KOLAY AYARLANABILIR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# ğŸ“Š TEST VERÄ°SÄ° KULLANIM ORANI (0.0 - 1.0 arasÄ±)\n",
    "# Ã–rnek: 0.25 = %25, 0.5 = %50, 1.0 = %100 (tÃ¼m veri)\n",
    "TEST_DATA_RATIO = 0.25  # ğŸ”§ Buradan kolayca ayarla!\n",
    "\n",
    "# ğŸ”„ MULTI-EXPERT ROUTING (URBAN/INDOOR/BACKGROUND karÄ±ÅŸÄ±klÄ±ÄŸÄ±nÄ± Ã§Ã¶zmek iÃ§in)\n",
    "USE_MULTI_EXPERT_ROUTING = True  # Multi-expert routing aktif mi?\n",
    "CONFUSING_DOMAINS = ['URBAN', 'INDOOR', 'BACKGROUND']  # KarÄ±ÅŸan domain'ler\n",
    "ROUTING_THRESHOLD = 0.90  # Bu threshold'un altÄ±nda multi-expert devreye girer\n",
    "\n",
    "# âš–ï¸ HYBRID SCORING PARAMETRELERÄ°\n",
    "# Score = Î± Ã— P(domain) + (1-Î±) Ã— P(mask) Ã— confidence_factor\n",
    "USE_DYNAMIC_ALPHA = True  # âœ¨ YENÄ°: Domain'e gÃ¶re dinamik alpha kullan\n",
    "HYBRID_ALPHA = 0.3  # 0.3 = Stage2'ye daha fazla gÃ¼ven (USE_DYNAMIC_ALPHA=False ise kullanÄ±lÄ±r)\n",
    "                    # 0.5 = EÅŸit aÄŸÄ±rlÄ±k\n",
    "                    # 0.7 = Stage1'e daha fazla gÃ¼ven\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ® HÄ°YERARÅÄ°K MASK DETECTION - RTX 5080 OPTIMIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"ğŸ”§ CUDA Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "print(f\"ğŸ”¥ CUDA Cores: ~{torch.cuda.get_device_properties(0).multi_processor_count * 128}\")\n",
    "print(\"â”€\"*70)\n",
    "print(f\"ğŸ“Š PERFORMANS AYARLARI (RTX 5080 Optimized):\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Gradient Accumulation: {ACCUMULATION_STEPS} (Effective Batch: {BATCH_SIZE * ACCUMULATION_STEPS})\")\n",
    "print(f\"   â€¢ DataLoader Workers: {NUM_WORKERS} / 32 threads\")\n",
    "print(f\"   â€¢ Mixed Precision (AMP): {'âœ… Aktif (VRAM tasarruf: %40)' if USE_AMP else 'âŒ KapalÄ±'}\")\n",
    "print(f\"   â€¢ PyTorch Compile: {'âœ… Aktif' if USE_COMPILE else 'âŒ KapalÄ± (RTX 5080 uyumluluk bekleniyor)'}\")\n",
    "print(\"â”€\"*70)\n",
    "print(f\"ğŸ¯ EÄÄ°TÄ°M AYARLARI:\")\n",
    "print(f\"   â€¢ Stage 1 Epochs: {NUM_EPOCHS_STAGE1}\")\n",
    "print(f\"   â€¢ Stage 2 Epochs: {NUM_EPOCHS_STAGE2}\")\n",
    "print(f\"   â€¢ Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   â€¢ Early Stop Patience: {EARLY_STOP_PATIENCE}\")\n",
    "print(\"â”€\"*70)\n",
    "print(f\"ğŸ“‚ VERÄ° KONUMLARI:\")\n",
    "print(f\"   â€¢ CSV Base: {CSV_BASE}\")\n",
    "print(f\"   â€¢ Extracted Base: {EXTRACTED_BASE}\")\n",
    "print(f\"   âš ï¸  KonumlarÄ± gÃ¼ncelleyin!\")\n",
    "print(\"â”€\"*70)\n",
    "print(f\"âš¡ TAHMÄ°NÄ° PERFORMANS (RTX 5080):\")\n",
    "print(f\"   â€¢ Epoch sÃ¼resi: ~2-3 dakika (80K gÃ¶rÃ¼ntÃ¼)\")\n",
    "print(f\"   â€¢ GPU kullanÄ±mÄ±: %95+\")\n",
    "print(f\"   â€¢ VRAM kullanÄ±mÄ±: ~9-10GB / 16GB\")\n",
    "print(f\"   â€¢ Throughput: ~800-1000 img/sec\")\n",
    "print(\"â”€\"*70)\n",
    "print(f\"ğŸ§ª TEST AYARLARI:\")\n",
    "print(f\"   â€¢ Test Veri OranÄ±: {TEST_DATA_RATIO*100:.0f}%\")\n",
    "print(f\"   â€¢ Multi-Expert Routing: {'âœ… Aktif' if USE_MULTI_EXPERT_ROUTING else 'âŒ KapalÄ±'}\")\n",
    "if USE_MULTI_EXPERT_ROUTING:\n",
    "    print(f\"   â€¢ KarÄ±ÅŸan Domain'ler: {CONFUSING_DOMAINS}\")\n",
    "    print(f\"   â€¢ Routing Threshold: {ROUTING_THRESHOLD}\")\n",
    "    if USE_DYNAMIC_ALPHA:\n",
    "        print(f\"   â€¢ Hybrid Alpha: {'âœ… Dinamik (Domain bazlÄ±)'}\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Hybrid Alpha: {HYBRID_ALPHA} (Stage1 aÄŸÄ±rlÄ±ÄŸÄ± - Sabit)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# âœ¨ YENÄ°: Mixed Precision Scaler\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "print(\"\\nâœ… GradScaler hazÄ±r (Mixed Precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“‹ HÃœCRE 2 - DOMAIN KATEGORÄ°LERÄ°\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‹ DOMAIN KATEGORÄ°LERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "DOMAIN_CATEGORIES = {\n",
    "    'URBAN': [\n",
    "        'building', 'car', 'road', 'sidewalk', 'vegetation', 'sky', \n",
    "        'wall', 'window', 'door', 'tree', 'trees', 'pole', 'fence', \n",
    "        'parking', 'terrain'\n",
    "    ],\n",
    "    \n",
    "    'HUMAN_BODY': [\n",
    "        'hair', 'face', 'skin', 'neck', 'nose', 'Right-arm', 'Left-arm',\n",
    "        'Right-leg', 'Left-leg', 'left_ear', 'right_ear', 'mouth', \n",
    "        'upper_lip', 'lower_lip'\n",
    "    ],\n",
    "    \n",
    "    'CLOTHING': [\n",
    "        'Upper-clothes', 'Pants', 'skirt', 'dress', 'hat', 'bag', 'cloth'\n",
    "    ],\n",
    "    \n",
    "    'INDOOR': [\n",
    "        'floor', 'ceiling', 'chair', 'table', 'seat', 'column', 'ground', 'grass'\n",
    "    ],\n",
    "    \n",
    "    'BACKGROUND': [\n",
    "        'background', 'out of roi', 'ego vehicle', 'rectification border', 'static'\n",
    "    ]\n",
    "}\n",
    "\n",
    "DOMAIN_NAMES = list(DOMAIN_CATEGORIES.keys())\n",
    "NUM_DOMAINS = len(DOMAIN_NAMES)\n",
    "\n",
    "print(f\"\\nâœ… {NUM_DOMAINS} Domain TanÄ±mlandÄ±:\")\n",
    "total_masks = 0\n",
    "for i, (domain, masks) in enumerate(DOMAIN_CATEGORIES.items()):\n",
    "    print(f\"   [{i}] {domain:<15} â†’ {len(masks):2d} masks\")\n",
    "    total_masks += len(masks)\n",
    "\n",
    "print(f\"\\n   TOPLAM: {total_masks} mask tÃ¼rÃ¼\")\n",
    "\n",
    "def get_domain(mask_name):\n",
    "    for domain, masks in DOMAIN_CATEGORIES.items():\n",
    "        if mask_name in masks:\n",
    "            return domain\n",
    "    return None\n",
    "\n",
    "print(\"\\nâœ… get_domain() fonksiyonu tanÄ±mlandÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š HÃœCRE 3 - CSV YÃœKLEME VE DOMAIN ETÄ°KETLEME\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š CSV YÃœKLEMESÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "csv_files = glob.glob(os.path.join(CSV_BASE, \"**\", \"*_meta.csv\"), recursive=True)\n",
    "print(f\"\\nâœ… Bulunan CSV: {len(csv_files)} dosya\")\n",
    "\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"âŒ CSV dosyasÄ± bulunamadÄ±: {CSV_BASE}\")\n",
    "\n",
    "all_dfs = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    csv_dir = os.path.dirname(csv_file)\n",
    "    parent_folder = os.path.basename(os.path.dirname(csv_dir))\n",
    "    model_name = os.path.basename(csv_dir)\n",
    "    df['parent_dataset'] = parent_folder\n",
    "    df['model'] = model_name\n",
    "    df['dataset'] = f\"{parent_folder}_{model_name}\"\n",
    "    all_dfs.append(df)\n",
    "    print(f\"   âœ… {parent_folder}/{model_name}: {len(df):,} satÄ±r\")\n",
    "\n",
    "df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\nâœ… Toplam: {len(df_combined):,} gÃ¶rsel\")\n",
    "\n",
    "# Domain etiketleme\n",
    "df_combined['domain'] = df_combined['mask_name'].apply(get_domain)\n",
    "df_with_domain = df_combined[df_combined['domain'].notna()].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Domain DaÄŸÄ±lÄ±mÄ±:\")\n",
    "domain_counts = df_with_domain['domain'].value_counts()\n",
    "for domain in DOMAIN_NAMES:\n",
    "    count = domain_counts.get(domain, 0)\n",
    "    ratio = (count / len(df_with_domain)) * 100\n",
    "    print(f\"   {domain:<15}: {count:>6,} ({ratio:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Domain etiketlendi: {len(df_with_domain):,} / {len(df_combined):,} gÃ¶rsel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ—ï¸ HÃœCRE 4 - MODEL MÄ°MARÄ°LERÄ°\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ—ï¸ MODEL MÄ°MARÄ°LERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    \"\"\"Stage 1: Domain sÄ±nÄ±flandÄ±rÄ±cÄ±\"\"\"\n",
    "    \n",
    "    def __init__(self, num_domains=5, dropout_rate=0.3):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        backbone_out = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(backbone_out, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "class DomainMaskDetector(nn.Module):\n",
    "    \"\"\"Stage 2: Domain-specific mask detector\"\"\"\n",
    "    \n",
    "    def __init__(self, num_masks, dropout_rate=0.3):\n",
    "        super(DomainMaskDetector, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        backbone_out = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(backbone_out, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_masks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "print(\"\\nâœ… DomainClassifier tanÄ±mlandÄ±\")\n",
    "print(\"âœ… DomainMaskDetector tanÄ±mlandÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ HÃœCRE 6 - STAGE 1 DATASET (Domain Classification) - RECURSIVE FIX\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‚ STAGE 1 DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Domain label mapping\n",
    "domain_to_label = {domain: idx for idx, domain in enumerate(DOMAIN_NAMES)}\n",
    "label_to_domain = {idx: domain for domain, idx in domain_to_label.items()}\n",
    "\n",
    "print(\"\\nDomain â†’ Label:\")\n",
    "for domain, label in domain_to_label.items():\n",
    "    print(f\"   [{label}] {domain}\")\n",
    "\n",
    "# Etiket sÃ¶zlÃ¼ÄŸÃ¼\n",
    "labels_dict_by_dataset = {}\n",
    "for _, row in df_with_domain.iterrows():\n",
    "    img_id = str(row['perturbed_img_id'])\n",
    "    domain = row['domain']\n",
    "    label = domain_to_label[domain]\n",
    "    dataset_key = row['dataset']\n",
    "    \n",
    "    if dataset_key not in labels_dict_by_dataset:\n",
    "        labels_dict_by_dataset[dataset_key] = {}\n",
    "    \n",
    "    for ext in ['', '.png', '.jpg', '.jpeg']:\n",
    "        labels_dict_by_dataset[dataset_key][img_id + ext] = label\n",
    "\n",
    "print(f\"\\nâœ… Etiket dictionary oluÅŸturuldu: {len(labels_dict_by_dataset)} dataset\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ” RECURSIVE DOSYA TARAMA (os.walk kullanarak)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nğŸ“‚ Extract edilmiÅŸ dosyalar taranÄ±yor (recursive)...\")\n",
    "\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "found_by_dataset = {}\n",
    "\n",
    "# Her dataset klasÃ¶rÃ¼nÃ¼ tara\n",
    "for parent_dataset in os.listdir(EXTRACTED_BASE):\n",
    "    parent_path = os.path.join(EXTRACTED_BASE, parent_dataset)\n",
    "    if not os.path.isdir(parent_path):\n",
    "        continue\n",
    "    \n",
    "    for model_name in os.listdir(parent_path):\n",
    "        model_path = os.path.join(parent_path, model_name)\n",
    "        if not os.path.isdir(model_path):\n",
    "            continue\n",
    "        \n",
    "        dataset_key = f\"{parent_dataset}_{model_name}\"\n",
    "        \n",
    "        # Bu dataset iÃ§in etiket sÃ¶zlÃ¼ÄŸÃ¼ var mÄ±?\n",
    "        if dataset_key not in labels_dict_by_dataset:\n",
    "            continue\n",
    "        \n",
    "        labels_dict = labels_dict_by_dataset[dataset_key]\n",
    "        found_by_dataset[dataset_key] = 0\n",
    "        \n",
    "        print(f\"\\nğŸ” {dataset_key}\")\n",
    "        print(f\"   KlasÃ¶r: {model_path}\")\n",
    "        \n",
    "        # ğŸ”‘ æ ¸å¿ƒ: os.walk ile recursive tara (alt klasÃ¶rler dahil)\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    # Dosya adÄ± varyasyonlarÄ±\n",
    "                    base_name = file\n",
    "                    name_without_ext = os.path.splitext(file)[0]\n",
    "                    \n",
    "                    label = None\n",
    "                    \n",
    "                    # FarklÄ± adlarla dene\n",
    "                    for name in [base_name, name_without_ext,\n",
    "                               name_without_ext + '.png',\n",
    "                               name_without_ext + '.jpg',\n",
    "                               name_without_ext + '.jpeg']:\n",
    "                        if name in labels_dict:\n",
    "                            label = labels_dict[name]\n",
    "                            break\n",
    "                    \n",
    "                    if label is not None:\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        all_image_paths.append(full_path)\n",
    "                        all_labels.append(label)\n",
    "                        found_by_dataset[dataset_key] += 1\n",
    "        \n",
    "        if dataset_key in found_by_dataset:\n",
    "            print(f\"   âœ… {found_by_dataset[dataset_key]:,} gÃ¶rÃ¼ntÃ¼ bulundu\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… TOPLAM: {len(all_image_paths):,} gÃ¶rÃ¼ntÃ¼ bulundu\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if len(all_image_paths) == 0:\n",
    "    print(\"\\nâŒ HATA: HiÃ§ gÃ¶rÃ¼ntÃ¼ bulunamadÄ±!\")\n",
    "    print(\"\\nğŸ“‹ Bulunan dataset'ler:\")\n",
    "    for dataset_key in labels_dict_by_dataset.keys():\n",
    "        print(f\"   - {dataset_key}: {len(labels_dict_by_dataset[dataset_key]):,} etiket\")\n",
    "    raise FileNotFoundError(\"GÃ¶rÃ¼ntÃ¼ dosyalarÄ± bulunamadÄ±!\")\n",
    "\n",
    "# Dataset bazÄ±nda istatistik\n",
    "print(f\"\\nğŸ“Š Dataset BazÄ±nda Bulunan:\")\n",
    "for dataset_key in sorted(found_by_dataset.keys()):\n",
    "    expected = len([k for k in labels_dict_by_dataset[dataset_key].keys() if not k.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    found = found_by_dataset[dataset_key]\n",
    "    ratio = (found / expected * 100) if expected > 0 else 0\n",
    "    print(f\"   {dataset_key:<35}: {found:>6,} / {expected:>6,} ({ratio:>5.1f}%)\")\n",
    "\n",
    "# Dataset class\n",
    "class DomainDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            # Hata durumunda rastgele baÅŸka Ã¶rnek dÃ¶ndÃ¼r\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Train/Val split\n",
    "indices = list(range(len(all_image_paths)))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "split_idx = int(len(indices) * (1 - VAL_SPLIT))\n",
    "train_indices = indices[:split_idx]\n",
    "val_indices = indices[split_idx:]\n",
    "\n",
    "train_paths_s1 = [all_image_paths[i] for i in train_indices]\n",
    "train_labels_s1 = [all_labels[i] for i in train_indices]\n",
    "val_paths_s1 = [all_image_paths[i] for i in val_indices]\n",
    "val_labels_s1 = [all_labels[i] for i in val_indices]\n",
    "\n",
    "print(f\"\\nâœ… Train/Val Split:\")\n",
    "print(f\"   Train: {len(train_paths_s1):,} gÃ¶rÃ¼ntÃ¼\")\n",
    "print(f\"   Val: {len(val_paths_s1):,} gÃ¶rÃ¼ntÃ¼\")\n",
    "\n",
    "# ğŸ”’ STAGE 1 val_indices'i KORU (Cell 8'de override edilmesin!)\n",
    "stage1_val_indices = val_indices.copy()\n",
    "stage1_all_image_paths = all_image_paths.copy()\n",
    "stage1_all_labels = all_labels.copy()\n",
    "\n",
    "print(f\"\\nğŸ”’ Stage 1 validation set korundu:\")\n",
    "print(f\"   stage1_val_indices: {len(stage1_val_indices):,} Ã¶rnek\")\n",
    "print(f\"   stage1_all_image_paths: {len(stage1_all_image_paths):,} dosya\")\n",
    "\n",
    "# âœ¨ OPTIMIZED DataLoaders\n",
    "train_dataset_s1 = DomainDataset(train_paths_s1, train_labels_s1, train_transform)\n",
    "val_dataset_s1 = DomainDataset(val_paths_s1, val_labels_s1, val_transform)\n",
    "\n",
    "train_loader_s1 = DataLoader(\n",
    "    train_dataset_s1, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,  # âœ¨ Multi-threaded loading\n",
    "    pin_memory=True, \n",
    "    drop_last=True,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # âœ¨ Worker'larÄ± canlÄ± tut\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None  # âœ¨ Ã–nden 2 batch yÃ¼kle\n",
    ")\n",
    "\n",
    "val_loader_s1 = DataLoader(\n",
    "    val_dataset_s1, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,  # âœ¨ Multi-threaded loading\n",
    "    pin_memory=True, \n",
    "    drop_last=False,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # âœ¨ Worker'larÄ± canlÄ± tut\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None  # âœ¨ Ã–nden 2 batch yÃ¼kle\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… OPTIMIZED DataLoader'lar oluÅŸturuldu:\")\n",
    "print(f\"   Train batches: {len(train_loader_s1)}\")\n",
    "print(f\"   Val batches: {len(val_loader_s1)}\")\n",
    "print(f\"   Workers: {NUM_WORKERS}\")\n",
    "print(f\"   Persistent Workers: {True if NUM_WORKERS > 0 else False}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… STAGE 1 DATASET HAZIR\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ HÃœCRE 8 - STAGE 2 DATASET HAZIRLIÄI (Domain-Specific Mask Detection)\n",
    "Her domain iÃ§in ayrÄ± dataset ve DataLoader hazÄ±rla\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‚ STAGE 2 DATASET HAZIRLIÄI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ HER DOMAIN Ä°Ã‡Ä°N MASK LABEL MAPPINGS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Domain-specific mask mappings oluÅŸturuluyor...\")\n",
    "\n",
    "domain_mask_mappings = {}\n",
    "\n",
    "for domain, masks in DOMAIN_CATEGORIES.items():\n",
    "    domain_mask_mappings[domain] = {\n",
    "        'masks': masks,\n",
    "        'mask_to_label': {mask: idx for idx, mask in enumerate(masks)},\n",
    "        'label_to_mask': {idx: mask for idx, mask in enumerate(masks)},\n",
    "        'num_masks': len(masks)\n",
    "    }\n",
    "\n",
    "print(\"\\nğŸ“Š Domain Mask Counts:\")\n",
    "for domain, info in domain_mask_mappings.items():\n",
    "    print(f\"   {domain:<15}: {info['num_masks']:>2} masks\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ HER DOMAIN Ä°Ã‡Ä°N DATASET FÄ°LTRELE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Her domain iÃ§in dataset filtreleniyor...\")\n",
    "\n",
    "# Dataset class tanÄ±mla (HÃ¼cre 6'daki ile aynÄ±)\n",
    "class DomainDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            # Hata durumunda rastgele baÅŸka Ã¶rnek dÃ¶ndÃ¼r\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "# Transforms (HÃ¼cre 6'daki ile aynÄ±)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Her domain iÃ§in etiket sÃ¶zlÃ¼kleri oluÅŸtur\n",
    "domain_labels_dicts = {}\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    print(f\"\\nğŸ” {domain} iÃ§in etiketler hazÄ±rlanÄ±yor...\")\n",
    "    \n",
    "    domain_masks = domain_mask_mappings[domain]['masks']\n",
    "    mask_to_label = domain_mask_mappings[domain]['mask_to_label']\n",
    "    \n",
    "    # Bu domain'e ait mask'lara sahip satÄ±rlarÄ± filtrele\n",
    "    domain_df = df_with_domain[\n",
    "        (df_with_domain['domain'] == domain) &\n",
    "        (df_with_domain['mask_name'].isin(domain_masks))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"   CSV'de {len(domain_df):,} Ã¶rnek bulundu\")\n",
    "    \n",
    "    # Etiket sÃ¶zlÃ¼ÄŸÃ¼ oluÅŸtur (dataset bazÄ±nda)\n",
    "    labels_dict_by_dataset = {}\n",
    "    \n",
    "    for _, row in domain_df.iterrows():\n",
    "        img_id = str(row['perturbed_img_id'])\n",
    "        mask_name = row['mask_name']\n",
    "        label = mask_to_label[mask_name]\n",
    "        dataset_key = row['dataset']\n",
    "        \n",
    "        if dataset_key not in labels_dict_by_dataset:\n",
    "            labels_dict_by_dataset[dataset_key] = {}\n",
    "        \n",
    "        # FarklÄ± uzantÄ±larla denemeler\n",
    "        for ext in ['', '.png', '.jpg', '.jpeg']:\n",
    "            labels_dict_by_dataset[dataset_key][img_id + ext] = label\n",
    "    \n",
    "    domain_labels_dicts[domain] = labels_dict_by_dataset\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ DOSYA TARAMA (RECURSIVE os.walk)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Dosyalar taranÄ±yor (recursive)...\")\n",
    "\n",
    "domain_image_data = {}\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    print(f\"\\nğŸ“‚ {domain}\")\n",
    "    \n",
    "    labels_dict_by_dataset = domain_labels_dicts[domain]\n",
    "    \n",
    "    all_image_paths = []\n",
    "    all_labels = []\n",
    "    found_by_dataset = {}\n",
    "    \n",
    "    # Her dataset klasÃ¶rÃ¼nÃ¼ tara\n",
    "    for parent_dataset in os.listdir(EXTRACTED_BASE):\n",
    "        parent_path = os.path.join(EXTRACTED_BASE, parent_dataset)\n",
    "        if not os.path.isdir(parent_path):\n",
    "            continue\n",
    "        \n",
    "        for model_name in os.listdir(parent_path):\n",
    "            model_path = os.path.join(parent_path, model_name)\n",
    "            if not os.path.isdir(model_path):\n",
    "                continue\n",
    "            \n",
    "            dataset_key = f\"{parent_dataset}_{model_name}\"\n",
    "            \n",
    "            # Bu dataset iÃ§in etiket sÃ¶zlÃ¼ÄŸÃ¼ var mÄ±?\n",
    "            if dataset_key not in labels_dict_by_dataset:\n",
    "                continue\n",
    "            \n",
    "            labels_dict = labels_dict_by_dataset[dataset_key]\n",
    "            found_by_dataset[dataset_key] = 0\n",
    "            \n",
    "            # ğŸ”‘ RECURSIVE TARAMA (os.walk)\n",
    "            for root, dirs, files in os.walk(model_path):\n",
    "                for file in files:\n",
    "                    if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        # Dosya adÄ± varyasyonlarÄ±\n",
    "                        base_name = file\n",
    "                        name_without_ext = os.path.splitext(file)[0]\n",
    "                        \n",
    "                        label = None\n",
    "                        \n",
    "                        # FarklÄ± adlarla dene\n",
    "                        for name in [base_name, name_without_ext,\n",
    "                                   name_without_ext + '.png',\n",
    "                                   name_without_ext + '.jpg',\n",
    "                                   name_without_ext + '.jpeg']:\n",
    "                            if name in labels_dict:\n",
    "                                label = labels_dict[name]\n",
    "                                break\n",
    "                        \n",
    "                        if label is not None:\n",
    "                            full_path = os.path.join(root, file)\n",
    "                            all_image_paths.append(full_path)\n",
    "                            all_labels.append(label)\n",
    "                            found_by_dataset[dataset_key] += 1\n",
    "    \n",
    "    # SonuÃ§larÄ± kaydet\n",
    "    domain_image_data[domain] = {\n",
    "        'paths': all_image_paths,\n",
    "        'labels': all_labels,\n",
    "        'found_by_dataset': found_by_dataset\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Toplam: {len(all_image_paths):,} gÃ¶rÃ¼ntÃ¼ bulundu\")\n",
    "    \n",
    "    if len(found_by_dataset) > 0:\n",
    "        for dataset_key, count in found_by_dataset.items():\n",
    "            print(f\"      â€¢ {dataset_key}: {count:,}\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ TRAIN/VAL SPLIT VE DATALOADER'LAR\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Train/Val split ve DataLoader'lar oluÅŸturuluyor...\")\n",
    "\n",
    "domain_datasets = {}\n",
    "domain_loaders = {}\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    print(f\"\\nğŸ“Š {domain}\")\n",
    "    \n",
    "    image_data = domain_image_data[domain]\n",
    "    all_paths = image_data['paths']\n",
    "    all_labels = image_data['labels']\n",
    "    \n",
    "    if len(all_paths) == 0:\n",
    "        print(f\"   âš ï¸  GÃ¶rÃ¼ntÃ¼ bulunamadÄ±, atlanÄ±yor\")\n",
    "        continue\n",
    "    \n",
    "    # Train/Val split\n",
    "    indices = list(range(len(all_paths)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    split_idx = int(len(indices) * (1 - VAL_SPLIT))\n",
    "    train_indices = indices[:split_idx]\n",
    "    val_indices = indices[split_idx:]\n",
    "    \n",
    "    train_paths = [all_paths[i] for i in train_indices]\n",
    "    train_labels = [all_labels[i] for i in train_indices]\n",
    "    val_paths = [all_paths[i] for i in val_indices]\n",
    "    val_labels = [all_labels[i] for i in val_indices]\n",
    "    \n",
    "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,}\")\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = DomainDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = DomainDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    # âœ¨ OPTIMIZED DataLoaders (Stage 2)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,  # âœ¨ Multi-threaded loading\n",
    "        pin_memory=True, \n",
    "        drop_last=True,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False,  # âœ¨ Worker'larÄ± canlÄ± tut\n",
    "        prefetch_factor=2 if NUM_WORKERS > 0 else None  # âœ¨ Ã–nden 2 batch yÃ¼kle\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,  # âœ¨ Multi-threaded loading\n",
    "        pin_memory=True, \n",
    "        drop_last=False,\n",
    "        persistent_workers=True if NUM_WORKERS > 0 else False,  # âœ¨ Worker'larÄ± canlÄ± tut\n",
    "        prefetch_factor=2 if NUM_WORKERS > 0 else None  # âœ¨ Ã–nden 2 batch yÃ¼kle\n",
    "    )\n",
    "    \n",
    "    # Kaydet\n",
    "    domain_datasets[domain] = {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset\n",
    "    }\n",
    "    \n",
    "    domain_loaders[domain] = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… STAGE 2 DATASET HAZIR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ã–zet\n",
    "print(f\"\\nğŸ“Š Ã–zet:\")\n",
    "print(f\"   HazÄ±r domain sayÄ±sÄ±: {len(domain_loaders)}\")\n",
    "for domain in domain_loaders.keys():\n",
    "    train_size = len(domain_datasets[domain]['train'])\n",
    "    val_size = len(domain_datasets[domain]['val'])\n",
    "    print(f\"   â€¢ {domain:<15}: {train_size:>6,} train + {val_size:>5,} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ KAPSAMLI HÄ°YERARÅÄ°K VALÄ°DATÄ°ON TEST SÄ°STEMÄ°\n",
    "\n",
    "## Bu bÃ¶lÃ¼mde:\n",
    "1. **HierarchicalDataset**: Hem domain hem mask label'Ä± iÃ§eren dataset\n",
    "2. **Validation Loader OluÅŸturma**: CSV'den mask mapping ile\n",
    "3. **Stage 1 â†’ Stage 2 Pipeline Test**: TÃ¼m validation verisi iÃ§in\n",
    "4. **Hata Analizi**: Hangi verilerin nerede yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±ldÄ±ÄŸÄ±\n",
    "5. **Profesyonel GÃ¶rselleÅŸtirmeler**:\n",
    "   - Confusion Matrices (Stage 1 ve Stage 2)\n",
    "   - Error Flow Sankey Diagram\n",
    "   - Per-Domain Accuracy Charts\n",
    "   - Confidence Distribution Plots\n",
    "   - Misclassified Sample Image Grids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”¬ HÃœCRE 18 - HÄ°YERARÅÄ°K DATASET VE HELPER FUNCTIONS\n",
    "Hem domain hem mask label'Ä±nÄ± iÃ§eren dataset sÄ±nÄ±fÄ±\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”¬ HÄ°YERARÅÄ°K VALÄ°DASYON TEST SÄ°STEMÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“¦ HÄ°YERARÅÄ°K DATASET CLASS\n",
    "# =============================================\n",
    "\n",
    "class HierarchicalValidationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Hem domain hem mask label'Ä±nÄ± dÃ¶ndÃ¼ren dataset.\n",
    "    Stage 1 ve Stage 2 testleri iÃ§in gerekli tÃ¼m bilgileri iÃ§erir.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, domain_labels, mask_names, mask_labels, \n",
    "                 domain_names_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of image file paths\n",
    "            domain_labels: List of domain label indices (0-4)\n",
    "            mask_names: List of actual mask names (string)\n",
    "            mask_labels: List of mask label indices (domain-specific)\n",
    "            domain_names_list: List of domain name strings (for reference)\n",
    "            transform: Image transforms\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.domain_labels = domain_labels\n",
    "        self.mask_names = mask_names\n",
    "        self.mask_labels = mask_labels\n",
    "        self.domain_names_list = domain_names_list\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Validate lengths\n",
    "        assert len(image_paths) == len(domain_labels) == len(mask_names) == len(mask_labels), \\\n",
    "            f\"Length mismatch! paths:{len(image_paths)}, domains:{len(domain_labels)}, \" \\\n",
    "            f\"mask_names:{len(mask_names)}, mask_labels:{len(mask_labels)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            domain_label = self.domain_labels[idx]\n",
    "            mask_name = self.mask_names[idx]\n",
    "            mask_label = self.mask_labels[idx]\n",
    "            \n",
    "            return img, domain_label, mask_label, idx  # idx for tracking\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.image_paths[idx]}: {e}\")\n",
    "            # Fallback to random valid sample\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "# =============================================\n",
    "# ğŸ› ï¸ HELPER FUNCTIONS\n",
    "# =============================================\n",
    "\n",
    "def load_hierarchical_config(config_path):\n",
    "    \"\"\"Hierarchical config JSON'Ä± yÃ¼kle\"\"\"\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def create_stage2_mask_mappings(config):\n",
    "    \"\"\"Stage 2 mask mappings'i oluÅŸtur\"\"\"\n",
    "    mappings = {}\n",
    "    for domain_name, domain_info in config['stage2'].items():\n",
    "        mappings[domain_name] = {\n",
    "            'mask_to_label': domain_info['mask_to_label'],\n",
    "            'label_to_mask': {int(k): v for k, v in domain_info['label_to_mask'].items()},\n",
    "            'num_masks': domain_info['num_masks']\n",
    "        }\n",
    "    return mappings\n",
    "\n",
    "def get_mask_label_for_domain(mask_name, domain_name, stage2_mappings):\n",
    "    \"\"\"Belirli bir domain iÃ§in mask label'Ä± dÃ¶ndÃ¼r\"\"\"\n",
    "    if domain_name not in stage2_mappings:\n",
    "        return None\n",
    "    \n",
    "    mapping = stage2_mappings[domain_name]\n",
    "    if mask_name in mapping['mask_to_label']:\n",
    "        return mapping['mask_to_label'][mask_name]\n",
    "    return None\n",
    "\n",
    "print(\"âœ… HierarchicalValidationDataset sÄ±nÄ±fÄ± tanÄ±mlandÄ±\")\n",
    "print(\"âœ… Helper functions tanÄ±mlandÄ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“¦ HÃœCRE 19 - HÄ°YERARÅÄ°K VALÄ°DASYON LOADER OLUÅTURMA\n",
    "CSV'den mask bilgilerini alarak hierarchical validation loader hazÄ±rla\n",
    "\n",
    "NOT: Bu hÃ¼cre df_with_domain, all_image_paths, all_labels, val_indices \n",
    "deÄŸiÅŸkenlerinin tanÄ±mlÄ± olmasÄ±nÄ± gerektirir (Cell 3 ve Cell 6)\n",
    "\"\"\"\n",
    "\n",
    "import re  # regex iÃ§in\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“¦ HÄ°YERARÅÄ°K VALÄ°DASYON LOADER OLUÅTURMA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 0ï¸âƒ£ GEREKLÄ° DEÄÄ°ÅKENLERÄ° KONTROL ET\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n0ï¸âƒ£ Gerekli deÄŸiÅŸkenler kontrol ediliyor...\")\n",
    "\n",
    "required_vars = ['df_with_domain', 'stage1_all_image_paths', 'stage1_all_labels', 'stage1_val_indices', \n",
    "                 'domain_to_label', 'label_to_domain', 'DOMAIN_NAMES', 'DOMAIN_CATEGORIES']\n",
    "missing_vars = [v for v in required_vars if v not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"   âŒ Eksik deÄŸiÅŸkenler: {missing_vars}\")\n",
    "    print(\"   âš ï¸  Ã–nce Cell 1-6'yÄ± Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    raise NameError(f\"Eksik deÄŸiÅŸkenler: {missing_vars}\")\n",
    "else:\n",
    "    print(f\"   âœ… TÃ¼m gerekli deÄŸiÅŸkenler mevcut\")\n",
    "    print(f\"      df_with_domain: {len(df_with_domain):,} kayÄ±t\")\n",
    "    print(f\"      stage1_all_image_paths: {len(stage1_all_image_paths):,} dosya\")\n",
    "    print(f\"      stage1_val_indices: {len(stage1_val_indices):,} Ã¶rnek\")\n",
    "    \n",
    "    # ğŸ” BOYUT KONTROLÃœ\n",
    "    if len(df_with_domain) < 100000:\n",
    "        raise ValueError(f\"\\nâŒ df_with_domain Ã§ok kÃ¼Ã§Ã¼k! ({len(df_with_domain):,})\\n\"\n",
    "                         f\"   Beklenen: ~121,000 â†’ Cell 3'Ã¼ Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    \n",
    "    if len(stage1_all_image_paths) < 50000:\n",
    "        raise ValueError(f\"\\nâŒ stage1_all_image_paths Ã§ok kÃ¼Ã§Ã¼k! ({len(stage1_all_image_paths):,})\\n\"\n",
    "                         f\"   Beklenen: ~81,000 â†’ Cell 6'yÄ± Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    \n",
    "    if len(stage1_val_indices) < 10000:\n",
    "        raise ValueError(f\"\\nâŒ stage1_val_indices Ã§ok kÃ¼Ã§Ã¼k! ({len(stage1_val_indices):,})\\n\"\n",
    "                         f\"   Beklenen: ~16,000 â†’ Cell 6'yÄ± Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    \n",
    "    print(f\"      âœ… Boyut kontrolleri PASSED!\")\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ CONFIG VE MAPPINGS YÃœKLE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Config ve mappings yÃ¼kleniyor...\")\n",
    "\n",
    "config_path = os.path.join(\"eÄŸitim_sonuÃ§larÄ±\", NOTEBOOK_NAME, \"veriler\", \"hierarchical_config.json\")\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config bulunamadÄ±: {config_path}\")\n",
    "\n",
    "hierarchical_config = load_hierarchical_config(config_path)\n",
    "stage2_mappings = create_stage2_mask_mappings(hierarchical_config)\n",
    "\n",
    "print(f\"   âœ… Config yÃ¼klendi: {config_path}\")\n",
    "print(f\"   Stage 1 best accuracy: {hierarchical_config['stage1']['best_val_acc']*100:.2f}%\")\n",
    "\n",
    "for domain, info in stage2_mappings.items():\n",
    "    print(f\"   Stage 2 {domain}: {info['num_masks']} masks\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ IMAGE PATH â†’ MASK NAME MAPPING OLUÅTUR\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Image â†’ Mask mapping oluÅŸturuluyor...\")\n",
    "\n",
    "# df_with_domain zaten notebook'ta tanÄ±mlÄ± (Cell 3'ten)\n",
    "# Dataset bazlÄ± mask dictionary oluÅŸtur - perturbed_img_id â†’ mask_name\n",
    "mask_dict_by_dataset = {}\n",
    "\n",
    "for _, row in df_with_domain.iterrows():\n",
    "    img_id = str(row['perturbed_img_id'])\n",
    "    mask_name = row['mask_name']\n",
    "    dataset_key = row['dataset']\n",
    "    \n",
    "    if dataset_key not in mask_dict_by_dataset:\n",
    "        mask_dict_by_dataset[dataset_key] = {}\n",
    "    \n",
    "    # FarklÄ± key formatlarÄ± iÃ§in (Cell 6'daki mantÄ±kla aynÄ±)\n",
    "    for ext in ['', '.png', '.jpg', '.jpeg']:\n",
    "        mask_dict_by_dataset[dataset_key][img_id + ext] = mask_name\n",
    "\n",
    "print(f\"   âœ… {len(mask_dict_by_dataset)} dataset iÃ§in mask mapping oluÅŸturuldu\")\n",
    "for ds, masks in mask_dict_by_dataset.items():\n",
    "    unique_masks = len(set(masks.values()))\n",
    "    print(f\"      {ds}: {len(masks)//4:,} image, {unique_masks} unique mask\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ VALÄ°DATION SET Ä°Ã‡Ä°N LABEL'LARI HAZIRLA\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ TEST VERÄ°SÄ° HAZIRLANIYOR\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“Š TEST_DATA_RATIO ile test edilecek Ã¶rnek sayÄ±sÄ±nÄ± hesapla\n",
    "# Bu deÄŸiÅŸken Cell 2'de (Setup) tanÄ±mlÄ±\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "target_count = int(len(stage1_all_image_paths) * TEST_DATA_RATIO)\n",
    "target_count = max(1000, target_count)  # Minimum 1000 Ã¶rnek\n",
    "\n",
    "print(f\"   ğŸ“Š TEST_DATA_RATIO: {TEST_DATA_RATIO*100:.0f}%\")\n",
    "print(f\"   ğŸ“Š Hedef Ã¶rnek sayÄ±sÄ±: {target_count:,} / {len(stage1_all_image_paths):,} gÃ¶rsel\")\n",
    "print(f\"   âš ï¸  NOT: BazÄ± Ã¶rnekler atlanabilir (mask bulunamazsa)\")\n",
    "print(f\"   ğŸ’¡ OranÄ± deÄŸiÅŸtirmek iÃ§in Cell 2'de TEST_DATA_RATIO deÄŸiÅŸkenini gÃ¼ncelle\")\n",
    "\n",
    "val_image_paths = []\n",
    "val_domain_labels = []\n",
    "val_mask_names = []\n",
    "val_mask_labels = []\n",
    "\n",
    "skipped_no_domain = 0\n",
    "skipped_no_dataset = 0\n",
    "skipped_no_mask = 0\n",
    "skipped_no_mapping = 0\n",
    "\n",
    "# DetaylÄ± istatistik iÃ§in\n",
    "found_datasets = defaultdict(int)\n",
    "found_domains = defaultdict(int)\n",
    "\n",
    "# ğŸ”„ RANDOM SAMPLING: TÃ¼m Ã¶rneklerden random seÃ§ (daha dengeli daÄŸÄ±lÄ±m)\n",
    "# Ã–nce tÃ¼m geÃ§erli Ã¶rnekleri bul, sonra random seÃ§\n",
    "print(f\"\\n   ğŸ”„ Ã–rnekler taranÄ±yor ve geÃ§erli olanlar toplanÄ±yor...\")\n",
    "print(f\"   âš ï¸  Bu iÅŸlem biraz zaman alabilir...\")\n",
    "\n",
    "# TÃ¼m Ã¶rnekleri random shuffle et (daha dengeli dataset daÄŸÄ±lÄ±mÄ± iÃ§in)\n",
    "all_indices = list(range(len(stage1_all_image_paths)))\n",
    "random.seed(42)\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "# ğŸ”„ PROGRESS BAR: TÃ¼m Ã¶rnekleri tara, geÃ§erli olanlarÄ± topla\n",
    "for idx in tqdm(all_indices, \n",
    "                desc=f\"ğŸ”„ Scanning {len(all_indices):,} samples\", \n",
    "                total=len(all_indices), \n",
    "                ncols=100, \n",
    "                leave=True, \n",
    "                unit=\"img\"):\n",
    "    \n",
    "    # Hedef sayÄ±ya ulaÅŸtÄ±ysak dur\n",
    "    if len(val_image_paths) >= target_count:\n",
    "        break\n",
    "    img_path = stage1_all_image_paths[idx]\n",
    "    domain_label = stage1_all_labels[idx]\n",
    "    \n",
    "    # Domain name bul - label_to_domain'Ä±n key tipi deÄŸiÅŸebilir\n",
    "    domain_name = label_to_domain.get(domain_label) or label_to_domain.get(str(domain_label))\n",
    "    if domain_name is None and isinstance(domain_label, (np.integer, np.int64)):\n",
    "        domain_name = label_to_domain.get(int(domain_label))\n",
    "    \n",
    "    if domain_name is None:\n",
    "        skipped_no_domain += 1\n",
    "        continue\n",
    "    \n",
    "    # Image path'den dataset key'i Ã§Ä±kar\n",
    "    # Path format: .../inpainting/{parent}/{model}/.../{filename}\n",
    "    path_parts = img_path.replace('\\\\', '/').split('/')\n",
    "    \n",
    "    dataset_key = None\n",
    "    for i, part in enumerate(path_parts):\n",
    "        if part == 'inpainting' and i + 2 < len(path_parts):\n",
    "            parent = path_parts[i + 1]\n",
    "            model = path_parts[i + 2]\n",
    "            dataset_key = f\"{parent}_{model}\"\n",
    "            break\n",
    "    \n",
    "    if dataset_key is None or dataset_key not in mask_dict_by_dataset:\n",
    "        skipped_no_dataset += 1\n",
    "        continue\n",
    "    \n",
    "    found_datasets[dataset_key] += 1\n",
    "    \n",
    "    # Filename'den mask name bul\n",
    "    filename = os.path.basename(img_path)\n",
    "    filename_no_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    mask_dict = mask_dict_by_dataset[dataset_key]\n",
    "    mask_name = None\n",
    "    \n",
    "    # Key arama - Cell 6'daki mantÄ±kla aynÄ±\n",
    "    for key in [filename, filename_no_ext, \n",
    "                filename_no_ext + '.png', filename_no_ext + '.jpg', filename_no_ext + '.jpeg']:\n",
    "        if key in mask_dict:\n",
    "            mask_name = mask_dict[key]\n",
    "            break\n",
    "    \n",
    "    if mask_name is None:\n",
    "        skipped_no_mask += 1\n",
    "        continue\n",
    "    \n",
    "    # Mask label (domain-specific)\n",
    "    mask_label = get_mask_label_for_domain(mask_name, domain_name, stage2_mappings)\n",
    "    \n",
    "    if mask_label is None:\n",
    "        skipped_no_mapping += 1\n",
    "        continue\n",
    "    \n",
    "    found_domains[domain_name] += 1\n",
    "    \n",
    "    # TÃ¼m bilgileri ekle\n",
    "    val_image_paths.append(img_path)\n",
    "    val_domain_labels.append(domain_label)\n",
    "    val_mask_names.append(mask_name)\n",
    "    val_mask_labels.append(mask_label)\n",
    "\n",
    "print(f\"\\n   âœ… {len(val_image_paths):,} geÃ§erli Ã¶rnek hazÄ±r\")\n",
    "print(f\"   ğŸ“Š Hedef: {target_count:,} | Bulunan: {len(val_image_paths):,} | Fark: {target_count - len(val_image_paths):+,}\")\n",
    "\n",
    "total_skipped = skipped_no_domain + skipped_no_dataset + skipped_no_mask + skipped_no_mapping\n",
    "total_processed = len(val_image_paths) + total_skipped\n",
    "if total_skipped > 0:\n",
    "    print(f\"\\n   âš ï¸ Atlanan Ã¶rnek detaylarÄ± (toplam {total_processed:,} Ã¶rnek taranÄ±ldÄ±):\")\n",
    "    print(f\"      Domain bulunamadÄ±: {skipped_no_domain:,} ({skipped_no_domain/total_processed*100:.1f}%)\")\n",
    "    print(f\"      Dataset bulunamadÄ±: {skipped_no_dataset:,} ({skipped_no_dataset/total_processed*100:.1f}%)\")\n",
    "    print(f\"      Mask bulunamadÄ±: {skipped_no_mask:,} ({skipped_no_mask/total_processed*100:.1f}%)\")\n",
    "    print(f\"      Mask label bulunamadÄ±: {skipped_no_mapping:,} ({skipped_no_mapping/total_processed*100:.1f}%)\")\n",
    "    print(f\"      TOPLAM atlanan: {total_skipped:,} ({total_skipped/total_processed*100:.1f}%)\")\n",
    "    print(f\"      âœ… GeÃ§erli: {len(val_image_paths):,} ({len(val_image_paths)/total_processed*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Bulunan dataset daÄŸÄ±lÄ±mÄ±:\")\n",
    "for ds, count in sorted(found_datasets.items(), key=lambda x: -x[1]):\n",
    "    print(f\"      {ds}: {count:,}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Bulunan domain daÄŸÄ±lÄ±mÄ±:\")\n",
    "for dom, count in sorted(found_domains.items(), key=lambda x: -x[1]):\n",
    "    print(f\"      {dom}: {count:,}\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ DATASET VE DATALOADER OLUÅTUR\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Dataset ve DataLoader oluÅŸturuluyor...\")\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "hierarchical_val_dataset = HierarchicalValidationDataset(\n",
    "    image_paths=val_image_paths,\n",
    "    domain_labels=val_domain_labels,\n",
    "    mask_names=val_mask_names,\n",
    "    mask_labels=val_mask_labels,\n",
    "    domain_names_list=DOMAIN_NAMES,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# âœ¨ OPTIMIZED Hierarchical Validation DataLoader\n",
    "hierarchical_val_loader = DataLoader(\n",
    "    hierarchical_val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,  # âœ¨ Multi-threaded loading\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,  # âœ¨ Worker'larÄ± canlÄ± tut\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None  # âœ¨ Ã–nden 2 batch yÃ¼kle\n",
    ")\n",
    "\n",
    "print(f\"   âœ… OPTIMIZED DataLoader hazÄ±r\")\n",
    "print(f\"   Batch sayÄ±sÄ±: {len(hierarchical_val_loader)}\")\n",
    "print(f\"   Workers: {NUM_WORKERS}\")\n",
    "print(f\"   Toplam Ã¶rnek: {len(hierarchical_val_dataset):,}\")\n",
    "\n",
    "# =============================================\n",
    "# 5ï¸âƒ£ DOMAIN DAÄILIMI\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ Domain daÄŸÄ±lÄ±mÄ±:\")\n",
    "\n",
    "domain_dist = defaultdict(int)\n",
    "mask_dist_per_domain = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for domain_label, mask_name in zip(val_domain_labels, val_mask_names):\n",
    "    domain_name = label_to_domain.get(domain_label) or label_to_domain.get(str(domain_label))\n",
    "    domain_dist[domain_name] += 1\n",
    "    mask_dist_per_domain[domain_name][mask_name] += 1\n",
    "\n",
    "print(f\"\\n   {'Domain':<15} {'Samples':>10} {'Unique Masks':>12} {'%':>8}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "\n",
    "total_samples = len(val_domain_labels)\n",
    "for domain in DOMAIN_NAMES:\n",
    "    count = domain_dist[domain]\n",
    "    unique_masks = len(mask_dist_per_domain[domain])\n",
    "    pct = (count / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f\"   {domain:<15} {count:>10,} {unique_masks:>12} {pct:>7.1f}%\")\n",
    "\n",
    "print(f\"   {'-'*50}\")\n",
    "print(f\"   {'TOPLAM':<15} {total_samples:>10,}\")\n",
    "\n",
    "# =============================================\n",
    "# 6ï¸âƒ£ DOÄRULAMA TESTÄ° (OPTÄ°MÄ°ZE - ATLANDI)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ DoÄŸrulama testi...\")\n",
    "\n",
    "# âš¡ OPTÄ°MÄ°ZASYON: Ä°lk batch testi atlanÄ±yor (Cell 20'de test edilecek)\n",
    "# Windows'ta multi-worker baÅŸlatma yavaÅŸ olabilir, direkt Cell 20'ye geÃ§\n",
    "print(f\"   â­ï¸  Ä°lk batch testi atlanÄ±yor (Cell 20'de test edilecek)\")\n",
    "print(f\"   âœ… DataLoader hazÄ±r: {len(hierarchical_val_loader)} batch\")\n",
    "print(f\"   âœ… Toplam Ã¶rnek: {len(hierarchical_val_dataset):,}\")\n",
    "print(f\"   ğŸ’¡ GerÃ§ek test Cell 20'de yapÄ±lacak (model inference)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… HÄ°YERARÅÄ°K VALÄ°DASYON LOADER HAZIR!\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# âœ¨ YENÄ°: DÄ°NAMÄ°K ALPHA FONKSÄ°YONU\n",
    "# =============================================\n",
    "\n",
    "def get_dynamic_alpha(domain_name, stage2_models):\n",
    "    \"\"\"\n",
    "    âœ¨ YENÄ°: Domain'in Stage1 ve Stage2 accuracy'sine gÃ¶re dinamik alpha belirle\n",
    "    \n",
    "    Args:\n",
    "        domain_name: Domain adÄ± (URBAN, INDOOR, vb.)\n",
    "        stage2_models: Stage2 modelleri dict'i (val_acc bilgisi iÃ§erir)\n",
    "    \n",
    "    Returns:\n",
    "        alpha: Stage1 aÄŸÄ±rlÄ±ÄŸÄ± (0-1 arasÄ±)\n",
    "    \"\"\"\n",
    "    # Stage2 accuracy'yi al\n",
    "    if domain_name in stage2_models:\n",
    "        stage2_acc = stage2_models[domain_name].get('val_acc', 0.7)\n",
    "    else:\n",
    "        stage2_acc = 0.7  # VarsayÄ±lan\n",
    "    \n",
    "    # Domain'e gÃ¶re strateji (gerÃ§ek accuracy sonuÃ§larÄ±na gÃ¶re)\n",
    "    if domain_name == 'INDOOR':\n",
    "        # Stage1: 62.47%, Stage2: 87.32% â†’ Stage2'ye Ã§ok gÃ¼ven\n",
    "        return 0.2  # Stage1: %20, Stage2: %80\n",
    "    \n",
    "    elif domain_name == 'URBAN':\n",
    "        # Stage1: 84.05%, Stage2: 63.76% â†’ Stage1'e gÃ¼ven\n",
    "        return 0.6  # Stage1: %60, Stage2: %40\n",
    "    \n",
    "    elif domain_name == 'CLOTHING':\n",
    "        # Stage1: 7.27%, Stage2: 75.00% â†’ Stage2'ye Ã§ok gÃ¼ven\n",
    "        return 0.1  # Stage1: %10, Stage2: %90\n",
    "    \n",
    "    elif domain_name == 'HUMAN_BODY':\n",
    "        # Stage1: 10.53%, Stage2: 0.00% â†’ Her ikisi de kÃ¶tÃ¼, dengeli ama Stage2'ye biraz daha gÃ¼ven\n",
    "        return 0.4  # Stage1: %40, Stage2: %60\n",
    "    \n",
    "    elif domain_name == 'BACKGROUND':\n",
    "        # Stage1: 100%, Stage2: 0% â†’ Stage1'e gÃ¼ven\n",
    "        return 0.7  # Stage1: %70, Stage2: %30\n",
    "    \n",
    "    else:\n",
    "        # VarsayÄ±lan: Stage2 accuracy'sine gÃ¶re\n",
    "        if stage2_acc > 0.85:\n",
    "            return 0.2  # Stage2 Ã§ok iyi\n",
    "        elif stage2_acc > 0.70:\n",
    "            return 0.4  # Stage2 iyi\n",
    "        else:\n",
    "            return 0.6  # Stage2 kÃ¶tÃ¼ â†’ Stage1'e gÃ¼ven\n",
    "\n",
    "print(\"âœ… Dinamik alpha fonksiyonu tanÄ±mlandÄ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”„ HÃœCRE 20 - HÄ°YERARÅÄ°K TEST PIPELINE (STAGE 1 â†’ STAGE 2)\n",
    "âœ¨ YENÄ°: Multi-Expert Routing + Hybrid Scoring\n",
    "\n",
    "URBAN/INDOOR/BACKGROUND karÄ±ÅŸÄ±klÄ±ÄŸÄ±nÄ± Ã§Ã¶zmek iÃ§in:\n",
    "- Stage1 confidence dÃ¼ÅŸÃ¼kse VE karÄ±ÅŸan domain'lerdeyse\n",
    "- TÃ¼m karÄ±ÅŸan domain'lerin Stage2 modeline sor\n",
    "- Hybrid Score = Î± Ã— P(domain) + (1-Î±) Ã— P(mask) Ã— confidence_factor\n",
    "- En yÃ¼ksek hybrid score'u seÃ§\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”„ HÄ°YERARÅÄ°K TEST PIPELINE + MULTI-EXPERT ROUTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 0ï¸âƒ£ HYBRID SCORING FONKSÄ°YONU\n",
    "# =============================================\n",
    "\n",
    "def calculate_entropy(probs):\n",
    "    \"\"\"Probability daÄŸÄ±lÄ±mÄ±nÄ±n entropy'sini hesapla\"\"\"\n",
    "    probs = np.clip(probs, 1e-10, 1.0)  # Log(0) Ã¶nle\n",
    "    entropy = -np.sum(probs * np.log(probs))\n",
    "    return entropy\n",
    "\n",
    "def calculate_hybrid_score(domain_prob, mask_probs, alpha=0.3):\n",
    "    \"\"\"\n",
    "    Hybrid scoring: Stage1 + Stage2 + Entropy kombinasyonu\n",
    "    \n",
    "    Args:\n",
    "        domain_prob: Stage1'den gelen bu domain'in olasÄ±lÄ±ÄŸÄ±\n",
    "        mask_probs: Stage2'den gelen mask olasÄ±lÄ±klarÄ± (numpy array)\n",
    "        alpha: Stage1 aÄŸÄ±rlÄ±ÄŸÄ± (0-1 arasÄ±)\n",
    "    \n",
    "    Returns:\n",
    "        hybrid_score: Kombine skor\n",
    "        mask_confidence: En yÃ¼ksek mask olasÄ±lÄ±ÄŸÄ±\n",
    "        mask_pred: Tahmin edilen mask label\n",
    "        confidence_factor: Entropy bazlÄ± gÃ¼ven faktÃ¶rÃ¼\n",
    "    \"\"\"\n",
    "    # Mask prediction\n",
    "    mask_confidence = np.max(mask_probs)\n",
    "    mask_pred = np.argmax(mask_probs)\n",
    "    \n",
    "    # Entropy hesapla (dÃ¼ÅŸÃ¼k entropy = yÃ¼ksek gÃ¼ven)\n",
    "    entropy = calculate_entropy(mask_probs)\n",
    "    max_entropy = np.log(len(mask_probs))  # Uniform daÄŸÄ±lÄ±m entropy'si\n",
    "    \n",
    "    # Confidence factor: 0 (Ã§ok belirsiz) â†’ 1 (Ã§ok emin)\n",
    "    confidence_factor = 1 - (entropy / max_entropy) if max_entropy > 0 else 1.0\n",
    "    \n",
    "    # Hybrid score hesapla\n",
    "    # Î± Ã— P(domain) + (1-Î±) Ã— P(mask) Ã— confidence_factor\n",
    "    hybrid_score = alpha * domain_prob + (1 - alpha) * mask_confidence * confidence_factor\n",
    "    \n",
    "    return hybrid_score, mask_confidence, mask_pred, confidence_factor\n",
    "\n",
    "print(\"âœ… Hybrid scoring fonksiyonu tanÄ±mlandÄ±\")\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ STAGE 1 MODEL YÃœKLE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Stage 1 model yÃ¼kleniyor...\")\n",
    "\n",
    "stage1_model_path = hierarchical_config['stage1']['model_path']\n",
    "if not os.path.exists(stage1_model_path):\n",
    "    stage1_model_path = os.path.join(\"eÄŸitim_sonuÃ§larÄ±\", NOTEBOOK_NAME, \"stage1_domain\", \"models\", \"best_model.pth\")\n",
    "\n",
    "stage1_checkpoint = torch.load(stage1_model_path, map_location=device, weights_only=False)\n",
    "stage1_model = DomainClassifier(num_domains=NUM_DOMAINS).to(device)\n",
    "stage1_model.load_state_dict(stage1_checkpoint['model_state_dict'])\n",
    "stage1_model.eval()\n",
    "# âœ¨ GPU optimizasyonu: Model'i GPU'da tut ve inference iÃ§in hazÄ±rla\n",
    "torch.cuda.empty_cache()  # Ã–nceki cache'i temizle\n",
    "\n",
    "print(f\"   âœ… Stage 1 model yÃ¼klendi\")\n",
    "print(f\"   Path: {stage1_model_path}\")\n",
    "print(f\"   Val Accuracy: {stage1_checkpoint.get('val_acc', 0)*100:.2f}%\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ STAGE 2 MODELS YÃœKLE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Stage 2 models yÃ¼kleniyor...\")\n",
    "\n",
    "stage2_models = {}\n",
    "\n",
    "for domain_name in DOMAIN_NAMES:\n",
    "    model_path = hierarchical_config['stage2'][domain_name]['model_path']\n",
    "    if not os.path.exists(model_path):\n",
    "        model_path = os.path.join(\"eÄŸitim_sonuÃ§larÄ±\", NOTEBOOK_NAME, \n",
    "                                  f\"stage2_{domain_name.lower()}\", \"models\", \"best_model.pth\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"   âš ï¸  {domain_name}: Model bulunamadÄ±!\")\n",
    "        continue\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    num_masks = checkpoint['num_masks']\n",
    "    \n",
    "    model = DomainMaskDetector(num_masks=num_masks).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    # âœ¨ GPU optimizasyonu: Model'i GPU'da tut\n",
    "    \n",
    "    stage2_models[domain_name] = {\n",
    "        'model': model,\n",
    "        'num_masks': num_masks,\n",
    "        'label_to_mask': checkpoint['label_to_mask'],\n",
    "        'val_acc': checkpoint.get('val_acc', 0)\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… {domain_name}: {num_masks} masks, Acc={checkpoint.get('val_acc', 0)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n   Toplam {len(stage2_models)} Stage 2 model yÃ¼klendi\")\n",
    "# âœ¨ GPU cache temizle ve hazÄ±r ol\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()  # GPU iÅŸlemlerinin tamamlanmasÄ±nÄ± bekle\n",
    "\n",
    "# âœ¨ GPU kullanÄ±mÄ±nÄ± kontrol et\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"   âœ… TÃ¼m modeller GPU'da hazÄ±r\")\n",
    "    print(f\"   ğŸ“Š GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  CUDA kullanÄ±lamÄ±yor! CPU'da Ã§alÄ±ÅŸÄ±yor olabilir.\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ HÄ°YERARÅÄ°K TEST BAÅLAT (MULTI-EXPERT ROUTING Ä°LE)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Hierarchical test baÅŸlatÄ±lÄ±yor...\")\n",
    "print(f\"   Toplam Ã¶rnek: {len(hierarchical_val_dataset):,}\")\n",
    "print(f\"   Batch sayÄ±sÄ±: {len(hierarchical_val_loader)}\")\n",
    "print(f\"   ğŸš€ GPU OptimizasyonlarÄ±:\")\n",
    "print(f\"      â€¢ Mixed Precision (AMP): {'âœ… Aktif (FP16 - %40 VRAM tasarruf)' if USE_AMP else 'âŒ KapalÄ±'}\")\n",
    "print(f\"      â€¢ Non-blocking Transfer: âœ… Aktif (CPU-GPU overlap)\")\n",
    "print(f\"      â€¢ Device: {device}\")\n",
    "print(f\"      â€¢ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"      â€¢ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"      â€¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Multi-expert routing ayarlarÄ±nÄ± gÃ¶ster\n",
    "print(f\"\\n   ğŸ”„ MULTI-EXPERT ROUTING:\")\n",
    "print(f\"      Aktif: {'âœ… Evet' if USE_MULTI_EXPERT_ROUTING else 'âŒ HayÄ±r'}\")\n",
    "if USE_MULTI_EXPERT_ROUTING:\n",
    "    print(f\"      KarÄ±ÅŸan Domain'ler: {CONFUSING_DOMAINS}\")\n",
    "    print(f\"      Routing Threshold: {ROUTING_THRESHOLD}\")\n",
    "    print(f\"      Hybrid Alpha: {HYBRID_ALPHA}\")\n",
    "\n",
    "# SonuÃ§larÄ± saklamak iÃ§in listeler\n",
    "all_results = []\n",
    "\n",
    "# Ä°statistikler\n",
    "stats = {\n",
    "    'normal_routing': 0,       # Normal routing kullanÄ±ldÄ±\n",
    "    'multi_expert_routing': 0,  # Multi-expert routing kullanÄ±ldÄ±\n",
    "    'multi_expert_changed': 0,  # Multi-expert domain'i deÄŸiÅŸtirdi\n",
    "    'multi_expert_same': 0      # Multi-expert aynÄ± domain'i seÃ§ti\n",
    "}\n",
    "\n",
    "# âœ¨ GPU optimizasyonu: Inference iÃ§in hazÄ±r\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # âœ¨ Mixed Precision context manager (tÃ¼m inference iÃ§in)\n",
    "    for batch_idx, (images, true_domains, true_masks, sample_indices) in enumerate(\n",
    "        tqdm(hierarchical_val_loader, \n",
    "             desc=\"ğŸ”„ Hierarchical Testing + Multi-Expert\",\n",
    "             total=len(hierarchical_val_loader),\n",
    "             ncols=100,\n",
    "             leave=True,\n",
    "             unit=\"batch\",\n",
    "             bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\")\n",
    "    ):\n",
    "        images = images.to(device, non_blocking=True)  # âœ¨ Non-blocking transfer\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # ============================================\n",
    "        # STAGE 1: Domain Classification (GPU OPTIMIZED)\n",
    "        # ============================================\n",
    "        \n",
    "        # âœ¨ Mixed Precision (AMP) ile GPU'da iÅŸle\n",
    "        with autocast(enabled=USE_AMP):\n",
    "            domain_outputs = stage1_model(images)\n",
    "        \n",
    "        domain_probs_tensor = torch.softmax(domain_outputs, dim=1)\n",
    "        domain_confidences, pred_domains = torch.max(domain_probs_tensor, 1)\n",
    "        \n",
    "        # CPU'ya taÅŸÄ±\n",
    "        domain_probs_np = domain_probs_tensor.cpu().numpy()  # TÃ¼m domain olasÄ±lÄ±klarÄ±\n",
    "        pred_domains = pred_domains.cpu().numpy()\n",
    "        domain_confidences = domain_confidences.cpu().numpy()\n",
    "        true_domains_np = true_domains.numpy()\n",
    "        true_masks_np = true_masks.numpy()\n",
    "        sample_indices_np = sample_indices.numpy()\n",
    "        \n",
    "        # ============================================\n",
    "        # STAGE 2: Mask Classification\n",
    "        # âœ¨ YENÄ°: Multi-Expert Routing ile\n",
    "        # ============================================\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            img = images[i:i+1]  # Keep batch dimension\n",
    "            \n",
    "            true_domain_label = true_domains_np[i]\n",
    "            pred_domain_label = pred_domains[i]\n",
    "            domain_conf = domain_confidences[i]\n",
    "            true_mask_label = true_masks_np[i]\n",
    "            sample_idx = sample_indices_np[i]\n",
    "            all_domain_probs = domain_probs_np[i]  # TÃ¼m domain olasÄ±lÄ±klarÄ±\n",
    "            \n",
    "            # Domain names\n",
    "            true_domain_name = label_to_domain.get(true_domain_label) or label_to_domain.get(str(true_domain_label))\n",
    "            original_pred_domain_name = label_to_domain.get(pred_domain_label) or label_to_domain.get(str(pred_domain_label))\n",
    "            \n",
    "            # True mask name (dataset'ten)\n",
    "            true_mask_name = hierarchical_val_dataset.mask_names[sample_idx]\n",
    "            \n",
    "            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "            # ğŸ”„ MULTI-EXPERT ROUTING KARARI\n",
    "            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "            \n",
    "            use_multi_expert = False\n",
    "            final_pred_domain_name = original_pred_domain_name\n",
    "            routing_method = \"normal\"\n",
    "            \n",
    "            if USE_MULTI_EXPERT_ROUTING:\n",
    "                # KoÅŸul: DÃ¼ÅŸÃ¼k confidence VE karÄ±ÅŸan domain'lerden biri\n",
    "                if domain_conf < ROUTING_THRESHOLD and original_pred_domain_name in CONFUSING_DOMAINS:\n",
    "                    use_multi_expert = True\n",
    "            \n",
    "            if use_multi_expert:\n",
    "                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                # ğŸ¯ MULTI-EXPERT: TÃ¼m karÄ±ÅŸan domain'lere sor\n",
    "                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                \n",
    "                candidates = []\n",
    "                \n",
    "                for domain_name in CONFUSING_DOMAINS:\n",
    "                    if domain_name not in stage2_models:\n",
    "                        continue\n",
    "                    \n",
    "                    # Bu domain iÃ§in Stage2 Ã§Ä±ktÄ±sÄ± al (GPU'da Mixed Precision ile)\n",
    "                    s2_model = stage2_models[domain_name]['model']\n",
    "                    with autocast(enabled=USE_AMP):\n",
    "                        s2_outputs = s2_model(img)\n",
    "                    s2_probs = torch.softmax(s2_outputs, dim=1).cpu().numpy()[0]\n",
    "                    \n",
    "                    # Bu domain'in Stage1 olasÄ±lÄ±ÄŸÄ±nÄ± bul\n",
    "                    domain_idx = domain_to_label[domain_name]\n",
    "                    domain_prob = all_domain_probs[domain_idx]\n",
    "                    \n",
    "                    # Hybrid score hesapla (dinamik alpha ile)\n",
    "                    if USE_DYNAMIC_ALPHA:\n",
    "                        dynamic_alpha = get_dynamic_alpha(domain_name, stage2_models)\n",
    "                        hybrid_score, mask_conf, mask_pred, conf_factor = calculate_hybrid_score(\n",
    "                            domain_prob, s2_probs, alpha=dynamic_alpha\n",
    "                        )\n",
    "                    else:\n",
    "                        hybrid_score, mask_conf, mask_pred, conf_factor = calculate_hybrid_score(\n",
    "                            domain_prob, s2_probs, alpha=HYBRID_ALPHA\n",
    "                        )\n",
    "                    \n",
    "                    # Label to mask name\n",
    "                    l2m = stage2_models[domain_name]['label_to_mask']\n",
    "                    mask_name = l2m.get(int(mask_pred)) or l2m.get(str(mask_pred), \"UNKNOWN\")\n",
    "                    \n",
    "                    candidates.append({\n",
    "                        'domain': domain_name,\n",
    "                        'domain_prob': domain_prob,\n",
    "                        'mask_pred': int(mask_pred),\n",
    "                        'mask_name': mask_name,\n",
    "                        'mask_confidence': mask_conf,\n",
    "                        'confidence_factor': conf_factor,\n",
    "                        'hybrid_score': hybrid_score\n",
    "                    })\n",
    "                \n",
    "                # En yÃ¼ksek hybrid score'u seÃ§\n",
    "                if candidates:\n",
    "                    best = max(candidates, key=lambda x: x['hybrid_score'])\n",
    "                    \n",
    "                    final_pred_domain_name = best['domain']\n",
    "                    pred_mask_label = best['mask_pred']\n",
    "                    pred_mask_name = best['mask_name']\n",
    "                    mask_confidence = best['mask_confidence']\n",
    "                    routing_method = \"multi_expert\"\n",
    "                    \n",
    "                    stats['multi_expert_routing'] += 1\n",
    "                    \n",
    "                    if final_pred_domain_name != original_pred_domain_name:\n",
    "                        stats['multi_expert_changed'] += 1\n",
    "                    else:\n",
    "                        stats['multi_expert_same'] += 1\n",
    "                else:\n",
    "                    # Fallback: Normal routing\n",
    "                    use_multi_expert = False\n",
    "            \n",
    "            if not use_multi_expert:\n",
    "                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                # ğŸ“ NORMAL ROUTING: Sadece tahmin edilen domain'e sor\n",
    "                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                \n",
    "                pred_mask_label = -1\n",
    "                pred_mask_name = \"UNKNOWN\"\n",
    "                mask_confidence = 0.0\n",
    "                \n",
    "                if original_pred_domain_name in stage2_models:\n",
    "                    s2_model = stage2_models[original_pred_domain_name]['model']\n",
    "                    # âœ¨ Mixed Precision (AMP) ile GPU'da iÅŸle\n",
    "                    with autocast(enabled=USE_AMP):\n",
    "                        s2_outputs = s2_model(img)\n",
    "                    s2_probs = torch.softmax(s2_outputs, dim=1)\n",
    "                    mask_conf, mask_pred = torch.max(s2_probs, 1)\n",
    "                    \n",
    "                    pred_mask_label = mask_pred.item()\n",
    "                    mask_confidence = mask_conf.item()\n",
    "                    \n",
    "                    l2m = stage2_models[original_pred_domain_name]['label_to_mask']\n",
    "                    pred_mask_name = l2m.get(pred_mask_label) or l2m.get(str(pred_mask_label), \"UNKNOWN\")\n",
    "                \n",
    "                final_pred_domain_name = original_pred_domain_name\n",
    "                routing_method = \"normal\"\n",
    "                stats['normal_routing'] += 1\n",
    "            \n",
    "            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "            # ğŸ“Š SONUÃ‡LARI DEÄERLENDIR\n",
    "            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "            \n",
    "            # Final domain label'Ä± bul\n",
    "            final_pred_domain_label = domain_to_label.get(final_pred_domain_name, pred_domain_label)\n",
    "            \n",
    "            # Domain doÄŸruluÄŸu (multi-expert sonrasÄ±)\n",
    "            domain_correct = (final_pred_domain_label == true_domain_label)\n",
    "            \n",
    "            # Mask doÄŸruluÄŸu\n",
    "            if domain_correct:\n",
    "                expected_mask_label = get_mask_label_for_domain(true_mask_name, true_domain_name, stage2_mappings)\n",
    "                mask_correct = (pred_mask_label == expected_mask_label) if expected_mask_label is not None else False\n",
    "            else:\n",
    "                mask_correct = False\n",
    "            \n",
    "            # Sonucu kaydet\n",
    "            result = {\n",
    "                'sample_idx': sample_idx,\n",
    "                'image_path': hierarchical_val_dataset.image_paths[sample_idx],\n",
    "                'true_domain_label': true_domain_label,\n",
    "                'original_pred_domain_label': pred_domain_label,  # Stage1'in orijinal tahmini\n",
    "                'final_pred_domain_label': final_pred_domain_label,  # Multi-expert sonrasÄ±\n",
    "                'true_domain_name': true_domain_name,\n",
    "                'original_pred_domain_name': original_pred_domain_name,\n",
    "                'final_pred_domain_name': final_pred_domain_name,\n",
    "                'domain_confidence': domain_conf,\n",
    "                'domain_correct': domain_correct,\n",
    "                'true_mask_label': true_mask_label,\n",
    "                'pred_mask_label': pred_mask_label,\n",
    "                'true_mask_name': true_mask_name,\n",
    "                'pred_mask_name': pred_mask_name,\n",
    "                'mask_confidence': mask_confidence,\n",
    "                'mask_correct': mask_correct,\n",
    "                'pipeline_correct': domain_correct and mask_correct,\n",
    "                'routing_method': routing_method  # \"normal\" veya \"multi_expert\"\n",
    "            }\n",
    "            \n",
    "            all_results.append(result)\n",
    "\n",
    "# DataFrame oluÅŸtur\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\n   âœ… Test tamamlandÄ±: {len(df_results):,} Ã¶rnek\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ MULTI-EXPERT ROUTING Ä°STATÄ°STÄ°KLERÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Multi-Expert Routing Ä°statistikleri:\")\n",
    "print(f\"   {'-'*50}\")\n",
    "print(f\"   Normal Routing kullanÄ±ldÄ±:        {stats['normal_routing']:>8,}\")\n",
    "print(f\"   Multi-Expert Routing kullanÄ±ldÄ±:  {stats['multi_expert_routing']:>8,}\")\n",
    "if stats['multi_expert_routing'] > 0:\n",
    "    print(f\"      â†’ Domain DEÄÄ°ÅTÄ°:              {stats['multi_expert_changed']:>8,}\")\n",
    "    print(f\"      â†’ Domain AYNI kaldÄ±:           {stats['multi_expert_same']:>8,}\")\n",
    "    change_rate = stats['multi_expert_changed'] / stats['multi_expert_routing'] * 100\n",
    "    print(f\"      â†’ DeÄŸiÅŸtirme oranÄ±:            {change_rate:>7.1f}%\")\n",
    "\n",
    "# =============================================\n",
    "# 5ï¸âƒ£ TEMEL METRÄ°KLER\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ Temel metrikler hesaplanÄ±yor...\")\n",
    "\n",
    "total_samples = len(df_results)\n",
    "domain_correct_count = df_results['domain_correct'].sum()\n",
    "domain_accuracy = domain_correct_count / total_samples\n",
    "\n",
    "# Mask accuracy (sadece domain doÄŸru olanlar iÃ§in)\n",
    "df_domain_correct = df_results[df_results['domain_correct'] == True]\n",
    "mask_correct_count = df_domain_correct['mask_correct'].sum()\n",
    "mask_accuracy_on_correct = mask_correct_count / len(df_domain_correct) if len(df_domain_correct) > 0 else 0\n",
    "\n",
    "# Full pipeline accuracy\n",
    "pipeline_correct_count = df_results['pipeline_correct'].sum()\n",
    "pipeline_accuracy = pipeline_correct_count / total_samples\n",
    "\n",
    "print(f\"\\n   ğŸ“Š DOMAIN CLASSIFICATION (Multi-Expert sonrasÄ±):\")\n",
    "print(f\"      Accuracy: {domain_accuracy*100:.2f}%\")\n",
    "print(f\"      DoÄŸru: {domain_correct_count:,} / {total_samples:,}\")\n",
    "print(f\"      YanlÄ±ÅŸ: {total_samples - domain_correct_count:,}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š MASK DETECTION (Domain doÄŸru olanlar iÃ§in):\")\n",
    "print(f\"      Accuracy: {mask_accuracy_on_correct*100:.2f}%\")\n",
    "print(f\"      DoÄŸru: {mask_correct_count:,} / {len(df_domain_correct):,}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š FULL PIPELINE (Domain + Mask):\")\n",
    "print(f\"      Accuracy: {pipeline_accuracy*100:.2f}%\")\n",
    "print(f\"      DoÄŸru: {pipeline_correct_count:,} / {total_samples:,}\")\n",
    "\n",
    "# =============================================\n",
    "# 6ï¸âƒ£ MULTI-EXPERT ETKÄ°SÄ° ANALÄ°ZÄ°\n",
    "# =============================================\n",
    "\n",
    "if USE_MULTI_EXPERT_ROUTING and stats['multi_expert_routing'] > 0:\n",
    "    print(\"\\n6ï¸âƒ£ Multi-Expert Etkisi Analizi:\")\n",
    "    \n",
    "    # Multi-expert kullanÄ±lan Ã¶rnekleri filtrele\n",
    "    df_multi_expert = df_results[df_results['routing_method'] == 'multi_expert']\n",
    "    df_normal = df_results[df_results['routing_method'] == 'normal']\n",
    "    \n",
    "    # Multi-expert'in dÃ¼zelttiÄŸi Ã¶rnekler (orijinal yanlÄ±ÅŸ, final doÄŸru)\n",
    "    df_fixed_by_multi = df_multi_expert[\n",
    "        (df_multi_expert['original_pred_domain_label'] != df_multi_expert['true_domain_label']) &\n",
    "        (df_multi_expert['final_pred_domain_label'] == df_multi_expert['true_domain_label'])\n",
    "    ]\n",
    "    \n",
    "    # Multi-expert'in bozduÄŸu Ã¶rnekler (orijinal doÄŸru, final yanlÄ±ÅŸ)\n",
    "    df_broken_by_multi = df_multi_expert[\n",
    "        (df_multi_expert['original_pred_domain_label'] == df_multi_expert['true_domain_label']) &\n",
    "        (df_multi_expert['final_pred_domain_label'] != df_multi_expert['true_domain_label'])\n",
    "    ]\n",
    "    \n",
    "    print(f\"   {'-'*50}\")\n",
    "    print(f\"   Multi-Expert kullanÄ±lan toplam: {len(df_multi_expert):,}\")\n",
    "    print(f\"   âœ… Multi-Expert'in DÃœZELTTÄ°ÄÄ°:  {len(df_fixed_by_multi):,}\")\n",
    "    print(f\"   âŒ Multi-Expert'in BOZDUÄU:     {len(df_broken_by_multi):,}\")\n",
    "    net_improvement = len(df_fixed_by_multi) - len(df_broken_by_multi)\n",
    "    print(f\"   ğŸ“ˆ NET Ä°YÄ°LEÅTÄ°RME:             {net_improvement:+,}\")\n",
    "    \n",
    "    # EÄŸer net iyileÅŸtirme varsa, yÃ¼zde hesapla\n",
    "    if net_improvement != 0:\n",
    "        improvement_pct = net_improvement / total_samples * 100\n",
    "        print(f\"   ğŸ“ˆ Pipeline accuracy artÄ±ÅŸÄ±:   {improvement_pct:+.2f}%\")\n",
    "    \n",
    "    # Orijinal vs Final karÅŸÄ±laÅŸtÄ±rma\n",
    "    original_domain_correct = (df_results['original_pred_domain_label'] == df_results['true_domain_label']).sum()\n",
    "    final_domain_correct = (df_results['final_pred_domain_label'] == df_results['true_domain_label']).sum()\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š KARÅILAÅTIRMA:\")\n",
    "    print(f\"      Orijinal Stage1 doÄŸru:  {original_domain_correct:,} ({original_domain_correct/total_samples*100:.2f}%)\")\n",
    "    print(f\"      Multi-Expert sonrasÄ±:   {final_domain_correct:,} ({final_domain_correct/total_samples*100:.2f}%)\")\n",
    "    print(f\"      Fark:                   {final_domain_correct - original_domain_correct:+,}\")\n",
    "\n",
    "# Summary dict oluÅŸtur\n",
    "test_summary = {\n",
    "    'total_samples': total_samples,\n",
    "    'domain_accuracy': domain_accuracy,\n",
    "    'domain_correct': int(domain_correct_count),\n",
    "    'domain_wrong': int(total_samples - domain_correct_count),\n",
    "    'mask_accuracy_on_correct_domain': mask_accuracy_on_correct,\n",
    "    'mask_correct': int(mask_correct_count),\n",
    "    'pipeline_accuracy': pipeline_accuracy,\n",
    "    'pipeline_correct': int(pipeline_correct_count),\n",
    "    'multi_expert_stats': stats,\n",
    "    'routing_threshold': ROUTING_THRESHOLD if USE_MULTI_EXPERT_ROUTING else None,\n",
    "    'hybrid_alpha': HYBRID_ALPHA if USE_MULTI_EXPERT_ROUTING else None\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… HÄ°YERARÅÄ°K TEST + MULTI-EXPERT ROUTING TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ” HÃœCRE 21 - DETAYLI HATA ANALÄ°ZÄ°\n",
    "Hata tiplerini kategorize et ve detaylÄ± analiz yap\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” DETAYLI HATA ANALÄ°ZÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ HATA KATEGORÄ°LERÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Hata kategorileri oluÅŸturuluyor...\")\n",
    "\n",
    "# Kategorilere ayÄ±r\n",
    "df_stage1_correct_stage2_correct = df_results[(df_results['domain_correct'] == True) & (df_results['mask_correct'] == True)]\n",
    "df_stage1_correct_stage2_wrong = df_results[(df_results['domain_correct'] == True) & (df_results['mask_correct'] == False)]\n",
    "df_stage1_wrong = df_results[df_results['domain_correct'] == False]\n",
    "\n",
    "error_categories = {\n",
    "    'Stage1âœ“_Stage2âœ“': len(df_stage1_correct_stage2_correct),\n",
    "    'Stage1âœ“_Stage2âœ—': len(df_stage1_correct_stage2_wrong),\n",
    "    'Stage1âœ—': len(df_stage1_wrong)\n",
    "}\n",
    "\n",
    "print(f\"\\n   ğŸ“Š HATA KATEGORÄ°LERÄ°:\")\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'Kategori':<30} {'SayÄ±':>10} {'Oran':>10}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "\n",
    "for category, count in error_categories.items():\n",
    "    pct = (count / total_samples) * 100\n",
    "    print(f\"   {category:<30} {count:>10,} {pct:>9.2f}%\")\n",
    "\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'TOPLAM':<30} {total_samples:>10,}\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ DOMAIN BAZINDA HATA ANALÄ°ZÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Domain bazÄ±nda hata analizi...\")\n",
    "\n",
    "domain_error_analysis = []\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_domain = df_results[df_results['true_domain_name'] == domain]\n",
    "    \n",
    "    if len(df_domain) == 0:\n",
    "        continue\n",
    "    \n",
    "    domain_correct = df_domain['domain_correct'].sum()\n",
    "    domain_wrong = len(df_domain) - domain_correct\n",
    "    \n",
    "    df_domain_correct_subset = df_domain[df_domain['domain_correct'] == True]\n",
    "    mask_correct = df_domain_correct_subset['mask_correct'].sum() if len(df_domain_correct_subset) > 0 else 0\n",
    "    mask_wrong = len(df_domain_correct_subset) - mask_correct if len(df_domain_correct_subset) > 0 else 0\n",
    "    \n",
    "    pipeline_correct = df_domain['pipeline_correct'].sum()\n",
    "    \n",
    "    domain_error_analysis.append({\n",
    "        'domain': domain,\n",
    "        'total': len(df_domain),\n",
    "        'stage1_correct': domain_correct,\n",
    "        'stage1_wrong': domain_wrong,\n",
    "        'stage1_accuracy': domain_correct / len(df_domain),\n",
    "        'stage2_correct': mask_correct,\n",
    "        'stage2_wrong': mask_wrong,\n",
    "        'stage2_accuracy': mask_correct / domain_correct if domain_correct > 0 else 0,\n",
    "        'pipeline_correct': pipeline_correct,\n",
    "        'pipeline_accuracy': pipeline_correct / len(df_domain)\n",
    "    })\n",
    "\n",
    "df_domain_analysis = pd.DataFrame(domain_error_analysis)\n",
    "\n",
    "print(f\"\\n   {'Domain':<15} {'Total':>8} {'S1 Acc':>10} {'S2 Acc':>10} {'Pipeline':>10}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "\n",
    "for _, row in df_domain_analysis.iterrows():\n",
    "    print(f\"   {row['domain']:<15} {row['total']:>8,} \"\n",
    "          f\"{row['stage1_accuracy']*100:>9.2f}% \"\n",
    "          f\"{row['stage2_accuracy']*100:>9.2f}% \"\n",
    "          f\"{row['pipeline_accuracy']*100:>9.2f}%\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ EN Ã‡OK KARIÅTIRILAN DOMAIN Ã‡Ä°FTLERÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ En Ã§ok karÄ±ÅŸtÄ±rÄ±lan domain Ã§iftleri (Multi-Expert sonrasÄ±)...\")\n",
    "\n",
    "domain_confusion_pairs = df_stage1_wrong.groupby(['true_domain_name', 'final_pred_domain_name']).size().reset_index(name='count')\n",
    "domain_confusion_pairs = domain_confusion_pairs.sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"\\n   {'True Domain':<15} {'Pred Domain':<15} {'Count':>8} {'%':>8}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "\n",
    "for _, row in domain_confusion_pairs.head(10).iterrows():\n",
    "    pct = (row['count'] / len(df_stage1_wrong)) * 100 if len(df_stage1_wrong) > 0 else 0\n",
    "    print(f\"   {row['true_domain_name']:<15} {row['final_pred_domain_name']:<15} {row['count']:>8,} {pct:>7.2f}%\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ EN Ã‡OK KARIÅTIRILAN MASK'LAR (Domain DoÄŸru Olanlar Ä°Ã§in)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ En Ã§ok karÄ±ÅŸtÄ±rÄ±lan mask'lar (domain doÄŸru olanlar iÃ§in)...\")\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_domain_s2_wrong = df_stage1_correct_stage2_wrong[df_stage1_correct_stage2_wrong['true_domain_name'] == domain]\n",
    "    \n",
    "    if len(df_domain_s2_wrong) == 0:\n",
    "        continue\n",
    "    \n",
    "    mask_confusion = df_domain_s2_wrong.groupby(['true_mask_name', 'pred_mask_name']).size().reset_index(name='count')\n",
    "    mask_confusion = mask_confusion.sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"\\n   ğŸ¯ {domain}:\")\n",
    "    for _, row in mask_confusion.head(5).iterrows():\n",
    "        print(f\"      {row['true_mask_name']:<20} â†’ {row['pred_mask_name']:<20} ({row['count']})\")\n",
    "\n",
    "# =============================================\n",
    "# 5ï¸âƒ£ CONFIDENCE ANALÄ°ZÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ Confidence analizi...\")\n",
    "\n",
    "# Domain confidence by correctness\n",
    "avg_conf_correct = df_results[df_results['domain_correct'] == True]['domain_confidence'].mean()\n",
    "avg_conf_wrong = df_results[df_results['domain_correct'] == False]['domain_confidence'].mean()\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Domain Confidence:\")\n",
    "print(f\"      DoÄŸru tahminler ortalama confidence: {avg_conf_correct:.4f}\")\n",
    "print(f\"      YanlÄ±ÅŸ tahminler ortalama confidence: {avg_conf_wrong:.4f}\")\n",
    "print(f\"      Fark: {avg_conf_correct - avg_conf_wrong:.4f}\")\n",
    "\n",
    "# Low confidence analysis\n",
    "low_conf_threshold = 0.7\n",
    "df_low_conf = df_results[df_results['domain_confidence'] < low_conf_threshold]\n",
    "low_conf_wrong = df_low_conf[df_low_conf['domain_correct'] == False]\n",
    "\n",
    "print(f\"\\n   ğŸ“Š DÃ¼ÅŸÃ¼k Confidence (<{low_conf_threshold}) Analizi:\")\n",
    "print(f\"      Toplam dÃ¼ÅŸÃ¼k confidence: {len(df_low_conf):,} ({len(df_low_conf)/total_samples*100:.2f}%)\")\n",
    "print(f\"      BunlarÄ±n yanlÄ±ÅŸ olanlarÄ±: {len(low_conf_wrong):,} ({len(low_conf_wrong)/len(df_low_conf)*100:.2f}% of low conf)\")\n",
    "\n",
    "# High confidence analysis\n",
    "high_conf_threshold = 0.9\n",
    "df_high_conf = df_results[df_results['domain_confidence'] >= high_conf_threshold]\n",
    "high_conf_correct = df_high_conf[df_high_conf['domain_correct'] == True]\n",
    "\n",
    "print(f\"\\n   ğŸ“Š YÃ¼ksek Confidence (â‰¥{high_conf_threshold}) Analizi:\")\n",
    "print(f\"      Toplam yÃ¼ksek confidence: {len(df_high_conf):,} ({len(df_high_conf)/total_samples*100:.2f}%)\")\n",
    "print(f\"      BunlarÄ±n doÄŸru olanlarÄ±: {len(high_conf_correct):,} ({len(high_conf_correct)/len(df_high_conf)*100:.2f}% of high conf)\")\n",
    "\n",
    "# =============================================\n",
    "# 6ï¸âƒ£ HATA ANALÄ°ZÄ° Ã–ZET\n",
    "# =============================================\n",
    "\n",
    "error_analysis_summary = {\n",
    "    'error_categories': error_categories,\n",
    "    'domain_analysis': df_domain_analysis.to_dict('records'),\n",
    "    'top_domain_confusions': domain_confusion_pairs.head(10).to_dict('records'),\n",
    "    'avg_confidence_correct': avg_conf_correct,\n",
    "    'avg_confidence_wrong': avg_conf_wrong,\n",
    "    'low_confidence_samples': len(df_low_conf),\n",
    "    'low_confidence_error_rate': len(low_conf_wrong) / len(df_low_conf) if len(df_low_conf) > 0 else 0,\n",
    "    'high_confidence_samples': len(df_high_conf),\n",
    "    'high_confidence_accuracy': len(high_conf_correct) / len(df_high_conf) if len(df_high_conf) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… HATA ANALÄ°ZÄ° TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š HÃœCRE 22 - PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 1\n",
    "Confusion Matrices ve Performance Charts\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ã‡Ä±ktÄ± klasÃ¶rÃ¼\n",
    "output_folder = \"hierarchical_validation_test_results\"\n",
    "viz_folder = os.path.join(output_folder, \"visualizations\")\n",
    "os.makedirs(viz_folder, exist_ok=True)\n",
    "\n",
    "# Custom color palette\n",
    "COLORS = {\n",
    "    'correct': '#2ecc71',      # Green\n",
    "    'wrong': '#e74c3c',        # Red\n",
    "    'domain_correct': '#3498db',  # Blue\n",
    "    'mask_wrong': '#f39c12',   # Orange\n",
    "    'neutral': '#95a5a6',      # Gray\n",
    "    'background': '#2c3e50',   # Dark blue\n",
    "    'accent': '#9b59b6'        # Purple\n",
    "}\n",
    "\n",
    "# Profesyonel style ayarlarÄ±\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ STAGE 1 CONFUSION MATRIX (5x5 Domain) - Multi-Expert SonrasÄ±\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Stage 1 Confusion Matrix oluÅŸturuluyor (Multi-Expert sonrasÄ±)...\")\n",
    "\n",
    "# Confusion matrix hesapla - final (multi-expert sonrasÄ±) tahminleri kullan\n",
    "true_domains = df_results['true_domain_label'].values\n",
    "pred_domains = df_results['final_pred_domain_label'].values\n",
    "\n",
    "cm_domain = confusion_matrix(true_domains, pred_domains, labels=range(NUM_DOMAINS))\n",
    "cm_domain_normalized = cm_domain.astype('float') / cm_domain.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Custom colormap\n",
    "cmap_counts = LinearSegmentedColormap.from_list('custom_blues', ['#ffffff', '#3498db', '#1a5276'])\n",
    "cmap_pct = LinearSegmentedColormap.from_list('custom_greens', ['#ffffff', '#27ae60', '#145a32'])\n",
    "\n",
    "# Sol: Raw counts\n",
    "sns.heatmap(cm_domain, annot=True, fmt='d', cmap=cmap_counts,\n",
    "            xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "            ax=axes[0], cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
    "            annot_kws={'size': 12, 'weight': 'bold'},\n",
    "            linewidths=0.5, linecolor='white')\n",
    "axes[0].set_xlabel('Predicted Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_ylabel('True Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_title('Domain Classification (Multi-Expert)\\n(Absolute Counts)', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[0].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "axes[0].tick_params(axis='y', rotation=0, labelsize=11)\n",
    "\n",
    "# SaÄŸ: Percentages\n",
    "sns.heatmap(cm_domain_normalized, annot=True, fmt='.1f', cmap=cmap_pct,\n",
    "            xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "            ax=axes[1], cbar_kws={'label': 'Percentage (%)', 'shrink': 0.8},\n",
    "            annot_kws={'size': 12, 'weight': 'bold'},\n",
    "            linewidths=0.5, linecolor='white', vmin=0, vmax=100)\n",
    "axes[1].set_xlabel('Predicted Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[1].set_ylabel('True Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[1].set_title('Domain Classification (Multi-Expert)\\n(Row-Normalized %)', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[1].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "axes[1].tick_params(axis='y', rotation=0, labelsize=11)\n",
    "\n",
    "# Accuracy text\n",
    "overall_acc = np.trace(cm_domain) / cm_domain.sum()\n",
    "fig.suptitle(f'Domain Classification Accuracy (Multi-Expert Routing): {overall_acc*100:.2f}%', \n",
    "             fontsize=16, fontweight='bold', y=1.02, color=COLORS['background'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'stage1_domain_confusion_matrix.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Stage 1 confusion matrix kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ STAGE 2 CONFUSION MATRICES (Her domain iÃ§in)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Stage 2 Confusion Matrices oluÅŸturuluyor...\")\n",
    "\n",
    "# Her domain iÃ§in ayrÄ± confusion matrix\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, domain in enumerate(DOMAIN_NAMES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Bu domain iÃ§in doÄŸru tahmin edilen Ã¶rnekleri al\n",
    "    df_domain_correct = df_results[(df_results['true_domain_name'] == domain) & \n",
    "                                    (df_results['domain_correct'] == True)]\n",
    "    \n",
    "    if len(df_domain_correct) == 0:\n",
    "        ax.text(0.5, 0.5, f'{domain}\\nNo data', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title(f'{domain} - No Data')\n",
    "        continue\n",
    "    \n",
    "    # Mask labels (true and predicted)\n",
    "    true_masks = df_domain_correct['true_mask_label'].values\n",
    "    pred_masks = df_domain_correct['pred_mask_label'].values\n",
    "    \n",
    "    # Unique labels\n",
    "    unique_labels = sorted(set(true_masks) | set(pred_masks))\n",
    "    \n",
    "    if len(unique_labels) == 0:\n",
    "        ax.text(0.5, 0.5, f'{domain}\\nNo valid masks', ha='center', va='center', fontsize=14)\n",
    "        continue\n",
    "    \n",
    "    # Mask names for this domain\n",
    "    l2m = stage2_models[domain]['label_to_mask'] if domain in stage2_models else {}\n",
    "    mask_names = [l2m.get(l) or l2m.get(str(l), f\"Mask{l}\") for l in unique_labels]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm_mask = confusion_matrix(true_masks, pred_masks, labels=unique_labels)\n",
    "    cm_mask_norm = cm_mask.astype('float') / (cm_mask.sum(axis=1)[:, np.newaxis] + 1e-10) * 100\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(cm_mask_norm, annot=True, fmt='.0f', cmap='YlOrRd',\n",
    "                xticklabels=mask_names, yticklabels=mask_names,\n",
    "                ax=ax, cbar_kws={'shrink': 0.6},\n",
    "                annot_kws={'size': max(6, 10 - len(unique_labels)//3)},\n",
    "                linewidths=0.5, linecolor='white', vmin=0, vmax=100)\n",
    "    \n",
    "    # Accuracy for this domain\n",
    "    domain_mask_acc = np.trace(cm_mask) / cm_mask.sum() if cm_mask.sum() > 0 else 0\n",
    "    \n",
    "    ax.set_xlabel('Predicted Mask', fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('True Mask', fontsize=10, fontweight='bold')\n",
    "    ax.set_title(f'{domain}\\nMask Accuracy: {domain_mask_acc*100:.1f}%\\n({len(df_domain_correct):,} samples)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=max(6, 9 - len(unique_labels)//3))\n",
    "    ax.tick_params(axis='y', rotation=0, labelsize=max(6, 9 - len(unique_labels)//3))\n",
    "\n",
    "# Son subplot'u gizle\n",
    "axes[5].axis('off')\n",
    "\n",
    "# BaÅŸlÄ±k\n",
    "fig.suptitle('Stage 2: Mask Detection Confusion Matrices by Domain\\n(Only Domain-Correct Samples)', \n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'stage2_mask_confusion_matrices.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Stage 2 confusion matrices kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ PER-DOMAIN ACCURACY BAR CHART\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Per-domain accuracy chart oluÅŸturuluyor...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(DOMAIN_NAMES))\n",
    "width = 0.28\n",
    "\n",
    "# Accuracy values from domain analysis\n",
    "s1_accs = [df_domain_analysis[df_domain_analysis['domain'] == d]['stage1_accuracy'].values[0] \n",
    "           if d in df_domain_analysis['domain'].values else 0 for d in DOMAIN_NAMES]\n",
    "s2_accs = [df_domain_analysis[df_domain_analysis['domain'] == d]['stage2_accuracy'].values[0]\n",
    "           if d in df_domain_analysis['domain'].values else 0 for d in DOMAIN_NAMES]\n",
    "pipeline_accs = [df_domain_analysis[df_domain_analysis['domain'] == d]['pipeline_accuracy'].values[0]\n",
    "                 if d in df_domain_analysis['domain'].values else 0 for d in DOMAIN_NAMES]\n",
    "\n",
    "# Bars\n",
    "bars1 = ax.bar(x - width, s1_accs, width, label='Stage 1 (Domain)', color=COLORS['domain_correct'], \n",
    "               edgecolor='white', linewidth=2, alpha=0.85)\n",
    "bars2 = ax.bar(x, s2_accs, width, label='Stage 2 (Mask)', color=COLORS['correct'], \n",
    "               edgecolor='white', linewidth=2, alpha=0.85)\n",
    "bars3 = ax.bar(x + width, pipeline_accs, width, label='Full Pipeline', color=COLORS['accent'], \n",
    "               edgecolor='white', linewidth=2, alpha=0.85)\n",
    "\n",
    "# Target line\n",
    "ax.axhline(y=0.85, color=COLORS['wrong'], linestyle='--', linewidth=2, \n",
    "          alpha=0.7, label='Target (85%)')\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_ylabel('Accuracy', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_title('Hierarchical Pipeline Performance by Domain', fontsize=16, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(DOMAIN_NAMES, fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right', framealpha=0.9)\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bar labels\n",
    "for bars, values in [(bars1, s1_accs), (bars2, s2_accs), (bars3, pipeline_accs)]:\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{val*100:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold', rotation=0)\n",
    "\n",
    "# Overall stats annotation\n",
    "textstr = f'Overall Stage 1: {domain_accuracy*100:.1f}%\\nOverall Stage 2: {mask_accuracy_on_correct*100:.1f}%\\nOverall Pipeline: {pipeline_accuracy*100:.1f}%'\n",
    "props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=COLORS['background'])\n",
    "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'per_domain_accuracy.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Per-domain accuracy chart kaydedildi\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… GÃ–RSELLEÅTÄ°RME BÃ–LÃœM 1 TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š HÃœCRE 23 - PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 2\n",
    "Error Flow Diagram, Confidence Distributions, Error Breakdown\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ ERROR FLOW DIAGRAM (Sankey-style)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Error Flow Diagram oluÅŸturuluyor...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Hesaplamalar\n",
    "n_total = len(df_results)\n",
    "n_s1_correct = len(df_stage1_correct_stage2_correct) + len(df_stage1_correct_stage2_wrong)\n",
    "n_s1_wrong = len(df_stage1_wrong)\n",
    "n_s2_correct = len(df_stage1_correct_stage2_correct)\n",
    "n_s2_wrong = len(df_stage1_correct_stage2_wrong)\n",
    "\n",
    "# Stage konumlarÄ±\n",
    "stages = ['Input\\nSamples', 'Stage 1\\nDomain', 'Stage 2\\nMask', 'Output']\n",
    "stage_x = [0, 1, 2, 3]\n",
    "\n",
    "# Y pozisyonlarÄ± (akÄ±ÅŸ yÃ¼kseklikleri)\n",
    "y_positions = {\n",
    "    'total': 0.5,\n",
    "    's1_correct': 0.65,\n",
    "    's1_wrong': 0.25,\n",
    "    's2_correct': 0.75,\n",
    "    's2_wrong': 0.55,\n",
    "    'final_correct': 0.75,\n",
    "    'final_wrong': 0.35\n",
    "}\n",
    "\n",
    "# Box'larÄ± Ã§iz\n",
    "def draw_box(ax, x, y, width, height, color, label, count, pct):\n",
    "    rect = plt.Rectangle((x - width/2, y - height/2), width, height, \n",
    "                         facecolor=color, edgecolor='white', linewidth=2, alpha=0.85)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, f'{label}\\n{count:,}\\n({pct:.1f}%)', \n",
    "            ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# AkÄ±ÅŸ Ã§izgileri\n",
    "def draw_flow(ax, x1, y1, x2, y2, width, color, alpha=0.4):\n",
    "    from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
    "    # Basit Ã§izgi Ã§iz\n",
    "    ax.fill([x1, x2, x2, x1], [y1-width/2, y2-width/2, y2+width/2, y1+width/2], \n",
    "            color=color, alpha=alpha, linewidth=0)\n",
    "\n",
    "# Total input\n",
    "draw_box(ax, 0, 0.5, 0.3, 0.35, COLORS['neutral'], 'Total Input', n_total, 100)\n",
    "\n",
    "# Stage 1 outputs\n",
    "s1_correct_pct = n_s1_correct/n_total*100\n",
    "s1_wrong_pct = n_s1_wrong/n_total*100\n",
    "draw_box(ax, 1, 0.7, 0.3, 0.25, COLORS['domain_correct'], 'Domain\\nCorrect', n_s1_correct, s1_correct_pct)\n",
    "draw_box(ax, 1, 0.25, 0.3, 0.2, COLORS['wrong'], 'Domain\\nWrong', n_s1_wrong, s1_wrong_pct)\n",
    "\n",
    "# Stage 2 outputs (sadece S1 doÄŸru olanlar iÃ§in)\n",
    "s2_correct_pct = n_s2_correct/n_total*100\n",
    "s2_wrong_pct = n_s2_wrong/n_total*100\n",
    "draw_box(ax, 2, 0.8, 0.3, 0.2, COLORS['correct'], 'Mask\\nCorrect', n_s2_correct, s2_correct_pct)\n",
    "draw_box(ax, 2, 0.55, 0.3, 0.18, COLORS['mask_wrong'], 'Mask\\nWrong', n_s2_wrong, s2_wrong_pct)\n",
    "\n",
    "# Final outputs\n",
    "final_correct = n_s2_correct\n",
    "final_wrong = n_s1_wrong + n_s2_wrong\n",
    "final_correct_pct = final_correct/n_total*100\n",
    "final_wrong_pct = final_wrong/n_total*100\n",
    "draw_box(ax, 3, 0.8, 0.3, 0.2, COLORS['correct'], 'SUCCESS', final_correct, final_correct_pct)\n",
    "draw_box(ax, 3, 0.3, 0.3, 0.3, COLORS['wrong'], 'FAILED', final_wrong, final_wrong_pct)\n",
    "\n",
    "# AkÄ±ÅŸ Ã§izgileri\n",
    "# Input â†’ S1 Correct\n",
    "draw_flow(ax, 0.15, 0.6, 0.85, 0.7, 0.15, COLORS['domain_correct'])\n",
    "# Input â†’ S1 Wrong\n",
    "draw_flow(ax, 0.15, 0.4, 0.85, 0.25, 0.1, COLORS['wrong'])\n",
    "# S1 Correct â†’ S2 Correct\n",
    "draw_flow(ax, 1.15, 0.75, 1.85, 0.8, 0.1, COLORS['correct'])\n",
    "# S1 Correct â†’ S2 Wrong\n",
    "draw_flow(ax, 1.15, 0.65, 1.85, 0.55, 0.08, COLORS['mask_wrong'])\n",
    "# S1 Wrong â†’ Final Wrong\n",
    "draw_flow(ax, 1.15, 0.25, 2.85, 0.3, 0.08, COLORS['wrong'], alpha=0.3)\n",
    "# S2 Correct â†’ Final Correct\n",
    "draw_flow(ax, 2.15, 0.8, 2.85, 0.8, 0.1, COLORS['correct'])\n",
    "# S2 Wrong â†’ Final Wrong\n",
    "draw_flow(ax, 2.15, 0.55, 2.85, 0.35, 0.08, COLORS['mask_wrong'], alpha=0.3)\n",
    "\n",
    "# Oklar ve etiketler\n",
    "ax.annotate('', xy=(0.85, 0.7), xytext=(0.15, 0.55), \n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['domain_correct'], lw=2))\n",
    "ax.annotate('', xy=(0.85, 0.25), xytext=(0.15, 0.45), \n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['wrong'], lw=2))\n",
    "\n",
    "# Stage labels\n",
    "for i, stage in enumerate(stages):\n",
    "    ax.text(i, -0.05, stage, ha='center', va='top', fontsize=12, fontweight='bold', \n",
    "            color=COLORS['background'])\n",
    "\n",
    "# BaÅŸlÄ±k ve legend\n",
    "ax.set_xlim(-0.3, 3.3)\n",
    "ax.set_ylim(-0.1, 1.05)\n",
    "ax.axis('off')\n",
    "ax.set_title('Hierarchical Pipeline Error Flow\\n(Stage 1 â†’ Stage 2 Error Propagation)', \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(color=COLORS['correct'], label=f'Success: {final_correct:,} ({final_correct_pct:.1f}%)'),\n",
    "    mpatches.Patch(color=COLORS['wrong'], label=f'Stage 1 Error: {n_s1_wrong:,} ({s1_wrong_pct:.1f}%)'),\n",
    "    mpatches.Patch(color=COLORS['mask_wrong'], label=f'Stage 2 Error: {n_s2_wrong:,} ({s2_wrong_pct:.1f}%)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=11, \n",
    "         framealpha=0.9, bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'error_flow_diagram.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Error flow diagram kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 5ï¸âƒ£ CONFIDENCE DISTRIBUTION PLOTS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ Confidence Distribution Plots oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 5a. Domain Confidence - Correct vs Wrong\n",
    "ax = axes[0, 0]\n",
    "correct_confs = df_results[df_results['domain_correct'] == True]['domain_confidence'].values\n",
    "wrong_confs = df_results[df_results['domain_correct'] == False]['domain_confidence'].values\n",
    "\n",
    "ax.hist(correct_confs, bins=50, alpha=0.7, color=COLORS['correct'], \n",
    "        label=f'Correct ({len(correct_confs):,})', density=True, edgecolor='white')\n",
    "ax.hist(wrong_confs, bins=50, alpha=0.7, color=COLORS['wrong'], \n",
    "        label=f'Wrong ({len(wrong_confs):,})', density=True, edgecolor='white')\n",
    "ax.axvline(x=0.9, color=COLORS['background'], linestyle='--', linewidth=2, label='High Conf (0.9)')\n",
    "ax.axvline(x=correct_confs.mean(), color=COLORS['correct'], linestyle=':', linewidth=2)\n",
    "ax.axvline(x=wrong_confs.mean() if len(wrong_confs) > 0 else 0, color=COLORS['wrong'], linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Domain Confidence', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stage 1: Domain Prediction Confidence\\n(Correct vs Wrong)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5b. Mask Confidence (Domain Correct olanlar iÃ§in)\n",
    "ax = axes[0, 1]\n",
    "mask_correct_confs = df_stage1_correct_stage2_correct['mask_confidence'].values\n",
    "mask_wrong_confs = df_stage1_correct_stage2_wrong['mask_confidence'].values\n",
    "\n",
    "if len(mask_correct_confs) > 0:\n",
    "    ax.hist(mask_correct_confs, bins=50, alpha=0.7, color=COLORS['correct'], \n",
    "            label=f'Mask Correct ({len(mask_correct_confs):,})', density=True, edgecolor='white')\n",
    "if len(mask_wrong_confs) > 0:\n",
    "    ax.hist(mask_wrong_confs, bins=50, alpha=0.7, color=COLORS['mask_wrong'], \n",
    "            label=f'Mask Wrong ({len(mask_wrong_confs):,})', density=True, edgecolor='white')\n",
    "ax.axvline(x=0.9, color=COLORS['background'], linestyle='--', linewidth=2, label='High Conf (0.9)')\n",
    "ax.set_xlabel('Mask Confidence', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stage 2: Mask Prediction Confidence\\n(Domain Correct Samples Only)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5c. Box Plot - Domain Confidence by Domain\n",
    "ax = axes[1, 0]\n",
    "domain_conf_data = []\n",
    "domain_labels = []\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_dom = df_results[df_results['true_domain_name'] == domain]\n",
    "    domain_conf_data.append(df_dom['domain_confidence'].values)\n",
    "    domain_labels.append(domain)\n",
    "\n",
    "bp = ax.boxplot(domain_conf_data, labels=domain_labels, patch_artist=True)\n",
    "colors_box = [COLORS['domain_correct'], COLORS['correct'], COLORS['mask_wrong'], \n",
    "              COLORS['accent'], COLORS['neutral']]\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_xlabel('Domain', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Domain Confidence', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Domain Confidence Distribution by True Domain', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5d. Confidence vs Error Rate\n",
    "ax = axes[1, 1]\n",
    "conf_bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (conf_bins[:-1] + conf_bins[1:]) / 2\n",
    "error_rates = []\n",
    "sample_counts = []\n",
    "\n",
    "for i in range(len(conf_bins) - 1):\n",
    "    mask = (df_results['domain_confidence'] >= conf_bins[i]) & (df_results['domain_confidence'] < conf_bins[i+1])\n",
    "    df_bin = df_results[mask]\n",
    "    if len(df_bin) > 0:\n",
    "        error_rate = 1 - df_bin['domain_correct'].mean()\n",
    "        error_rates.append(error_rate)\n",
    "        sample_counts.append(len(df_bin))\n",
    "    else:\n",
    "        error_rates.append(0)\n",
    "        sample_counts.append(0)\n",
    "\n",
    "# Bar plot\n",
    "bars = ax.bar(bin_centers, error_rates, width=0.08, color=COLORS['wrong'], \n",
    "              edgecolor='white', alpha=0.85)\n",
    "\n",
    "# Sample count as secondary y-axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(bin_centers, sample_counts, color=COLORS['domain_correct'], \n",
    "         marker='o', linewidth=2, label='Sample Count')\n",
    "ax2.set_ylabel('Sample Count', fontsize=12, fontweight='bold', color=COLORS['domain_correct'])\n",
    "ax2.tick_params(axis='y', labelcolor=COLORS['domain_correct'])\n",
    "\n",
    "ax.set_xlabel('Confidence Bin', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Error Rate', fontsize=12, fontweight='bold', color=COLORS['wrong'])\n",
    "ax.tick_params(axis='y', labelcolor=COLORS['wrong'])\n",
    "ax.set_title('Domain Error Rate vs Confidence\\n(Lower confidence = Higher error)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim([0, max(error_rates)*1.2 if error_rates else 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'confidence_distributions.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Confidence distribution plots kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 6ï¸âƒ£ ERROR BREAKDOWN PIE CHART\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ Error Breakdown Pie Chart oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Sol: Genel breakdown\n",
    "ax = axes[0]\n",
    "labels = ['Stage1âœ“ Stage2âœ“\\n(Full Success)', 'Stage1âœ“ Stage2âœ—\\n(Mask Error)', 'Stage1âœ—\\n(Domain Error)']\n",
    "sizes = [len(df_stage1_correct_stage2_correct), len(df_stage1_correct_stage2_wrong), len(df_stage1_wrong)]\n",
    "colors = [COLORS['correct'], COLORS['mask_wrong'], COLORS['wrong']]\n",
    "explode = (0.02, 0.02, 0.05)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                                   autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                                   textprops={'fontsize': 11, 'fontweight': 'bold'},\n",
    "                                   wedgeprops={'edgecolor': 'white', 'linewidth': 2})\n",
    "ax.set_title(f'Pipeline Result Distribution\\n(Total: {n_total:,} samples)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# SaÄŸ: Domain bazÄ±nda error breakdown\n",
    "ax = axes[1]\n",
    "domain_errors = []\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_dom = df_results[df_results['true_domain_name'] == domain]\n",
    "    domain_errors.append(len(df_dom[df_dom['domain_correct'] == False]))\n",
    "\n",
    "bars = ax.barh(DOMAIN_NAMES, domain_errors, color=COLORS['wrong'], \n",
    "               edgecolor='white', alpha=0.85)\n",
    "ax.set_xlabel('Number of Domain Errors', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Domain', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stage 1 Domain Errors by True Domain', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Bar labels\n",
    "for bar, err in zip(bars, domain_errors):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 10, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{err:,}',\n",
    "            ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'error_breakdown.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Error breakdown pie chart kaydedildi\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… GÃ–RSELLEÅTÄ°RME BÃ–LÃœM 2 TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ’¾ HÃœCRE 25 - SONUÃ‡LARI DIÅA AKTAR VE Ã–ZET RAPOR\n",
    "CSV, JSON ve HTML formatlarÄ±nda sonuÃ§larÄ± kaydet\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ’¾ SONUÃ‡LARI DIÅA AKTAR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 8ï¸âƒ£ CSV EXPORT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n8ï¸âƒ£ DetaylÄ± sonuÃ§lar CSV'ye kaydediliyor...\")\n",
    "\n",
    "# Main results CSV\n",
    "csv_path = os.path.join(output_folder, 'detailed_results.csv')\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"   âœ… DetaylÄ± sonuÃ§lar: {csv_path}\")\n",
    "\n",
    "# Domain analysis CSV\n",
    "domain_csv_path = os.path.join(output_folder, 'domain_analysis.csv')\n",
    "df_domain_analysis.to_csv(domain_csv_path, index=False)\n",
    "print(f\"   âœ… Domain analizi: {domain_csv_path}\")\n",
    "\n",
    "# Error pairs CSV\n",
    "error_pairs_path = os.path.join(output_folder, 'domain_confusion_pairs.csv')\n",
    "domain_confusion_pairs.to_csv(error_pairs_path, index=False)\n",
    "print(f\"   âœ… Domain karÄ±ÅŸÄ±klÄ±k Ã§iftleri: {error_pairs_path}\")\n",
    "\n",
    "# =============================================\n",
    "# 9ï¸âƒ£ JSON SUMMARY EXPORT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n9ï¸âƒ£ Ã–zet metrikler JSON'a kaydediliyor...\")\n",
    "\n",
    "summary_json = {\n",
    "    'metadata': {\n",
    "        'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'notebook': NOTEBOOK_NAME,\n",
    "        'total_validation_samples': int(total_samples)\n",
    "    },\n",
    "    'stage1_domain_classification': {\n",
    "        'accuracy': float(domain_accuracy),\n",
    "        'correct_count': int(domain_correct_count),\n",
    "        'wrong_count': int(total_samples - domain_correct_count),\n",
    "        'per_domain_accuracy': {\n",
    "            row['domain']: float(row['stage1_accuracy']) \n",
    "            for _, row in df_domain_analysis.iterrows()\n",
    "        }\n",
    "    },\n",
    "    'stage2_mask_detection': {\n",
    "        'accuracy_on_correct_domain': float(mask_accuracy_on_correct),\n",
    "        'correct_count': int(mask_correct_count),\n",
    "        'wrong_count': int(len(df_domain_correct) - mask_correct_count) if len(df_domain_correct) > 0 else 0,\n",
    "        'per_domain_accuracy': {\n",
    "            row['domain']: float(row['stage2_accuracy']) \n",
    "            for _, row in df_domain_analysis.iterrows()\n",
    "        }\n",
    "    },\n",
    "    'full_pipeline': {\n",
    "        'accuracy': float(pipeline_accuracy),\n",
    "        'correct_count': int(pipeline_correct_count),\n",
    "        'wrong_count': int(total_samples - pipeline_correct_count),\n",
    "        'per_domain_accuracy': {\n",
    "            row['domain']: float(row['pipeline_accuracy']) \n",
    "            for _, row in df_domain_analysis.iterrows()\n",
    "        }\n",
    "    },\n",
    "    'error_breakdown': {\n",
    "        'stage1_correct_stage2_correct': int(len(df_stage1_correct_stage2_correct)),\n",
    "        'stage1_correct_stage2_wrong': int(len(df_stage1_correct_stage2_wrong)),\n",
    "        'stage1_wrong': int(len(df_stage1_wrong))\n",
    "    },\n",
    "    'confidence_analysis': {\n",
    "        'avg_confidence_correct_domain': float(avg_conf_correct),\n",
    "        'avg_confidence_wrong_domain': float(avg_conf_wrong),\n",
    "        'low_confidence_threshold': 0.7,\n",
    "        'low_confidence_samples': int(len(df_low_conf)),\n",
    "        'high_confidence_threshold': 0.9,\n",
    "        'high_confidence_samples': int(len(df_high_conf))\n",
    "    },\n",
    "    'top_domain_confusions': [\n",
    "        {\n",
    "            'true_domain': row['true_domain_name'],\n",
    "            'pred_domain': row['final_pred_domain_name'],\n",
    "            'count': int(row['count'])\n",
    "        }\n",
    "        for _, row in domain_confusion_pairs.head(5).iterrows()\n",
    "    ]\n",
    "}\n",
    "\n",
    "json_path = os.path.join(output_folder, 'summary_metrics.json')\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_json, f, indent=2, ensure_ascii=False)\n",
    "print(f\"   âœ… Ã–zet metrikler: {json_path}\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ”Ÿ FINAL SUMMARY DISPLAY\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š KAPSAMLI HÄ°YERARÅÄ°K VALÄ°DASYON TEST SONUÃ‡LARI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ“‹ TEST Ã–ZETÄ°                                 â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "   ğŸ“Š Toplam Test Ã–rnekleri: {total_samples:,}\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ¯ STAGE 1 - DOMAIN CLASSIFICATION                 â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   âœ… Accuracy: {domain_accuracy*100:.2f}%\n",
    "   âœ… DoÄŸru:    {domain_correct_count:,} Ã¶rnek\n",
    "   âŒ YanlÄ±ÅŸ:   {total_samples - domain_correct_count:,} Ã¶rnek\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ­ STAGE 2 - MASK DETECTION                        â•‘\n",
    "â•‘                  (Sadece Domain DoÄŸru Olanlar Ä°Ã§in)                   â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   âœ… Accuracy: {mask_accuracy_on_correct*100:.2f}%\n",
    "   âœ… DoÄŸru:    {mask_correct_count:,} Ã¶rnek\n",
    "   âŒ YanlÄ±ÅŸ:   {len(df_domain_correct) - mask_correct_count:,} Ã¶rnek\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ”— FULL PIPELINE PERFORMANCE                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   âœ… Pipeline Accuracy: {pipeline_accuracy*100:.2f}%\n",
    "   âœ… Tam BaÅŸarÄ±:  {pipeline_correct_count:,} Ã¶rnek ({pipeline_accuracy*100:.1f}%)\n",
    "   âš ï¸ Stage 1 HatasÄ±: {len(df_stage1_wrong):,} Ã¶rnek ({len(df_stage1_wrong)/total_samples*100:.1f}%)\n",
    "   âš ï¸ Stage 2 HatasÄ±: {len(df_stage1_correct_stage2_wrong):,} Ã¶rnek ({len(df_stage1_correct_stage2_wrong)/total_samples*100:.1f}%)\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ“Š DOMAIN BAZINDA                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "print(f\"   {'Domain':<15} {'S1 Acc':>10} {'S2 Acc':>10} {'Pipeline':>10}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "for _, row in df_domain_analysis.iterrows():\n",
    "    print(f\"   {row['domain']:<15} {row['stage1_accuracy']*100:>9.1f}% {row['stage2_accuracy']*100:>9.1f}% {row['pipeline_accuracy']*100:>9.1f}%\")\n",
    "\n",
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ“‚ OLUÅTURULAN DOSYALAR                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   ğŸ“ {output_folder}/\n",
    "      â”œâ”€â”€ ğŸ“„ detailed_results.csv          (Her Ã¶rnek iÃ§in detaylÄ± sonuÃ§lar)\n",
    "      â”œâ”€â”€ ğŸ“„ domain_analysis.csv           (Domain bazÄ±nda analiz)\n",
    "      â”œâ”€â”€ ğŸ“„ domain_confusion_pairs.csv    (Domain karÄ±ÅŸÄ±klÄ±k Ã§iftleri)\n",
    "      â”œâ”€â”€ ğŸ“„ summary_metrics.json          (Ã–zet metrikler)\n",
    "      â”‚\n",
    "      â””â”€â”€ ğŸ“ visualizations/\n",
    "          â”œâ”€â”€ ğŸ“Š stage1_domain_confusion_matrix.png\n",
    "          â”œâ”€â”€ ğŸ“Š stage2_mask_confusion_matrices.png\n",
    "          â”œâ”€â”€ ğŸ“Š per_domain_accuracy.png\n",
    "          â”œâ”€â”€ ğŸ“Š error_flow_diagram.png\n",
    "          â”œâ”€â”€ ğŸ“Š confidence_distributions.png\n",
    "          â”œâ”€â”€ ğŸ“Š error_breakdown.png\n",
    "          â”œâ”€â”€ ğŸ–¼ï¸ misclassified_stage1_errors.png\n",
    "          â”œâ”€â”€ ğŸ–¼ï¸ misclassified_stage2_errors.png\n",
    "          â””â”€â”€ ğŸ–¼ï¸ success_samples.png\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ’¡ Ã–NERÄ°LER                                   â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Recommendations based on results\n",
    "if domain_accuracy < 0.85:\n",
    "    print(f\"   âš ï¸ Stage 1 accuracy dÃ¼ÅŸÃ¼k ({domain_accuracy*100:.1f}% < 85%)\")\n",
    "    print(f\"      â†’ Domain classifier modelini iyileÅŸtir\")\n",
    "    print(f\"      â†’ Daha fazla epoch veya data augmentation dene\")\n",
    "\n",
    "if mask_accuracy_on_correct < 0.75:\n",
    "    print(f\"   âš ï¸ Stage 2 accuracy dÃ¼ÅŸÃ¼k ({mask_accuracy_on_correct*100:.1f}% < 75%)\")\n",
    "    print(f\"      â†’ Domain-specific mask detector'larÄ± iyileÅŸtir\")\n",
    "    print(f\"      â†’ SÄ±nÄ±f dengesizliÄŸi iÃ§in class weighting dene\")\n",
    "\n",
    "if len(df_stage1_wrong) > total_samples * 0.2:\n",
    "    print(f\"   ğŸš¨ YÃ¼ksek Stage 1 hata oranÄ±!\")\n",
    "    print(f\"      â†’ {len(df_stage1_wrong):,} Ã¶rnek yanlÄ±ÅŸ domain'e gitti\")\n",
    "    print(f\"      â†’ En Ã§ok karÄ±ÅŸan domain Ã§iftlerini incele\")\n",
    "\n",
    "if avg_conf_wrong > 0.7:\n",
    "    print(f\"   âš ï¸ YanlÄ±ÅŸ tahminlerin confidence'Ä± yÃ¼ksek ({avg_conf_wrong:.2f})\")\n",
    "    print(f\"      â†’ Model calibration gerekebilir\")\n",
    "    print(f\"      â†’ Label smoothing veya temperature scaling dene\")\n",
    "\n",
    "print(f\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                    âœ… TEST TAMAMLANDI!\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
