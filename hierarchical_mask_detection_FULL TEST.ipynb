{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ğŸ—ï¸ HiyerarÅŸik Mask Detection - FULL VERSION\n",
    "\n",
    "## Konsept:\n",
    "```\n",
    "Stage 1: Domain Classification (5 sÄ±nÄ±f)\n",
    "  â”œâ”€ URBAN (15 masks)\n",
    "  â”œâ”€ HUMAN_BODY (14 masks)\n",
    "  â”œâ”€ CLOTHING (7 masks)\n",
    "  â”œâ”€ INDOOR (8 masks)\n",
    "  â””â”€ BACKGROUND (5 masks)\n",
    "\n",
    "Stage 2: Fine-Grained Mask Detection\n",
    "  â”œâ”€ URBAN Model â†’ building, car, road, ...\n",
    "  â”œâ”€ HUMAN_BODY Model â†’ hair, face, skin, ...\n",
    "  â”œâ”€ CLOTHING Model â†’ Upper-clothes, Pants, ...\n",
    "  â”œâ”€ INDOOR Model â†’ floor, ceiling, chair, ...\n",
    "  â””â”€ BACKGROUND Model â†’ background, ego vehicle, ...\n",
    "```\n",
    "\n",
    "## Coverage: ~77% (Top-50 eÅŸdeÄŸer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”§ HÃœCRE 1 - SETUP VE IMPORTS\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from utils import *\n",
    "\n",
    "cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"ğŸ”¥ GPU Ä±sÄ±ndÄ±rÄ±lÄ±yor...\")\n",
    "dummy = torch.randn(1000, 1000).cuda()\n",
    "for _ in range(10):\n",
    "    _ = torch.matmul(dummy, dummy)\n",
    "torch.cuda.synchronize()\n",
    "del dummy\n",
    "print(\"âœ… GPU hazÄ±r!\")\n",
    "\n",
    "# =============================================\n",
    "# âš™ï¸ AYARLAR\n",
    "# =============================================\n",
    "\n",
    "NOTEBOOK_NAME = \"hierarchical_mask_detection\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS_STAGE1 = 6  # Domain classifier\n",
    "NUM_EPOCHS_STAGE2 = 5  # Domain-specific models (daha az data)\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOP_PATIENCE = 8\n",
    "IMG_SIZE = 224\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "CSV_BASE = r\"C:\\AI_DATA\\SEMI_TRUTHS\\inpainting\"\n",
    "EXTRACTED_BASE = r\"C:\\AI_DATA\\SEMI_TRUTHS_extracted\\inpainting\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ® HÄ°YERARÅÄ°K MASK DETECTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Stage 1 Epochs: {NUM_EPOCHS_STAGE1}\")\n",
    "print(f\"Stage 2 Epochs: {NUM_EPOCHS_STAGE2}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“‹ HÃœCRE 2 - DOMAIN KATEGORÄ°LERÄ°\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‹ DOMAIN KATEGORÄ°LERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "DOMAIN_CATEGORIES = {\n",
    "    'URBAN': [\n",
    "        'building', 'car', 'road', 'sidewalk', 'vegetation', 'sky', \n",
    "        'wall', 'window', 'door', 'tree', 'trees', 'pole', 'fence', \n",
    "        'parking', 'terrain'\n",
    "    ],\n",
    "    \n",
    "    'HUMAN_BODY': [\n",
    "        'hair', 'face', 'skin', 'neck', 'nose', 'Right-arm', 'Left-arm',\n",
    "        'Right-leg', 'Left-leg', 'left_ear', 'right_ear', 'mouth', \n",
    "        'upper_lip', 'lower_lip'\n",
    "    ],\n",
    "    \n",
    "    'CLOTHING': [\n",
    "        'Upper-clothes', 'Pants', 'skirt', 'dress', 'hat', 'bag', 'cloth'\n",
    "    ],\n",
    "    \n",
    "    'INDOOR': [\n",
    "        'floor', 'ceiling', 'chair', 'table', 'seat', 'column', 'ground', 'grass'\n",
    "    ],\n",
    "    \n",
    "    'BACKGROUND': [\n",
    "        'background', 'out of roi', 'ego vehicle', 'rectification border', 'static'\n",
    "    ]\n",
    "}\n",
    "\n",
    "DOMAIN_NAMES = list(DOMAIN_CATEGORIES.keys())\n",
    "NUM_DOMAINS = len(DOMAIN_NAMES)\n",
    "\n",
    "print(f\"\\nâœ… {NUM_DOMAINS} Domain TanÄ±mlandÄ±:\")\n",
    "total_masks = 0\n",
    "for i, (domain, masks) in enumerate(DOMAIN_CATEGORIES.items()):\n",
    "    print(f\"   [{i}] {domain:<15} â†’ {len(masks):2d} masks\")\n",
    "    total_masks += len(masks)\n",
    "\n",
    "print(f\"\\n   TOPLAM: {total_masks} mask tÃ¼rÃ¼\")\n",
    "\n",
    "def get_domain(mask_name):\n",
    "    for domain, masks in DOMAIN_CATEGORIES.items():\n",
    "        if mask_name in masks:\n",
    "            return domain\n",
    "    return None\n",
    "\n",
    "print(\"\\nâœ… get_domain() fonksiyonu tanÄ±mlandÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š HÃœCRE 3 - CSV YÃœKLEME VE DOMAIN ETÄ°KETLEME\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š CSV YÃœKLEMESÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "csv_files = glob.glob(os.path.join(CSV_BASE, \"**\", \"*_meta.csv\"), recursive=True)\n",
    "print(f\"\\nâœ… Bulunan CSV: {len(csv_files)} dosya\")\n",
    "\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"âŒ CSV dosyasÄ± bulunamadÄ±: {CSV_BASE}\")\n",
    "\n",
    "all_dfs = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    csv_dir = os.path.dirname(csv_file)\n",
    "    parent_folder = os.path.basename(os.path.dirname(csv_dir))\n",
    "    model_name = os.path.basename(csv_dir)\n",
    "    df['parent_dataset'] = parent_folder\n",
    "    df['model'] = model_name\n",
    "    df['dataset'] = f\"{parent_folder}_{model_name}\"\n",
    "    all_dfs.append(df)\n",
    "    print(f\"   âœ… {parent_folder}/{model_name}: {len(df):,} satÄ±r\")\n",
    "\n",
    "df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\nâœ… Toplam: {len(df_combined):,} gÃ¶rsel\")\n",
    "\n",
    "# Domain etiketleme\n",
    "df_combined['domain'] = df_combined['mask_name'].apply(get_domain)\n",
    "df_with_domain = df_combined[df_combined['domain'].notna()].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Domain DaÄŸÄ±lÄ±mÄ±:\")\n",
    "domain_counts = df_with_domain['domain'].value_counts()\n",
    "for domain in DOMAIN_NAMES:\n",
    "    count = domain_counts.get(domain, 0)\n",
    "    ratio = (count / len(df_with_domain)) * 100\n",
    "    print(f\"   {domain:<15}: {count:>6,} ({ratio:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Domain etiketlendi: {len(df_with_domain):,} / {len(df_combined):,} gÃ¶rsel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ—ï¸ HÃœCRE 4 - MODEL MÄ°MARÄ°LERÄ°\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ—ï¸ MODEL MÄ°MARÄ°LERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    \"\"\"Stage 1: Domain sÄ±nÄ±flandÄ±rÄ±cÄ±\"\"\"\n",
    "    \n",
    "    def __init__(self, num_domains=5, dropout_rate=0.3):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        backbone_out = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(backbone_out, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "class DomainMaskDetector(nn.Module):\n",
    "    \"\"\"Stage 2: Domain-specific mask detector\"\"\"\n",
    "    \n",
    "    def __init__(self, num_masks, dropout_rate=0.3):\n",
    "        super(DomainMaskDetector, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        backbone_out = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(backbone_out, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_masks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "print(\"\\nâœ… DomainClassifier tanÄ±mlandÄ±\")\n",
    "print(\"âœ… DomainMaskDetector tanÄ±mlandÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ HÃœCRE 5 - EÄÄ°TÄ°M FONKSÄ°YONLARI\n",
    "\"\"\"\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, batch_metrics_file=None, epoch=0, phase='train'):\n",
    "    \"\"\"Bir epoch eÄŸitim\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with tqdm(loader, desc=f\"Train E{epoch}\", leave=False) as pbar:\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Batch metrics kaydet\n",
    "            if batch_metrics_file and batch_idx % 10 == 0:\n",
    "                with open(batch_metrics_file, 'a') as f:\n",
    "                    f.write(f\"{epoch},{batch_idx},{phase},{loss.item():.6f},{(correct/total):.6f}\\n\")\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(correct/total)*100:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def val_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Bir epoch validasyon\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(loader, desc=\"Val\", leave=False) as pbar:\n",
    "            for images, labels in pbar:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{(correct/total)*100:.2f}%'\n",
    "                })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, all_preds, all_labels\n",
    "\n",
    "print(\"âœ… train_epoch() ve val_epoch() tanÄ±mlandÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ HÃœCRE 6 - STAGE 1 DATASET (Domain Classification) - RECURSIVE FIX\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‚ STAGE 1 DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Domain label mapping\n",
    "domain_to_label = {domain: idx for idx, domain in enumerate(DOMAIN_NAMES)}\n",
    "label_to_domain = {idx: domain for domain, idx in domain_to_label.items()}\n",
    "\n",
    "print(\"\\nDomain â†’ Label:\")\n",
    "for domain, label in domain_to_label.items():\n",
    "    print(f\"   [{label}] {domain}\")\n",
    "\n",
    "# Etiket sÃ¶zlÃ¼ÄŸÃ¼\n",
    "labels_dict_by_dataset = {}\n",
    "for _, row in df_with_domain.iterrows():\n",
    "    img_id = str(row['perturbed_img_id'])\n",
    "    domain = row['domain']\n",
    "    label = domain_to_label[domain]\n",
    "    dataset_key = row['dataset']\n",
    "    \n",
    "    if dataset_key not in labels_dict_by_dataset:\n",
    "        labels_dict_by_dataset[dataset_key] = {}\n",
    "    \n",
    "    for ext in ['', '.png', '.jpg', '.jpeg']:\n",
    "        labels_dict_by_dataset[dataset_key][img_id + ext] = label\n",
    "\n",
    "print(f\"\\nâœ… Etiket dictionary oluÅŸturuldu: {len(labels_dict_by_dataset)} dataset\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ” RECURSIVE DOSYA TARAMA (os.walk kullanarak)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nğŸ“‚ Extract edilmiÅŸ dosyalar taranÄ±yor (recursive)...\")\n",
    "\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "found_by_dataset = {}\n",
    "\n",
    "# Her dataset klasÃ¶rÃ¼nÃ¼ tara\n",
    "for parent_dataset in os.listdir(EXTRACTED_BASE):\n",
    "    parent_path = os.path.join(EXTRACTED_BASE, parent_dataset)\n",
    "    if not os.path.isdir(parent_path):\n",
    "        continue\n",
    "    \n",
    "    for model_name in os.listdir(parent_path):\n",
    "        model_path = os.path.join(parent_path, model_name)\n",
    "        if not os.path.isdir(model_path):\n",
    "            continue\n",
    "        \n",
    "        dataset_key = f\"{parent_dataset}_{model_name}\"\n",
    "        \n",
    "        # Bu dataset iÃ§in etiket sÃ¶zlÃ¼ÄŸÃ¼ var mÄ±?\n",
    "        if dataset_key not in labels_dict_by_dataset:\n",
    "            continue\n",
    "        \n",
    "        labels_dict = labels_dict_by_dataset[dataset_key]\n",
    "        found_by_dataset[dataset_key] = 0\n",
    "        \n",
    "        print(f\"\\nğŸ” {dataset_key}\")\n",
    "        print(f\"   KlasÃ¶r: {model_path}\")\n",
    "        \n",
    "        # ğŸ”‘ æ ¸å¿ƒ: os.walk ile recursive tara (alt klasÃ¶rler dahil)\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    # Dosya adÄ± varyasyonlarÄ±\n",
    "                    base_name = file\n",
    "                    name_without_ext = os.path.splitext(file)[0]\n",
    "                    \n",
    "                    label = None\n",
    "                    \n",
    "                    # FarklÄ± adlarla dene\n",
    "                    for name in [base_name, name_without_ext,\n",
    "                               name_without_ext + '.png',\n",
    "                               name_without_ext + '.jpg',\n",
    "                               name_without_ext + '.jpeg']:\n",
    "                        if name in labels_dict:\n",
    "                            label = labels_dict[name]\n",
    "                            break\n",
    "                    \n",
    "                    if label is not None:\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        all_image_paths.append(full_path)\n",
    "                        all_labels.append(label)\n",
    "                        found_by_dataset[dataset_key] += 1\n",
    "        \n",
    "        if dataset_key in found_by_dataset:\n",
    "            print(f\"   âœ… {found_by_dataset[dataset_key]:,} gÃ¶rÃ¼ntÃ¼ bulundu\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… TOPLAM: {len(all_image_paths):,} gÃ¶rÃ¼ntÃ¼ bulundu\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if len(all_image_paths) == 0:\n",
    "    print(\"\\nâŒ HATA: HiÃ§ gÃ¶rÃ¼ntÃ¼ bulunamadÄ±!\")\n",
    "    print(\"\\nğŸ“‹ Bulunan dataset'ler:\")\n",
    "    for dataset_key in labels_dict_by_dataset.keys():\n",
    "        print(f\"   - {dataset_key}: {len(labels_dict_by_dataset[dataset_key]):,} etiket\")\n",
    "    raise FileNotFoundError(\"GÃ¶rÃ¼ntÃ¼ dosyalarÄ± bulunamadÄ±!\")\n",
    "\n",
    "# Dataset bazÄ±nda istatistik\n",
    "print(f\"\\nğŸ“Š Dataset BazÄ±nda Bulunan:\")\n",
    "for dataset_key in sorted(found_by_dataset.keys()):\n",
    "    expected = len([k for k in labels_dict_by_dataset[dataset_key].keys() if not k.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    found = found_by_dataset[dataset_key]\n",
    "    ratio = (found / expected * 100) if expected > 0 else 0\n",
    "    print(f\"   {dataset_key:<35}: {found:>6,} / {expected:>6,} ({ratio:>5.1f}%)\")\n",
    "\n",
    "# Dataset class\n",
    "class DomainDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            # Hata durumunda rastgele baÅŸka Ã¶rnek dÃ¶ndÃ¼r\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Train/Val split\n",
    "indices = list(range(len(all_image_paths)))\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "split_idx = int(len(indices) * (1 - VAL_SPLIT))\n",
    "train_indices = indices[:split_idx]\n",
    "val_indices = indices[split_idx:]\n",
    "\n",
    "train_paths_s1 = [all_image_paths[i] for i in train_indices]\n",
    "train_labels_s1 = [all_labels[i] for i in train_indices]\n",
    "val_paths_s1 = [all_image_paths[i] for i in val_indices]\n",
    "val_labels_s1 = [all_labels[i] for i in val_indices]\n",
    "\n",
    "print(f\"\\nâœ… Train/Val Split:\")\n",
    "print(f\"   Train: {len(train_paths_s1):,} gÃ¶rÃ¼ntÃ¼\")\n",
    "print(f\"   Val: {len(val_paths_s1):,} gÃ¶rÃ¼ntÃ¼\")\n",
    "\n",
    "# ğŸ”’ STAGE 1 val_indices'i KORU (Cell 8'de override edilmesin!)\n",
    "stage1_val_indices = val_indices.copy()\n",
    "stage1_all_image_paths = all_image_paths.copy()\n",
    "stage1_all_labels = all_labels.copy()\n",
    "\n",
    "print(f\"\\nğŸ”’ Stage 1 validation set korundu:\")\n",
    "print(f\"   stage1_val_indices: {len(stage1_val_indices):,} Ã¶rnek\")\n",
    "print(f\"   stage1_all_image_paths: {len(stage1_all_image_paths):,} dosya\")\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset_s1 = DomainDataset(train_paths_s1, train_labels_s1, train_transform)\n",
    "val_dataset_s1 = DomainDataset(val_paths_s1, val_labels_s1, val_transform)\n",
    "\n",
    "train_loader_s1 = DataLoader(train_dataset_s1, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              num_workers=0, pin_memory=True, drop_last=True)\n",
    "val_loader_s1 = DataLoader(val_dataset_s1, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "print(f\"\\nâœ… DataLoader'lar oluÅŸturuldu:\")\n",
    "print(f\"   Train batches: {len(train_loader_s1)}\")\n",
    "print(f\"   Val batches: {len(val_loader_s1)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… STAGE 1 DATASET HAZIR\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ HÃœCRE 8 - STAGE 2 DATASET HAZIRLIÄI (Domain-Specific Mask Detection)\n",
    "Her domain iÃ§in ayrÄ± dataset ve DataLoader hazÄ±rla\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‚ STAGE 2 DATASET HAZIRLIÄI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ HER DOMAIN Ä°Ã‡Ä°N MASK LABEL MAPPINGS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Domain-specific mask mappings oluÅŸturuluyor...\")\n",
    "\n",
    "domain_mask_mappings = {}\n",
    "\n",
    "for domain, masks in DOMAIN_CATEGORIES.items():\n",
    "    domain_mask_mappings[domain] = {\n",
    "        'masks': masks,\n",
    "        'mask_to_label': {mask: idx for idx, mask in enumerate(masks)},\n",
    "        'label_to_mask': {idx: mask for idx, mask in enumerate(masks)},\n",
    "        'num_masks': len(masks)\n",
    "    }\n",
    "\n",
    "print(\"\\nğŸ“Š Domain Mask Counts:\")\n",
    "for domain, info in domain_mask_mappings.items():\n",
    "    print(f\"   {domain:<15}: {info['num_masks']:>2} masks\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ HER DOMAIN Ä°Ã‡Ä°N DATASET FÄ°LTRELE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Her domain iÃ§in dataset filtreleniyor...\")\n",
    "\n",
    "# Dataset class tanÄ±mla (HÃ¼cre 6'daki ile aynÄ±)\n",
    "class DomainDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            # Hata durumunda rastgele baÅŸka Ã¶rnek dÃ¶ndÃ¼r\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "# Transforms (HÃ¼cre 6'daki ile aynÄ±)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Her domain iÃ§in etiket sÃ¶zlÃ¼kleri oluÅŸtur\n",
    "domain_labels_dicts = {}\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    print(f\"\\nğŸ” {domain} iÃ§in etiketler hazÄ±rlanÄ±yor...\")\n",
    "    \n",
    "    domain_masks = domain_mask_mappings[domain]['masks']\n",
    "    mask_to_label = domain_mask_mappings[domain]['mask_to_label']\n",
    "    \n",
    "    # Bu domain'e ait mask'lara sahip satÄ±rlarÄ± filtrele\n",
    "    domain_df = df_with_domain[\n",
    "        (df_with_domain['domain'] == domain) &\n",
    "        (df_with_domain['mask_name'].isin(domain_masks))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"   CSV'de {len(domain_df):,} Ã¶rnek bulundu\")\n",
    "    \n",
    "    # Etiket sÃ¶zlÃ¼ÄŸÃ¼ oluÅŸtur (dataset bazÄ±nda)\n",
    "    labels_dict_by_dataset = {}\n",
    "    \n",
    "    for _, row in domain_df.iterrows():\n",
    "        img_id = str(row['perturbed_img_id'])\n",
    "        mask_name = row['mask_name']\n",
    "        label = mask_to_label[mask_name]\n",
    "        dataset_key = row['dataset']\n",
    "        \n",
    "        if dataset_key not in labels_dict_by_dataset:\n",
    "            labels_dict_by_dataset[dataset_key] = {}\n",
    "        \n",
    "        # FarklÄ± uzantÄ±larla denemeler\n",
    "        for ext in ['', '.png', '.jpg', '.jpeg']:\n",
    "            labels_dict_by_dataset[dataset_key][img_id + ext] = label\n",
    "    \n",
    "    domain_labels_dicts[domain] = labels_dict_by_dataset\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ DOSYA TARAMA (RECURSIVE os.walk)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Dosyalar taranÄ±yor (recursive)...\")\n",
    "\n",
    "domain_image_data = {}\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    print(f\"\\nğŸ“‚ {domain}\")\n",
    "    \n",
    "    labels_dict_by_dataset = domain_labels_dicts[domain]\n",
    "    \n",
    "    all_image_paths = []\n",
    "    all_labels = []\n",
    "    found_by_dataset = {}\n",
    "    \n",
    "    # Her dataset klasÃ¶rÃ¼nÃ¼ tara\n",
    "    for parent_dataset in os.listdir(EXTRACTED_BASE):\n",
    "        parent_path = os.path.join(EXTRACTED_BASE, parent_dataset)\n",
    "        if not os.path.isdir(parent_path):\n",
    "            continue\n",
    "        \n",
    "        for model_name in os.listdir(parent_path):\n",
    "            model_path = os.path.join(parent_path, model_name)\n",
    "            if not os.path.isdir(model_path):\n",
    "                continue\n",
    "            \n",
    "            dataset_key = f\"{parent_dataset}_{model_name}\"\n",
    "            \n",
    "            # Bu dataset iÃ§in etiket sÃ¶zlÃ¼ÄŸÃ¼ var mÄ±?\n",
    "            if dataset_key not in labels_dict_by_dataset:\n",
    "                continue\n",
    "            \n",
    "            labels_dict = labels_dict_by_dataset[dataset_key]\n",
    "            found_by_dataset[dataset_key] = 0\n",
    "            \n",
    "            # ğŸ”‘ RECURSIVE TARAMA (os.walk)\n",
    "            for root, dirs, files in os.walk(model_path):\n",
    "                for file in files:\n",
    "                    if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        # Dosya adÄ± varyasyonlarÄ±\n",
    "                        base_name = file\n",
    "                        name_without_ext = os.path.splitext(file)[0]\n",
    "                        \n",
    "                        label = None\n",
    "                        \n",
    "                        # FarklÄ± adlarla dene\n",
    "                        for name in [base_name, name_without_ext,\n",
    "                                   name_without_ext + '.png',\n",
    "                                   name_without_ext + '.jpg',\n",
    "                                   name_without_ext + '.jpeg']:\n",
    "                            if name in labels_dict:\n",
    "                                label = labels_dict[name]\n",
    "                                break\n",
    "                        \n",
    "                        if label is not None:\n",
    "                            full_path = os.path.join(root, file)\n",
    "                            all_image_paths.append(full_path)\n",
    "                            all_labels.append(label)\n",
    "                            found_by_dataset[dataset_key] += 1\n",
    "    \n",
    "    # SonuÃ§larÄ± kaydet\n",
    "    domain_image_data[domain] = {\n",
    "        'paths': all_image_paths,\n",
    "        'labels': all_labels,\n",
    "        'found_by_dataset': found_by_dataset\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Toplam: {len(all_image_paths):,} gÃ¶rÃ¼ntÃ¼ bulundu\")\n",
    "    \n",
    "    if len(found_by_dataset) > 0:\n",
    "        for dataset_key, count in found_by_dataset.items():\n",
    "            print(f\"      â€¢ {dataset_key}: {count:,}\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ TRAIN/VAL SPLIT VE DATALOADER'LAR\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Train/Val split ve DataLoader'lar oluÅŸturuluyor...\")\n",
    "\n",
    "domain_datasets = {}\n",
    "domain_loaders = {}\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    print(f\"\\nğŸ“Š {domain}\")\n",
    "    \n",
    "    image_data = domain_image_data[domain]\n",
    "    all_paths = image_data['paths']\n",
    "    all_labels = image_data['labels']\n",
    "    \n",
    "    if len(all_paths) == 0:\n",
    "        print(f\"   âš ï¸  GÃ¶rÃ¼ntÃ¼ bulunamadÄ±, atlanÄ±yor\")\n",
    "        continue\n",
    "    \n",
    "    # Train/Val split\n",
    "    indices = list(range(len(all_paths)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    split_idx = int(len(indices) * (1 - VAL_SPLIT))\n",
    "    train_indices = indices[:split_idx]\n",
    "    val_indices = indices[split_idx:]\n",
    "    \n",
    "    train_paths = [all_paths[i] for i in train_indices]\n",
    "    train_labels = [all_labels[i] for i in train_indices]\n",
    "    val_paths = [all_paths[i] for i in val_indices]\n",
    "    val_labels = [all_labels[i] for i in val_indices]\n",
    "    \n",
    "    print(f\"   Train: {len(train_paths):,} | Val: {len(val_paths):,}\")\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = DomainDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = DomainDataset(val_paths, val_labels, val_transform)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=0, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    # Kaydet\n",
    "    domain_datasets[domain] = {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset\n",
    "    }\n",
    "    \n",
    "    domain_loaders[domain] = {\n",
    "        'train': train_loader,\n",
    "        'val': val_loader\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… STAGE 2 DATASET HAZIR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ã–zet\n",
    "print(f\"\\nğŸ“Š Ã–zet:\")\n",
    "print(f\"   HazÄ±r domain sayÄ±sÄ±: {len(domain_loaders)}\")\n",
    "for domain in domain_loaders.keys():\n",
    "    train_size = len(domain_datasets[domain]['train'])\n",
    "    val_size = len(domain_datasets[domain]['val'])\n",
    "    print(f\"   â€¢ {domain:<15}: {train_size:>6,} train + {val_size:>5,} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ KAPSAMLI HÄ°YERARÅÄ°K VALÄ°DATÄ°ON TEST SÄ°STEMÄ°\n",
    "\n",
    "## Bu bÃ¶lÃ¼mde:\n",
    "1. **HierarchicalDataset**: Hem domain hem mask label'Ä± iÃ§eren dataset\n",
    "2. **Validation Loader OluÅŸturma**: CSV'den mask mapping ile\n",
    "3. **Stage 1 â†’ Stage 2 Pipeline Test**: TÃ¼m validation verisi iÃ§in\n",
    "4. **Hata Analizi**: Hangi verilerin nerede yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±ldÄ±ÄŸÄ±\n",
    "5. **Profesyonel GÃ¶rselleÅŸtirmeler**:\n",
    "   - Confusion Matrices (Stage 1 ve Stage 2)\n",
    "   - Error Flow Sankey Diagram\n",
    "   - Per-Domain Accuracy Charts\n",
    "   - Confidence Distribution Plots\n",
    "   - Misclassified Sample Image Grids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”¬ HÃœCRE 18 - HÄ°YERARÅÄ°K DATASET VE HELPER FUNCTIONS\n",
    "Hem domain hem mask label'Ä±nÄ± iÃ§eren dataset sÄ±nÄ±fÄ±\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”¬ HÄ°YERARÅÄ°K VALÄ°DASYON TEST SÄ°STEMÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“¦ HÄ°YERARÅÄ°K DATASET CLASS\n",
    "# =============================================\n",
    "\n",
    "class HierarchicalValidationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Hem domain hem mask label'Ä±nÄ± dÃ¶ndÃ¼ren dataset.\n",
    "    Stage 1 ve Stage 2 testleri iÃ§in gerekli tÃ¼m bilgileri iÃ§erir.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, domain_labels, mask_names, mask_labels, \n",
    "                 domain_names_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of image file paths\n",
    "            domain_labels: List of domain label indices (0-4)\n",
    "            mask_names: List of actual mask names (string)\n",
    "            mask_labels: List of mask label indices (domain-specific)\n",
    "            domain_names_list: List of domain name strings (for reference)\n",
    "            transform: Image transforms\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.domain_labels = domain_labels\n",
    "        self.mask_names = mask_names\n",
    "        self.mask_labels = mask_labels\n",
    "        self.domain_names_list = domain_names_list\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Validate lengths\n",
    "        assert len(image_paths) == len(domain_labels) == len(mask_names) == len(mask_labels), \\\n",
    "            f\"Length mismatch! paths:{len(image_paths)}, domains:{len(domain_labels)}, \" \\\n",
    "            f\"mask_names:{len(mask_names)}, mask_labels:{len(mask_labels)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            domain_label = self.domain_labels[idx]\n",
    "            mask_name = self.mask_names[idx]\n",
    "            mask_label = self.mask_labels[idx]\n",
    "            \n",
    "            return img, domain_label, mask_label, idx  # idx for tracking\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.image_paths[idx]}: {e}\")\n",
    "            # Fallback to random valid sample\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "# =============================================\n",
    "# ğŸ› ï¸ HELPER FUNCTIONS\n",
    "# =============================================\n",
    "\n",
    "def load_hierarchical_config(config_path):\n",
    "    \"\"\"Hierarchical config JSON'Ä± yÃ¼kle\"\"\"\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def create_stage2_mask_mappings(config):\n",
    "    \"\"\"Stage 2 mask mappings'i oluÅŸtur\"\"\"\n",
    "    mappings = {}\n",
    "    for domain_name, domain_info in config['stage2'].items():\n",
    "        mappings[domain_name] = {\n",
    "            'mask_to_label': domain_info['mask_to_label'],\n",
    "            'label_to_mask': {int(k): v for k, v in domain_info['label_to_mask'].items()},\n",
    "            'num_masks': domain_info['num_masks']\n",
    "        }\n",
    "    return mappings\n",
    "\n",
    "def get_mask_label_for_domain(mask_name, domain_name, stage2_mappings):\n",
    "    \"\"\"Belirli bir domain iÃ§in mask label'Ä± dÃ¶ndÃ¼r\"\"\"\n",
    "    if domain_name not in stage2_mappings:\n",
    "        return None\n",
    "    \n",
    "    mapping = stage2_mappings[domain_name]\n",
    "    if mask_name in mapping['mask_to_label']:\n",
    "        return mapping['mask_to_label'][mask_name]\n",
    "    return None\n",
    "\n",
    "print(\"âœ… HierarchicalValidationDataset sÄ±nÄ±fÄ± tanÄ±mlandÄ±\")\n",
    "print(\"âœ… Helper functions tanÄ±mlandÄ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“¦ HÃœCRE 19 - HÄ°YERARÅÄ°K VALÄ°DASYON LOADER OLUÅTURMA\n",
    "CSV'den mask bilgilerini alarak hierarchical validation loader hazÄ±rla\n",
    "\n",
    "NOT: Bu hÃ¼cre df_with_domain, all_image_paths, all_labels, val_indices \n",
    "deÄŸiÅŸkenlerinin tanÄ±mlÄ± olmasÄ±nÄ± gerektirir (Cell 3 ve Cell 6)\n",
    "\"\"\"\n",
    "\n",
    "import re  # regex iÃ§in\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“¦ HÄ°YERARÅÄ°K VALÄ°DASYON LOADER OLUÅTURMA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 0ï¸âƒ£ GEREKLÄ° DEÄÄ°ÅKENLERÄ° KONTROL ET\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n0ï¸âƒ£ Gerekli deÄŸiÅŸkenler kontrol ediliyor...\")\n",
    "\n",
    "required_vars = ['df_with_domain', 'stage1_all_image_paths', 'stage1_all_labels', 'stage1_val_indices', \n",
    "                 'domain_to_label', 'label_to_domain', 'DOMAIN_NAMES', 'DOMAIN_CATEGORIES']\n",
    "missing_vars = [v for v in required_vars if v not in globals()]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"   âŒ Eksik deÄŸiÅŸkenler: {missing_vars}\")\n",
    "    print(\"   âš ï¸  Ã–nce Cell 1-6'yÄ± Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    raise NameError(f\"Eksik deÄŸiÅŸkenler: {missing_vars}\")\n",
    "else:\n",
    "    print(f\"   âœ… TÃ¼m gerekli deÄŸiÅŸkenler mevcut\")\n",
    "    print(f\"      df_with_domain: {len(df_with_domain):,} kayÄ±t\")\n",
    "    print(f\"      stage1_all_image_paths: {len(stage1_all_image_paths):,} dosya\")\n",
    "    print(f\"      stage1_val_indices: {len(stage1_val_indices):,} Ã¶rnek\")\n",
    "    \n",
    "    # ğŸ” BOYUT KONTROLÃœ\n",
    "    if len(df_with_domain) < 100000:\n",
    "        raise ValueError(f\"\\nâŒ df_with_domain Ã§ok kÃ¼Ã§Ã¼k! ({len(df_with_domain):,})\\n\"\n",
    "                         f\"   Beklenen: ~121,000 â†’ Cell 3'Ã¼ Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    \n",
    "    if len(stage1_all_image_paths) < 50000:\n",
    "        raise ValueError(f\"\\nâŒ stage1_all_image_paths Ã§ok kÃ¼Ã§Ã¼k! ({len(stage1_all_image_paths):,})\\n\"\n",
    "                         f\"   Beklenen: ~81,000 â†’ Cell 6'yÄ± Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    \n",
    "    if len(stage1_val_indices) < 10000:\n",
    "        raise ValueError(f\"\\nâŒ stage1_val_indices Ã§ok kÃ¼Ã§Ã¼k! ({len(stage1_val_indices):,})\\n\"\n",
    "                         f\"   Beklenen: ~16,000 â†’ Cell 6'yÄ± Ã§alÄ±ÅŸtÄ±rÄ±n!\")\n",
    "    \n",
    "    print(f\"      âœ… Boyut kontrolleri PASSED!\")\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ CONFIG VE MAPPINGS YÃœKLE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Config ve mappings yÃ¼kleniyor...\")\n",
    "\n",
    "config_path = os.path.join(\"eÄŸitim_sonuÃ§larÄ±\", NOTEBOOK_NAME, \"veriler\", \"hierarchical_config.json\")\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config bulunamadÄ±: {config_path}\")\n",
    "\n",
    "hierarchical_config = load_hierarchical_config(config_path)\n",
    "stage2_mappings = create_stage2_mask_mappings(hierarchical_config)\n",
    "\n",
    "print(f\"   âœ… Config yÃ¼klendi: {config_path}\")\n",
    "print(f\"   Stage 1 best accuracy: {hierarchical_config['stage1']['best_val_acc']*100:.2f}%\")\n",
    "\n",
    "for domain, info in stage2_mappings.items():\n",
    "    print(f\"   Stage 2 {domain}: {info['num_masks']} masks\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ IMAGE PATH â†’ MASK NAME MAPPING OLUÅTUR\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Image â†’ Mask mapping oluÅŸturuluyor...\")\n",
    "\n",
    "# df_with_domain zaten notebook'ta tanÄ±mlÄ± (Cell 3'ten)\n",
    "# Dataset bazlÄ± mask dictionary oluÅŸtur - perturbed_img_id â†’ mask_name\n",
    "mask_dict_by_dataset = {}\n",
    "\n",
    "for _, row in df_with_domain.iterrows():\n",
    "    img_id = str(row['perturbed_img_id'])\n",
    "    mask_name = row['mask_name']\n",
    "    dataset_key = row['dataset']\n",
    "    \n",
    "    if dataset_key not in mask_dict_by_dataset:\n",
    "        mask_dict_by_dataset[dataset_key] = {}\n",
    "    \n",
    "    # FarklÄ± key formatlarÄ± iÃ§in (Cell 6'daki mantÄ±kla aynÄ±)\n",
    "    for ext in ['', '.png', '.jpg', '.jpeg']:\n",
    "        mask_dict_by_dataset[dataset_key][img_id + ext] = mask_name\n",
    "\n",
    "print(f\"   âœ… {len(mask_dict_by_dataset)} dataset iÃ§in mask mapping oluÅŸturuldu\")\n",
    "for ds, masks in mask_dict_by_dataset.items():\n",
    "    unique_masks = len(set(masks.values()))\n",
    "    print(f\"      {ds}: {len(masks)//4:,} image, {unique_masks} unique mask\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ VALÄ°DATION SET Ä°Ã‡Ä°N LABEL'LARI HAZIRLA\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Validation set iÃ§in label'lar hazÄ±rlanÄ±yor...\")\n",
    "\n",
    "# val_indices zaten tanÄ±mlÄ± (Cell 6'dan)\n",
    "# all_image_paths ve all_labels zaten tanÄ±mlÄ±\n",
    "\n",
    "val_image_paths = []\n",
    "val_domain_labels = []\n",
    "val_mask_names = []\n",
    "val_mask_labels = []\n",
    "\n",
    "skipped_no_domain = 0\n",
    "skipped_no_dataset = 0\n",
    "skipped_no_mask = 0\n",
    "skipped_no_mapping = 0\n",
    "\n",
    "# DetaylÄ± istatistik iÃ§in\n",
    "found_datasets = defaultdict(int)\n",
    "found_domains = defaultdict(int)\n",
    "\n",
    "for idx in tqdm(stage1_val_indices, desc=\"Processing validation set\", leave=False):\n",
    "    img_path = stage1_all_image_paths[idx]\n",
    "    domain_label = stage1_all_labels[idx]\n",
    "    \n",
    "    # Domain name bul - label_to_domain'Ä±n key tipi deÄŸiÅŸebilir\n",
    "    domain_name = label_to_domain.get(domain_label) or label_to_domain.get(str(domain_label))\n",
    "    if domain_name is None and isinstance(domain_label, (np.integer, np.int64)):\n",
    "        domain_name = label_to_domain.get(int(domain_label))\n",
    "    \n",
    "    if domain_name is None:\n",
    "        skipped_no_domain += 1\n",
    "        continue\n",
    "    \n",
    "    # Image path'den dataset key'i Ã§Ä±kar\n",
    "    # Path format: .../inpainting/{parent}/{model}/.../{filename}\n",
    "    path_parts = img_path.replace('\\\\', '/').split('/')\n",
    "    \n",
    "    dataset_key = None\n",
    "    for i, part in enumerate(path_parts):\n",
    "        if part == 'inpainting' and i + 2 < len(path_parts):\n",
    "            parent = path_parts[i + 1]\n",
    "            model = path_parts[i + 2]\n",
    "            dataset_key = f\"{parent}_{model}\"\n",
    "            break\n",
    "    \n",
    "    if dataset_key is None or dataset_key not in mask_dict_by_dataset:\n",
    "        skipped_no_dataset += 1\n",
    "        continue\n",
    "    \n",
    "    found_datasets[dataset_key] += 1\n",
    "    \n",
    "    # Filename'den mask name bul\n",
    "    filename = os.path.basename(img_path)\n",
    "    filename_no_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    mask_dict = mask_dict_by_dataset[dataset_key]\n",
    "    mask_name = None\n",
    "    \n",
    "    # Key arama - Cell 6'daki mantÄ±kla aynÄ±\n",
    "    for key in [filename, filename_no_ext, \n",
    "                filename_no_ext + '.png', filename_no_ext + '.jpg', filename_no_ext + '.jpeg']:\n",
    "        if key in mask_dict:\n",
    "            mask_name = mask_dict[key]\n",
    "            break\n",
    "    \n",
    "    if mask_name is None:\n",
    "        skipped_no_mask += 1\n",
    "        continue\n",
    "    \n",
    "    # Mask label (domain-specific)\n",
    "    mask_label = get_mask_label_for_domain(mask_name, domain_name, stage2_mappings)\n",
    "    \n",
    "    if mask_label is None:\n",
    "        skipped_no_mapping += 1\n",
    "        continue\n",
    "    \n",
    "    found_domains[domain_name] += 1\n",
    "    \n",
    "    # TÃ¼m bilgileri ekle\n",
    "    val_image_paths.append(img_path)\n",
    "    val_domain_labels.append(domain_label)\n",
    "    val_mask_names.append(mask_name)\n",
    "    val_mask_labels.append(mask_label)\n",
    "\n",
    "print(f\"\\n   âœ… {len(val_image_paths):,} Ã¶rnek hazÄ±r\")\n",
    "\n",
    "total_skipped = skipped_no_domain + skipped_no_dataset + skipped_no_mask + skipped_no_mapping\n",
    "if total_skipped > 0:\n",
    "    print(f\"\\n   âš ï¸ Atlanan Ã¶rnek detaylarÄ±:\")\n",
    "    print(f\"      Domain bulunamadÄ±: {skipped_no_domain:,}\")\n",
    "    print(f\"      Dataset bulunamadÄ±: {skipped_no_dataset:,}\")\n",
    "    print(f\"      Mask bulunamadÄ±: {skipped_no_mask:,}\")\n",
    "    print(f\"      Mask label bulunamadÄ±: {skipped_no_mapping:,}\")\n",
    "    print(f\"      TOPLAM atlanan: {total_skipped:,}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Bulunan dataset daÄŸÄ±lÄ±mÄ±:\")\n",
    "for ds, count in sorted(found_datasets.items(), key=lambda x: -x[1]):\n",
    "    print(f\"      {ds}: {count:,}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Bulunan domain daÄŸÄ±lÄ±mÄ±:\")\n",
    "for dom, count in sorted(found_domains.items(), key=lambda x: -x[1]):\n",
    "    print(f\"      {dom}: {count:,}\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ DATASET VE DATALOADER OLUÅTUR\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Dataset ve DataLoader oluÅŸturuluyor...\")\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "hierarchical_val_dataset = HierarchicalValidationDataset(\n",
    "    image_paths=val_image_paths,\n",
    "    domain_labels=val_domain_labels,\n",
    "    mask_names=val_mask_names,\n",
    "    mask_labels=val_mask_labels,\n",
    "    domain_names_list=DOMAIN_NAMES,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "hierarchical_val_loader = DataLoader(\n",
    "    hierarchical_val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"   âœ… DataLoader hazÄ±r\")\n",
    "print(f\"   Batch sayÄ±sÄ±: {len(hierarchical_val_loader)}\")\n",
    "print(f\"   Toplam Ã¶rnek: {len(hierarchical_val_dataset):,}\")\n",
    "\n",
    "# =============================================\n",
    "# 5ï¸âƒ£ DOMAIN DAÄILIMI\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ Domain daÄŸÄ±lÄ±mÄ±:\")\n",
    "\n",
    "domain_dist = defaultdict(int)\n",
    "mask_dist_per_domain = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for domain_label, mask_name in zip(val_domain_labels, val_mask_names):\n",
    "    domain_name = label_to_domain.get(domain_label) or label_to_domain.get(str(domain_label))\n",
    "    domain_dist[domain_name] += 1\n",
    "    mask_dist_per_domain[domain_name][mask_name] += 1\n",
    "\n",
    "print(f\"\\n   {'Domain':<15} {'Samples':>10} {'Unique Masks':>12} {'%':>8}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "\n",
    "total_samples = len(val_domain_labels)\n",
    "for domain in DOMAIN_NAMES:\n",
    "    count = domain_dist[domain]\n",
    "    unique_masks = len(mask_dist_per_domain[domain])\n",
    "    pct = (count / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f\"   {domain:<15} {count:>10,} {unique_masks:>12} {pct:>7.1f}%\")\n",
    "\n",
    "print(f\"   {'-'*50}\")\n",
    "print(f\"   {'TOPLAM':<15} {total_samples:>10,}\")\n",
    "\n",
    "# =============================================\n",
    "# 6ï¸âƒ£ DOÄRULAMA TESTÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ DoÄŸrulama testi...\")\n",
    "\n",
    "# Ä°lk batch'i test et\n",
    "for images, domains, masks, indices in hierarchical_val_loader:\n",
    "    print(f\"\\n   âœ… Ä°lk batch kontrol:\")\n",
    "    print(f\"      Images shape: {images.shape}\")\n",
    "    print(f\"      Domains shape: {domains.shape} (min={domains.min()}, max={domains.max()})\")\n",
    "    print(f\"      Masks shape: {masks.shape} (min={masks.min()}, max={masks.max()})\")\n",
    "    print(f\"      Indices: {indices[:5].tolist()}...\")\n",
    "    break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… HÄ°YERARÅÄ°K VALÄ°DASYON LOADER HAZIR!\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”„ HÃœCRE 20 - HÄ°YERARÅÄ°K TEST PIPELINE (STAGE 1 â†’ STAGE 2)\n",
    "TÃ¼m modelleri yÃ¼kle ve validation setini test et\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”„ HÄ°YERARÅÄ°K TEST PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ STAGE 1 MODEL YÃœKLE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Stage 1 model yÃ¼kleniyor...\")\n",
    "\n",
    "stage1_model_path = hierarchical_config['stage1']['model_path']\n",
    "if not os.path.exists(stage1_model_path):\n",
    "    stage1_model_path = os.path.join(\"eÄŸitim_sonuÃ§larÄ±\", NOTEBOOK_NAME, \"stage1_domain\", \"models\", \"best_model.pth\")\n",
    "\n",
    "stage1_checkpoint = torch.load(stage1_model_path, map_location=device, weights_only=False)\n",
    "stage1_model = DomainClassifier(num_domains=NUM_DOMAINS).to(device)\n",
    "stage1_model.load_state_dict(stage1_checkpoint['model_state_dict'])\n",
    "stage1_model.eval()\n",
    "\n",
    "print(f\"   âœ… Stage 1 model yÃ¼klendi\")\n",
    "print(f\"   Path: {stage1_model_path}\")\n",
    "print(f\"   Val Accuracy: {stage1_checkpoint.get('val_acc', 0)*100:.2f}%\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ STAGE 2 MODELS YÃœKLE\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Stage 2 models yÃ¼kleniyor...\")\n",
    "\n",
    "stage2_models = {}\n",
    "\n",
    "for domain_name in DOMAIN_NAMES:\n",
    "    model_path = hierarchical_config['stage2'][domain_name]['model_path']\n",
    "    if not os.path.exists(model_path):\n",
    "        model_path = os.path.join(\"eÄŸitim_sonuÃ§larÄ±\", NOTEBOOK_NAME, \n",
    "                                  f\"stage2_{domain_name.lower()}\", \"models\", \"best_model.pth\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"   âš ï¸  {domain_name}: Model bulunamadÄ±!\")\n",
    "        continue\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    num_masks = checkpoint['num_masks']\n",
    "    \n",
    "    model = DomainMaskDetector(num_masks=num_masks).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    stage2_models[domain_name] = {\n",
    "        'model': model,\n",
    "        'num_masks': num_masks,\n",
    "        'label_to_mask': checkpoint['label_to_mask'],\n",
    "        'val_acc': checkpoint.get('val_acc', 0)\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… {domain_name}: {num_masks} masks, Acc={checkpoint.get('val_acc', 0)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n   Toplam {len(stage2_models)} Stage 2 model yÃ¼klendi\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ HÄ°YERARÅÄ°K TEST BAÅLAT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Hierarchical test baÅŸlatÄ±lÄ±yor...\")\n",
    "print(f\"   Toplam Ã¶rnek: {len(hierarchical_val_dataset):,}\")\n",
    "print(f\"   Batch sayÄ±sÄ±: {len(hierarchical_val_loader)}\")\n",
    "\n",
    "# SonuÃ§larÄ± saklamak iÃ§in listeler\n",
    "all_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, true_domains, true_masks, sample_indices) in enumerate(\n",
    "        tqdm(hierarchical_val_loader, desc=\"Hierarchical Testing\")\n",
    "    ):\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # ============================================\n",
    "        # STAGE 1: Domain Classification\n",
    "        # ============================================\n",
    "        \n",
    "        domain_outputs = stage1_model(images)\n",
    "        domain_probs = torch.softmax(domain_outputs, dim=1)\n",
    "        domain_confidences, pred_domains = torch.max(domain_probs, 1)\n",
    "        \n",
    "        # CPU'ya taÅŸÄ±\n",
    "        pred_domains = pred_domains.cpu().numpy()\n",
    "        domain_confidences = domain_confidences.cpu().numpy()\n",
    "        true_domains_np = true_domains.numpy()\n",
    "        true_masks_np = true_masks.numpy()\n",
    "        sample_indices_np = sample_indices.numpy()\n",
    "        \n",
    "        # ============================================\n",
    "        # STAGE 2: Mask Classification (Domain-based routing)\n",
    "        # ============================================\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            img = images[i:i+1]  # Keep batch dimension\n",
    "            \n",
    "            true_domain_label = true_domains_np[i]\n",
    "            pred_domain_label = pred_domains[i]\n",
    "            domain_conf = domain_confidences[i]\n",
    "            true_mask_label = true_masks_np[i]\n",
    "            sample_idx = sample_indices_np[i]\n",
    "            \n",
    "            # Domain names\n",
    "            true_domain_name = label_to_domain.get(true_domain_label) or label_to_domain.get(str(true_domain_label))\n",
    "            pred_domain_name = label_to_domain.get(pred_domain_label) or label_to_domain.get(str(pred_domain_label))\n",
    "            \n",
    "            # True mask name (dataset'ten)\n",
    "            true_mask_name = hierarchical_val_dataset.mask_names[sample_idx]\n",
    "            \n",
    "            # Stage 2: Tahmin edilen domain'e gÃ¶re mask predict et\n",
    "            pred_mask_label = -1\n",
    "            pred_mask_name = \"UNKNOWN\"\n",
    "            mask_confidence = 0.0\n",
    "            \n",
    "            if pred_domain_name in stage2_models:\n",
    "                s2_model = stage2_models[pred_domain_name]['model']\n",
    "                s2_outputs = s2_model(img)\n",
    "                s2_probs = torch.softmax(s2_outputs, dim=1)\n",
    "                mask_conf, mask_pred = torch.max(s2_probs, 1)\n",
    "                \n",
    "                pred_mask_label = mask_pred.item()\n",
    "                mask_confidence = mask_conf.item()\n",
    "                \n",
    "                # Label to mask name\n",
    "                l2m = stage2_models[pred_domain_name]['label_to_mask']\n",
    "                pred_mask_name = l2m.get(pred_mask_label) or l2m.get(str(pred_mask_label), \"UNKNOWN\")\n",
    "            \n",
    "            # Domain doÄŸruluÄŸu\n",
    "            domain_correct = (pred_domain_label == true_domain_label)\n",
    "            \n",
    "            # Mask doÄŸruluÄŸu (sadece domain doÄŸruysa anlamlÄ±)\n",
    "            # True mask'Ä±n tahmin edilen domain'deki label'Ä±nÄ± bul\n",
    "            if domain_correct:\n",
    "                expected_mask_label = get_mask_label_for_domain(true_mask_name, true_domain_name, stage2_mappings)\n",
    "                mask_correct = (pred_mask_label == expected_mask_label) if expected_mask_label is not None else False\n",
    "            else:\n",
    "                mask_correct = False  # Domain yanlÄ±ÅŸsa mask deÄŸerlendirmesi anlamsÄ±z\n",
    "            \n",
    "            # Sonucu kaydet\n",
    "            result = {\n",
    "                'sample_idx': sample_idx,\n",
    "                'image_path': hierarchical_val_dataset.image_paths[sample_idx],\n",
    "                'true_domain_label': true_domain_label,\n",
    "                'pred_domain_label': pred_domain_label,\n",
    "                'true_domain_name': true_domain_name,\n",
    "                'pred_domain_name': pred_domain_name,\n",
    "                'domain_confidence': domain_conf,\n",
    "                'domain_correct': domain_correct,\n",
    "                'true_mask_label': true_mask_label,\n",
    "                'pred_mask_label': pred_mask_label,\n",
    "                'true_mask_name': true_mask_name,\n",
    "                'pred_mask_name': pred_mask_name,\n",
    "                'mask_confidence': mask_confidence,\n",
    "                'mask_correct': mask_correct,\n",
    "                'pipeline_correct': domain_correct and mask_correct\n",
    "            }\n",
    "            \n",
    "            all_results.append(result)\n",
    "\n",
    "# DataFrame oluÅŸtur\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\n   âœ… Test tamamlandÄ±: {len(df_results):,} Ã¶rnek\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ TEMEL METRÄ°KLER\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Temel metrikler hesaplanÄ±yor...\")\n",
    "\n",
    "total_samples = len(df_results)\n",
    "domain_correct_count = df_results['domain_correct'].sum()\n",
    "domain_accuracy = domain_correct_count / total_samples\n",
    "\n",
    "# Mask accuracy (sadece domain doÄŸru olanlar iÃ§in)\n",
    "df_domain_correct = df_results[df_results['domain_correct'] == True]\n",
    "mask_correct_count = df_domain_correct['mask_correct'].sum()\n",
    "mask_accuracy_on_correct = mask_correct_count / len(df_domain_correct) if len(df_domain_correct) > 0 else 0\n",
    "\n",
    "# Full pipeline accuracy\n",
    "pipeline_correct_count = df_results['pipeline_correct'].sum()\n",
    "pipeline_accuracy = pipeline_correct_count / total_samples\n",
    "\n",
    "print(f\"\\n   ğŸ“Š STAGE 1 - Domain Classification:\")\n",
    "print(f\"      Accuracy: {domain_accuracy*100:.2f}%\")\n",
    "print(f\"      DoÄŸru: {domain_correct_count:,} / {total_samples:,}\")\n",
    "print(f\"      YanlÄ±ÅŸ: {total_samples - domain_correct_count:,}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š STAGE 2 - Mask Detection (Domain doÄŸru olanlar iÃ§in):\")\n",
    "print(f\"      Accuracy: {mask_accuracy_on_correct*100:.2f}%\")\n",
    "print(f\"      DoÄŸru: {mask_correct_count:,} / {len(df_domain_correct):,}\")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š FULL PIPELINE (Stage 1 + Stage 2):\")\n",
    "print(f\"      Accuracy: {pipeline_accuracy*100:.2f}%\")\n",
    "print(f\"      DoÄŸru: {pipeline_correct_count:,} / {total_samples:,}\")\n",
    "\n",
    "# Summary dict oluÅŸtur\n",
    "test_summary = {\n",
    "    'total_samples': total_samples,\n",
    "    'domain_accuracy': domain_accuracy,\n",
    "    'domain_correct': domain_correct_count,\n",
    "    'domain_wrong': total_samples - domain_correct_count,\n",
    "    'mask_accuracy_on_correct_domain': mask_accuracy_on_correct,\n",
    "    'mask_correct': mask_correct_count,\n",
    "    'pipeline_accuracy': pipeline_accuracy,\n",
    "    'pipeline_correct': pipeline_correct_count\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… HÄ°YERARÅÄ°K TEST TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ” HÃœCRE 21 - DETAYLI HATA ANALÄ°ZÄ°\n",
    "Hata tiplerini kategorize et ve detaylÄ± analiz yap\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” DETAYLI HATA ANALÄ°ZÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ HATA KATEGORÄ°LERÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Hata kategorileri oluÅŸturuluyor...\")\n",
    "\n",
    "# Kategorilere ayÄ±r\n",
    "df_stage1_correct_stage2_correct = df_results[(df_results['domain_correct'] == True) & (df_results['mask_correct'] == True)]\n",
    "df_stage1_correct_stage2_wrong = df_results[(df_results['domain_correct'] == True) & (df_results['mask_correct'] == False)]\n",
    "df_stage1_wrong = df_results[df_results['domain_correct'] == False]\n",
    "\n",
    "error_categories = {\n",
    "    'Stage1âœ“_Stage2âœ“': len(df_stage1_correct_stage2_correct),\n",
    "    'Stage1âœ“_Stage2âœ—': len(df_stage1_correct_stage2_wrong),\n",
    "    'Stage1âœ—': len(df_stage1_wrong)\n",
    "}\n",
    "\n",
    "print(f\"\\n   ğŸ“Š HATA KATEGORÄ°LERÄ°:\")\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'Kategori':<30} {'SayÄ±':>10} {'Oran':>10}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "\n",
    "for category, count in error_categories.items():\n",
    "    pct = (count / total_samples) * 100\n",
    "    print(f\"   {category:<30} {count:>10,} {pct:>9.2f}%\")\n",
    "\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'TOPLAM':<30} {total_samples:>10,}\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ DOMAIN BAZINDA HATA ANALÄ°ZÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Domain bazÄ±nda hata analizi...\")\n",
    "\n",
    "domain_error_analysis = []\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_domain = df_results[df_results['true_domain_name'] == domain]\n",
    "    \n",
    "    if len(df_domain) == 0:\n",
    "        continue\n",
    "    \n",
    "    domain_correct = df_domain['domain_correct'].sum()\n",
    "    domain_wrong = len(df_domain) - domain_correct\n",
    "    \n",
    "    df_domain_correct_subset = df_domain[df_domain['domain_correct'] == True]\n",
    "    mask_correct = df_domain_correct_subset['mask_correct'].sum() if len(df_domain_correct_subset) > 0 else 0\n",
    "    mask_wrong = len(df_domain_correct_subset) - mask_correct if len(df_domain_correct_subset) > 0 else 0\n",
    "    \n",
    "    pipeline_correct = df_domain['pipeline_correct'].sum()\n",
    "    \n",
    "    domain_error_analysis.append({\n",
    "        'domain': domain,\n",
    "        'total': len(df_domain),\n",
    "        'stage1_correct': domain_correct,\n",
    "        'stage1_wrong': domain_wrong,\n",
    "        'stage1_accuracy': domain_correct / len(df_domain),\n",
    "        'stage2_correct': mask_correct,\n",
    "        'stage2_wrong': mask_wrong,\n",
    "        'stage2_accuracy': mask_correct / domain_correct if domain_correct > 0 else 0,\n",
    "        'pipeline_correct': pipeline_correct,\n",
    "        'pipeline_accuracy': pipeline_correct / len(df_domain)\n",
    "    })\n",
    "\n",
    "df_domain_analysis = pd.DataFrame(domain_error_analysis)\n",
    "\n",
    "print(f\"\\n   {'Domain':<15} {'Total':>8} {'S1 Acc':>10} {'S2 Acc':>10} {'Pipeline':>10}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "\n",
    "for _, row in df_domain_analysis.iterrows():\n",
    "    print(f\"   {row['domain']:<15} {row['total']:>8,} \"\n",
    "          f\"{row['stage1_accuracy']*100:>9.2f}% \"\n",
    "          f\"{row['stage2_accuracy']*100:>9.2f}% \"\n",
    "          f\"{row['pipeline_accuracy']*100:>9.2f}%\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ EN Ã‡OK KARIÅTIRILAN DOMAIN Ã‡Ä°FTLERÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ En Ã§ok karÄ±ÅŸtÄ±rÄ±lan domain Ã§iftleri...\")\n",
    "\n",
    "domain_confusion_pairs = df_stage1_wrong.groupby(['true_domain_name', 'pred_domain_name']).size().reset_index(name='count')\n",
    "domain_confusion_pairs = domain_confusion_pairs.sort_values('count', ascending=False)\n",
    "\n",
    "print(f\"\\n   {'True Domain':<15} {'Pred Domain':<15} {'Count':>8} {'%':>8}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "\n",
    "for _, row in domain_confusion_pairs.head(10).iterrows():\n",
    "    pct = (row['count'] / len(df_stage1_wrong)) * 100 if len(df_stage1_wrong) > 0 else 0\n",
    "    print(f\"   {row['true_domain_name']:<15} {row['pred_domain_name']:<15} {row['count']:>8,} {pct:>7.2f}%\")\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ EN Ã‡OK KARIÅTIRILAN MASK'LAR (Domain DoÄŸru Olanlar Ä°Ã§in)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ En Ã§ok karÄ±ÅŸtÄ±rÄ±lan mask'lar (domain doÄŸru olanlar iÃ§in)...\")\n",
    "\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_domain_s2_wrong = df_stage1_correct_stage2_wrong[df_stage1_correct_stage2_wrong['true_domain_name'] == domain]\n",
    "    \n",
    "    if len(df_domain_s2_wrong) == 0:\n",
    "        continue\n",
    "    \n",
    "    mask_confusion = df_domain_s2_wrong.groupby(['true_mask_name', 'pred_mask_name']).size().reset_index(name='count')\n",
    "    mask_confusion = mask_confusion.sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"\\n   ğŸ¯ {domain}:\")\n",
    "    for _, row in mask_confusion.head(5).iterrows():\n",
    "        print(f\"      {row['true_mask_name']:<20} â†’ {row['pred_mask_name']:<20} ({row['count']})\")\n",
    "\n",
    "# =============================================\n",
    "# 5ï¸âƒ£ CONFIDENCE ANALÄ°ZÄ°\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ Confidence analizi...\")\n",
    "\n",
    "# Domain confidence by correctness\n",
    "avg_conf_correct = df_results[df_results['domain_correct'] == True]['domain_confidence'].mean()\n",
    "avg_conf_wrong = df_results[df_results['domain_correct'] == False]['domain_confidence'].mean()\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Domain Confidence:\")\n",
    "print(f\"      DoÄŸru tahminler ortalama confidence: {avg_conf_correct:.4f}\")\n",
    "print(f\"      YanlÄ±ÅŸ tahminler ortalama confidence: {avg_conf_wrong:.4f}\")\n",
    "print(f\"      Fark: {avg_conf_correct - avg_conf_wrong:.4f}\")\n",
    "\n",
    "# Low confidence analysis\n",
    "low_conf_threshold = 0.7\n",
    "df_low_conf = df_results[df_results['domain_confidence'] < low_conf_threshold]\n",
    "low_conf_wrong = df_low_conf[df_low_conf['domain_correct'] == False]\n",
    "\n",
    "print(f\"\\n   ğŸ“Š DÃ¼ÅŸÃ¼k Confidence (<{low_conf_threshold}) Analizi:\")\n",
    "print(f\"      Toplam dÃ¼ÅŸÃ¼k confidence: {len(df_low_conf):,} ({len(df_low_conf)/total_samples*100:.2f}%)\")\n",
    "print(f\"      BunlarÄ±n yanlÄ±ÅŸ olanlarÄ±: {len(low_conf_wrong):,} ({len(low_conf_wrong)/len(df_low_conf)*100:.2f}% of low conf)\")\n",
    "\n",
    "# High confidence analysis\n",
    "high_conf_threshold = 0.9\n",
    "df_high_conf = df_results[df_results['domain_confidence'] >= high_conf_threshold]\n",
    "high_conf_correct = df_high_conf[df_high_conf['domain_correct'] == True]\n",
    "\n",
    "print(f\"\\n   ğŸ“Š YÃ¼ksek Confidence (â‰¥{high_conf_threshold}) Analizi:\")\n",
    "print(f\"      Toplam yÃ¼ksek confidence: {len(df_high_conf):,} ({len(df_high_conf)/total_samples*100:.2f}%)\")\n",
    "print(f\"      BunlarÄ±n doÄŸru olanlarÄ±: {len(high_conf_correct):,} ({len(high_conf_correct)/len(df_high_conf)*100:.2f}% of high conf)\")\n",
    "\n",
    "# =============================================\n",
    "# 6ï¸âƒ£ HATA ANALÄ°ZÄ° Ã–ZET\n",
    "# =============================================\n",
    "\n",
    "error_analysis_summary = {\n",
    "    'error_categories': error_categories,\n",
    "    'domain_analysis': df_domain_analysis.to_dict('records'),\n",
    "    'top_domain_confusions': domain_confusion_pairs.head(10).to_dict('records'),\n",
    "    'avg_confidence_correct': avg_conf_correct,\n",
    "    'avg_confidence_wrong': avg_conf_wrong,\n",
    "    'low_confidence_samples': len(df_low_conf),\n",
    "    'low_confidence_error_rate': len(low_conf_wrong) / len(df_low_conf) if len(df_low_conf) > 0 else 0,\n",
    "    'high_confidence_samples': len(df_high_conf),\n",
    "    'high_confidence_accuracy': len(high_conf_correct) / len(df_high_conf) if len(df_high_conf) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… HATA ANALÄ°ZÄ° TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š HÃœCRE 22 - PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 1\n",
    "Confusion Matrices ve Performance Charts\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ã‡Ä±ktÄ± klasÃ¶rÃ¼\n",
    "output_folder = \"hierarchical_validation_test_results\"\n",
    "viz_folder = os.path.join(output_folder, \"visualizations\")\n",
    "os.makedirs(viz_folder, exist_ok=True)\n",
    "\n",
    "# Custom color palette\n",
    "COLORS = {\n",
    "    'correct': '#2ecc71',      # Green\n",
    "    'wrong': '#e74c3c',        # Red\n",
    "    'domain_correct': '#3498db',  # Blue\n",
    "    'mask_wrong': '#f39c12',   # Orange\n",
    "    'neutral': '#95a5a6',      # Gray\n",
    "    'background': '#2c3e50',   # Dark blue\n",
    "    'accent': '#9b59b6'        # Purple\n",
    "}\n",
    "\n",
    "# Profesyonel style ayarlarÄ±\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# =============================================\n",
    "# 1ï¸âƒ£ STAGE 1 CONFUSION MATRIX (5x5 Domain)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Stage 1 Confusion Matrix oluÅŸturuluyor...\")\n",
    "\n",
    "# Confusion matrix hesapla\n",
    "true_domains = df_results['true_domain_label'].values\n",
    "pred_domains = df_results['pred_domain_label'].values\n",
    "\n",
    "cm_domain = confusion_matrix(true_domains, pred_domains, labels=range(NUM_DOMAINS))\n",
    "cm_domain_normalized = cm_domain.astype('float') / cm_domain.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Custom colormap\n",
    "cmap_counts = LinearSegmentedColormap.from_list('custom_blues', ['#ffffff', '#3498db', '#1a5276'])\n",
    "cmap_pct = LinearSegmentedColormap.from_list('custom_greens', ['#ffffff', '#27ae60', '#145a32'])\n",
    "\n",
    "# Sol: Raw counts\n",
    "sns.heatmap(cm_domain, annot=True, fmt='d', cmap=cmap_counts,\n",
    "            xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "            ax=axes[0], cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
    "            annot_kws={'size': 12, 'weight': 'bold'},\n",
    "            linewidths=0.5, linecolor='white')\n",
    "axes[0].set_xlabel('Predicted Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_ylabel('True Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_title('Stage 1: Domain Classification\\n(Absolute Counts)', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[0].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "axes[0].tick_params(axis='y', rotation=0, labelsize=11)\n",
    "\n",
    "# SaÄŸ: Percentages\n",
    "sns.heatmap(cm_domain_normalized, annot=True, fmt='.1f', cmap=cmap_pct,\n",
    "            xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "            ax=axes[1], cbar_kws={'label': 'Percentage (%)', 'shrink': 0.8},\n",
    "            annot_kws={'size': 12, 'weight': 'bold'},\n",
    "            linewidths=0.5, linecolor='white', vmin=0, vmax=100)\n",
    "axes[1].set_xlabel('Predicted Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[1].set_ylabel('True Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[1].set_title('Stage 1: Domain Classification\\n(Row-Normalized %)', fontsize=15, fontweight='bold', pad=15)\n",
    "axes[1].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "axes[1].tick_params(axis='y', rotation=0, labelsize=11)\n",
    "\n",
    "# Accuracy text\n",
    "overall_acc = np.trace(cm_domain) / cm_domain.sum()\n",
    "fig.suptitle(f'Overall Domain Classification Accuracy: {overall_acc*100:.2f}%', \n",
    "             fontsize=16, fontweight='bold', y=1.02, color=COLORS['background'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'stage1_domain_confusion_matrix.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Stage 1 confusion matrix kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 2ï¸âƒ£ STAGE 2 CONFUSION MATRICES (Her domain iÃ§in)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Stage 2 Confusion Matrices oluÅŸturuluyor...\")\n",
    "\n",
    "# Her domain iÃ§in ayrÄ± confusion matrix\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, domain in enumerate(DOMAIN_NAMES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Bu domain iÃ§in doÄŸru tahmin edilen Ã¶rnekleri al\n",
    "    df_domain_correct = df_results[(df_results['true_domain_name'] == domain) & \n",
    "                                    (df_results['domain_correct'] == True)]\n",
    "    \n",
    "    if len(df_domain_correct) == 0:\n",
    "        ax.text(0.5, 0.5, f'{domain}\\nNo data', ha='center', va='center', fontsize=14)\n",
    "        ax.set_title(f'{domain} - No Data')\n",
    "        continue\n",
    "    \n",
    "    # Mask labels (true and predicted)\n",
    "    true_masks = df_domain_correct['true_mask_label'].values\n",
    "    pred_masks = df_domain_correct['pred_mask_label'].values\n",
    "    \n",
    "    # Unique labels\n",
    "    unique_labels = sorted(set(true_masks) | set(pred_masks))\n",
    "    \n",
    "    if len(unique_labels) == 0:\n",
    "        ax.text(0.5, 0.5, f'{domain}\\nNo valid masks', ha='center', va='center', fontsize=14)\n",
    "        continue\n",
    "    \n",
    "    # Mask names for this domain\n",
    "    l2m = stage2_models[domain]['label_to_mask'] if domain in stage2_models else {}\n",
    "    mask_names = [l2m.get(l) or l2m.get(str(l), f\"Mask{l}\") for l in unique_labels]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm_mask = confusion_matrix(true_masks, pred_masks, labels=unique_labels)\n",
    "    cm_mask_norm = cm_mask.astype('float') / (cm_mask.sum(axis=1)[:, np.newaxis] + 1e-10) * 100\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(cm_mask_norm, annot=True, fmt='.0f', cmap='YlOrRd',\n",
    "                xticklabels=mask_names, yticklabels=mask_names,\n",
    "                ax=ax, cbar_kws={'shrink': 0.6},\n",
    "                annot_kws={'size': max(6, 10 - len(unique_labels)//3)},\n",
    "                linewidths=0.5, linecolor='white', vmin=0, vmax=100)\n",
    "    \n",
    "    # Accuracy for this domain\n",
    "    domain_mask_acc = np.trace(cm_mask) / cm_mask.sum() if cm_mask.sum() > 0 else 0\n",
    "    \n",
    "    ax.set_xlabel('Predicted Mask', fontsize=10, fontweight='bold')\n",
    "    ax.set_ylabel('True Mask', fontsize=10, fontweight='bold')\n",
    "    ax.set_title(f'{domain}\\nMask Accuracy: {domain_mask_acc*100:.1f}%\\n({len(df_domain_correct):,} samples)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=max(6, 9 - len(unique_labels)//3))\n",
    "    ax.tick_params(axis='y', rotation=0, labelsize=max(6, 9 - len(unique_labels)//3))\n",
    "\n",
    "# Son subplot'u gizle\n",
    "axes[5].axis('off')\n",
    "\n",
    "# BaÅŸlÄ±k\n",
    "fig.suptitle('Stage 2: Mask Detection Confusion Matrices by Domain\\n(Only Domain-Correct Samples)', \n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'stage2_mask_confusion_matrices.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Stage 2 confusion matrices kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 3ï¸âƒ£ PER-DOMAIN ACCURACY BAR CHART\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Per-domain accuracy chart oluÅŸturuluyor...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(DOMAIN_NAMES))\n",
    "width = 0.28\n",
    "\n",
    "# Accuracy values from domain analysis\n",
    "s1_accs = [df_domain_analysis[df_domain_analysis['domain'] == d]['stage1_accuracy'].values[0] \n",
    "           if d in df_domain_analysis['domain'].values else 0 for d in DOMAIN_NAMES]\n",
    "s2_accs = [df_domain_analysis[df_domain_analysis['domain'] == d]['stage2_accuracy'].values[0]\n",
    "           if d in df_domain_analysis['domain'].values else 0 for d in DOMAIN_NAMES]\n",
    "pipeline_accs = [df_domain_analysis[df_domain_analysis['domain'] == d]['pipeline_accuracy'].values[0]\n",
    "                 if d in df_domain_analysis['domain'].values else 0 for d in DOMAIN_NAMES]\n",
    "\n",
    "# Bars\n",
    "bars1 = ax.bar(x - width, s1_accs, width, label='Stage 1 (Domain)', color=COLORS['domain_correct'], \n",
    "               edgecolor='white', linewidth=2, alpha=0.85)\n",
    "bars2 = ax.bar(x, s2_accs, width, label='Stage 2 (Mask)', color=COLORS['correct'], \n",
    "               edgecolor='white', linewidth=2, alpha=0.85)\n",
    "bars3 = ax.bar(x + width, pipeline_accs, width, label='Full Pipeline', color=COLORS['accent'], \n",
    "               edgecolor='white', linewidth=2, alpha=0.85)\n",
    "\n",
    "# Target line\n",
    "ax.axhline(y=0.85, color=COLORS['wrong'], linestyle='--', linewidth=2, \n",
    "          alpha=0.7, label='Target (85%)')\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_ylabel('Accuracy', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_title('Hierarchical Pipeline Performance by Domain', fontsize=16, fontweight='bold', pad=15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(DOMAIN_NAMES, fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right', framealpha=0.9)\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bar labels\n",
    "for bars, values in [(bars1, s1_accs), (bars2, s2_accs), (bars3, pipeline_accs)]:\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{val*100:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold', rotation=0)\n",
    "\n",
    "# Overall stats annotation\n",
    "textstr = f'Overall Stage 1: {domain_accuracy*100:.1f}%\\nOverall Stage 2: {mask_accuracy_on_correct*100:.1f}%\\nOverall Pipeline: {pipeline_accuracy*100:.1f}%'\n",
    "props = dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9, edgecolor=COLORS['background'])\n",
    "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'per_domain_accuracy.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Per-domain accuracy chart kaydedildi\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… GÃ–RSELLEÅTÄ°RME BÃ–LÃœM 1 TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š HÃœCRE 23 - PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 2\n",
    "Error Flow Diagram, Confidence Distributions, Error Breakdown\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š PROFESYONEL GÃ–RSELLEÅTÄ°RME - BÃ–LÃœM 2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 4ï¸âƒ£ ERROR FLOW DIAGRAM (Sankey-style)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Error Flow Diagram oluÅŸturuluyor...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Hesaplamalar\n",
    "n_total = len(df_results)\n",
    "n_s1_correct = len(df_stage1_correct_stage2_correct) + len(df_stage1_correct_stage2_wrong)\n",
    "n_s1_wrong = len(df_stage1_wrong)\n",
    "n_s2_correct = len(df_stage1_correct_stage2_correct)\n",
    "n_s2_wrong = len(df_stage1_correct_stage2_wrong)\n",
    "\n",
    "# Stage konumlarÄ±\n",
    "stages = ['Input\\nSamples', 'Stage 1\\nDomain', 'Stage 2\\nMask', 'Output']\n",
    "stage_x = [0, 1, 2, 3]\n",
    "\n",
    "# Y pozisyonlarÄ± (akÄ±ÅŸ yÃ¼kseklikleri)\n",
    "y_positions = {\n",
    "    'total': 0.5,\n",
    "    's1_correct': 0.65,\n",
    "    's1_wrong': 0.25,\n",
    "    's2_correct': 0.75,\n",
    "    's2_wrong': 0.55,\n",
    "    'final_correct': 0.75,\n",
    "    'final_wrong': 0.35\n",
    "}\n",
    "\n",
    "# Box'larÄ± Ã§iz\n",
    "def draw_box(ax, x, y, width, height, color, label, count, pct):\n",
    "    rect = plt.Rectangle((x - width/2, y - height/2), width, height, \n",
    "                         facecolor=color, edgecolor='white', linewidth=2, alpha=0.85)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, f'{label}\\n{count:,}\\n({pct:.1f}%)', \n",
    "            ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# AkÄ±ÅŸ Ã§izgileri\n",
    "def draw_flow(ax, x1, y1, x2, y2, width, color, alpha=0.4):\n",
    "    from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
    "    # Basit Ã§izgi Ã§iz\n",
    "    ax.fill([x1, x2, x2, x1], [y1-width/2, y2-width/2, y2+width/2, y1+width/2], \n",
    "            color=color, alpha=alpha, linewidth=0)\n",
    "\n",
    "# Total input\n",
    "draw_box(ax, 0, 0.5, 0.3, 0.35, COLORS['neutral'], 'Total Input', n_total, 100)\n",
    "\n",
    "# Stage 1 outputs\n",
    "s1_correct_pct = n_s1_correct/n_total*100\n",
    "s1_wrong_pct = n_s1_wrong/n_total*100\n",
    "draw_box(ax, 1, 0.7, 0.3, 0.25, COLORS['domain_correct'], 'Domain\\nCorrect', n_s1_correct, s1_correct_pct)\n",
    "draw_box(ax, 1, 0.25, 0.3, 0.2, COLORS['wrong'], 'Domain\\nWrong', n_s1_wrong, s1_wrong_pct)\n",
    "\n",
    "# Stage 2 outputs (sadece S1 doÄŸru olanlar iÃ§in)\n",
    "s2_correct_pct = n_s2_correct/n_total*100\n",
    "s2_wrong_pct = n_s2_wrong/n_total*100\n",
    "draw_box(ax, 2, 0.8, 0.3, 0.2, COLORS['correct'], 'Mask\\nCorrect', n_s2_correct, s2_correct_pct)\n",
    "draw_box(ax, 2, 0.55, 0.3, 0.18, COLORS['mask_wrong'], 'Mask\\nWrong', n_s2_wrong, s2_wrong_pct)\n",
    "\n",
    "# Final outputs\n",
    "final_correct = n_s2_correct\n",
    "final_wrong = n_s1_wrong + n_s2_wrong\n",
    "final_correct_pct = final_correct/n_total*100\n",
    "final_wrong_pct = final_wrong/n_total*100\n",
    "draw_box(ax, 3, 0.8, 0.3, 0.2, COLORS['correct'], 'SUCCESS', final_correct, final_correct_pct)\n",
    "draw_box(ax, 3, 0.3, 0.3, 0.3, COLORS['wrong'], 'FAILED', final_wrong, final_wrong_pct)\n",
    "\n",
    "# AkÄ±ÅŸ Ã§izgileri\n",
    "# Input â†’ S1 Correct\n",
    "draw_flow(ax, 0.15, 0.6, 0.85, 0.7, 0.15, COLORS['domain_correct'])\n",
    "# Input â†’ S1 Wrong\n",
    "draw_flow(ax, 0.15, 0.4, 0.85, 0.25, 0.1, COLORS['wrong'])\n",
    "# S1 Correct â†’ S2 Correct\n",
    "draw_flow(ax, 1.15, 0.75, 1.85, 0.8, 0.1, COLORS['correct'])\n",
    "# S1 Correct â†’ S2 Wrong\n",
    "draw_flow(ax, 1.15, 0.65, 1.85, 0.55, 0.08, COLORS['mask_wrong'])\n",
    "# S1 Wrong â†’ Final Wrong\n",
    "draw_flow(ax, 1.15, 0.25, 2.85, 0.3, 0.08, COLORS['wrong'], alpha=0.3)\n",
    "# S2 Correct â†’ Final Correct\n",
    "draw_flow(ax, 2.15, 0.8, 2.85, 0.8, 0.1, COLORS['correct'])\n",
    "# S2 Wrong â†’ Final Wrong\n",
    "draw_flow(ax, 2.15, 0.55, 2.85, 0.35, 0.08, COLORS['mask_wrong'], alpha=0.3)\n",
    "\n",
    "# Oklar ve etiketler\n",
    "ax.annotate('', xy=(0.85, 0.7), xytext=(0.15, 0.55), \n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['domain_correct'], lw=2))\n",
    "ax.annotate('', xy=(0.85, 0.25), xytext=(0.15, 0.45), \n",
    "            arrowprops=dict(arrowstyle='->', color=COLORS['wrong'], lw=2))\n",
    "\n",
    "# Stage labels\n",
    "for i, stage in enumerate(stages):\n",
    "    ax.text(i, -0.05, stage, ha='center', va='top', fontsize=12, fontweight='bold', \n",
    "            color=COLORS['background'])\n",
    "\n",
    "# BaÅŸlÄ±k ve legend\n",
    "ax.set_xlim(-0.3, 3.3)\n",
    "ax.set_ylim(-0.1, 1.05)\n",
    "ax.axis('off')\n",
    "ax.set_title('Hierarchical Pipeline Error Flow\\n(Stage 1 â†’ Stage 2 Error Propagation)', \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(color=COLORS['correct'], label=f'Success: {final_correct:,} ({final_correct_pct:.1f}%)'),\n",
    "    mpatches.Patch(color=COLORS['wrong'], label=f'Stage 1 Error: {n_s1_wrong:,} ({s1_wrong_pct:.1f}%)'),\n",
    "    mpatches.Patch(color=COLORS['mask_wrong'], label=f'Stage 2 Error: {n_s2_wrong:,} ({s2_wrong_pct:.1f}%)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=11, \n",
    "         framealpha=0.9, bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'error_flow_diagram.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Error flow diagram kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 5ï¸âƒ£ CONFIDENCE DISTRIBUTION PLOTS\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ Confidence Distribution Plots oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 5a. Domain Confidence - Correct vs Wrong\n",
    "ax = axes[0, 0]\n",
    "correct_confs = df_results[df_results['domain_correct'] == True]['domain_confidence'].values\n",
    "wrong_confs = df_results[df_results['domain_correct'] == False]['domain_confidence'].values\n",
    "\n",
    "ax.hist(correct_confs, bins=50, alpha=0.7, color=COLORS['correct'], \n",
    "        label=f'Correct ({len(correct_confs):,})', density=True, edgecolor='white')\n",
    "ax.hist(wrong_confs, bins=50, alpha=0.7, color=COLORS['wrong'], \n",
    "        label=f'Wrong ({len(wrong_confs):,})', density=True, edgecolor='white')\n",
    "ax.axvline(x=0.9, color=COLORS['background'], linestyle='--', linewidth=2, label='High Conf (0.9)')\n",
    "ax.axvline(x=correct_confs.mean(), color=COLORS['correct'], linestyle=':', linewidth=2)\n",
    "ax.axvline(x=wrong_confs.mean() if len(wrong_confs) > 0 else 0, color=COLORS['wrong'], linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Domain Confidence', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stage 1: Domain Prediction Confidence\\n(Correct vs Wrong)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5b. Mask Confidence (Domain Correct olanlar iÃ§in)\n",
    "ax = axes[0, 1]\n",
    "mask_correct_confs = df_stage1_correct_stage2_correct['mask_confidence'].values\n",
    "mask_wrong_confs = df_stage1_correct_stage2_wrong['mask_confidence'].values\n",
    "\n",
    "if len(mask_correct_confs) > 0:\n",
    "    ax.hist(mask_correct_confs, bins=50, alpha=0.7, color=COLORS['correct'], \n",
    "            label=f'Mask Correct ({len(mask_correct_confs):,})', density=True, edgecolor='white')\n",
    "if len(mask_wrong_confs) > 0:\n",
    "    ax.hist(mask_wrong_confs, bins=50, alpha=0.7, color=COLORS['mask_wrong'], \n",
    "            label=f'Mask Wrong ({len(mask_wrong_confs):,})', density=True, edgecolor='white')\n",
    "ax.axvline(x=0.9, color=COLORS['background'], linestyle='--', linewidth=2, label='High Conf (0.9)')\n",
    "ax.set_xlabel('Mask Confidence', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stage 2: Mask Prediction Confidence\\n(Domain Correct Samples Only)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5c. Box Plot - Domain Confidence by Domain\n",
    "ax = axes[1, 0]\n",
    "domain_conf_data = []\n",
    "domain_labels = []\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_dom = df_results[df_results['true_domain_name'] == domain]\n",
    "    domain_conf_data.append(df_dom['domain_confidence'].values)\n",
    "    domain_labels.append(domain)\n",
    "\n",
    "bp = ax.boxplot(domain_conf_data, labels=domain_labels, patch_artist=True)\n",
    "colors_box = [COLORS['domain_correct'], COLORS['correct'], COLORS['mask_wrong'], \n",
    "              COLORS['accent'], COLORS['neutral']]\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_xlabel('Domain', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Domain Confidence', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Domain Confidence Distribution by True Domain', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5d. Confidence vs Error Rate\n",
    "ax = axes[1, 1]\n",
    "conf_bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (conf_bins[:-1] + conf_bins[1:]) / 2\n",
    "error_rates = []\n",
    "sample_counts = []\n",
    "\n",
    "for i in range(len(conf_bins) - 1):\n",
    "    mask = (df_results['domain_confidence'] >= conf_bins[i]) & (df_results['domain_confidence'] < conf_bins[i+1])\n",
    "    df_bin = df_results[mask]\n",
    "    if len(df_bin) > 0:\n",
    "        error_rate = 1 - df_bin['domain_correct'].mean()\n",
    "        error_rates.append(error_rate)\n",
    "        sample_counts.append(len(df_bin))\n",
    "    else:\n",
    "        error_rates.append(0)\n",
    "        sample_counts.append(0)\n",
    "\n",
    "# Bar plot\n",
    "bars = ax.bar(bin_centers, error_rates, width=0.08, color=COLORS['wrong'], \n",
    "              edgecolor='white', alpha=0.85)\n",
    "\n",
    "# Sample count as secondary y-axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(bin_centers, sample_counts, color=COLORS['domain_correct'], \n",
    "         marker='o', linewidth=2, label='Sample Count')\n",
    "ax2.set_ylabel('Sample Count', fontsize=12, fontweight='bold', color=COLORS['domain_correct'])\n",
    "ax2.tick_params(axis='y', labelcolor=COLORS['domain_correct'])\n",
    "\n",
    "ax.set_xlabel('Confidence Bin', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Error Rate', fontsize=12, fontweight='bold', color=COLORS['wrong'])\n",
    "ax.tick_params(axis='y', labelcolor=COLORS['wrong'])\n",
    "ax.set_title('Domain Error Rate vs Confidence\\n(Lower confidence = Higher error)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim([0, max(error_rates)*1.2 if error_rates else 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'confidence_distributions.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Confidence distribution plots kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 6ï¸âƒ£ ERROR BREAKDOWN PIE CHART\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ Error Breakdown Pie Chart oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Sol: Genel breakdown\n",
    "ax = axes[0]\n",
    "labels = ['Stage1âœ“ Stage2âœ“\\n(Full Success)', 'Stage1âœ“ Stage2âœ—\\n(Mask Error)', 'Stage1âœ—\\n(Domain Error)']\n",
    "sizes = [len(df_stage1_correct_stage2_correct), len(df_stage1_correct_stage2_wrong), len(df_stage1_wrong)]\n",
    "colors = [COLORS['correct'], COLORS['mask_wrong'], COLORS['wrong']]\n",
    "explode = (0.02, 0.02, 0.05)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                                   autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                                   textprops={'fontsize': 11, 'fontweight': 'bold'},\n",
    "                                   wedgeprops={'edgecolor': 'white', 'linewidth': 2})\n",
    "ax.set_title(f'Pipeline Result Distribution\\n(Total: {n_total:,} samples)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# SaÄŸ: Domain bazÄ±nda error breakdown\n",
    "ax = axes[1]\n",
    "domain_errors = []\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_dom = df_results[df_results['true_domain_name'] == domain]\n",
    "    domain_errors.append(len(df_dom[df_dom['domain_correct'] == False]))\n",
    "\n",
    "bars = ax.barh(DOMAIN_NAMES, domain_errors, color=COLORS['wrong'], \n",
    "               edgecolor='white', alpha=0.85)\n",
    "ax.set_xlabel('Number of Domain Errors', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Domain', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stage 1 Domain Errors by True Domain', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Bar labels\n",
    "for bar, err in zip(bars, domain_errors):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 10, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{err:,}',\n",
    "            ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(viz_folder, 'error_breakdown.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Error breakdown pie chart kaydedildi\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… GÃ–RSELLEÅTÄ°RME BÃ–LÃœM 2 TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ’¾ HÃœCRE 25 - SONUÃ‡LARI DIÅA AKTAR VE Ã–ZET RAPOR\n",
    "CSV, JSON ve HTML formatlarÄ±nda sonuÃ§larÄ± kaydet\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ’¾ SONUÃ‡LARI DIÅA AKTAR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 8ï¸âƒ£ CSV EXPORT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n8ï¸âƒ£ DetaylÄ± sonuÃ§lar CSV'ye kaydediliyor...\")\n",
    "\n",
    "# Main results CSV\n",
    "csv_path = os.path.join(output_folder, 'detailed_results.csv')\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"   âœ… DetaylÄ± sonuÃ§lar: {csv_path}\")\n",
    "\n",
    "# Domain analysis CSV\n",
    "domain_csv_path = os.path.join(output_folder, 'domain_analysis.csv')\n",
    "df_domain_analysis.to_csv(domain_csv_path, index=False)\n",
    "print(f\"   âœ… Domain analizi: {domain_csv_path}\")\n",
    "\n",
    "# Error pairs CSV\n",
    "error_pairs_path = os.path.join(output_folder, 'domain_confusion_pairs.csv')\n",
    "domain_confusion_pairs.to_csv(error_pairs_path, index=False)\n",
    "print(f\"   âœ… Domain karÄ±ÅŸÄ±klÄ±k Ã§iftleri: {error_pairs_path}\")\n",
    "\n",
    "# =============================================\n",
    "# 9ï¸âƒ£ JSON SUMMARY EXPORT\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n9ï¸âƒ£ Ã–zet metrikler JSON'a kaydediliyor...\")\n",
    "\n",
    "summary_json = {\n",
    "    'metadata': {\n",
    "        'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'notebook': NOTEBOOK_NAME,\n",
    "        'total_validation_samples': int(total_samples)\n",
    "    },\n",
    "    'stage1_domain_classification': {\n",
    "        'accuracy': float(domain_accuracy),\n",
    "        'correct_count': int(domain_correct_count),\n",
    "        'wrong_count': int(total_samples - domain_correct_count),\n",
    "        'per_domain_accuracy': {\n",
    "            row['domain']: float(row['stage1_accuracy']) \n",
    "            for _, row in df_domain_analysis.iterrows()\n",
    "        }\n",
    "    },\n",
    "    'stage2_mask_detection': {\n",
    "        'accuracy_on_correct_domain': float(mask_accuracy_on_correct),\n",
    "        'correct_count': int(mask_correct_count),\n",
    "        'wrong_count': int(len(df_domain_correct) - mask_correct_count) if len(df_domain_correct) > 0 else 0,\n",
    "        'per_domain_accuracy': {\n",
    "            row['domain']: float(row['stage2_accuracy']) \n",
    "            for _, row in df_domain_analysis.iterrows()\n",
    "        }\n",
    "    },\n",
    "    'full_pipeline': {\n",
    "        'accuracy': float(pipeline_accuracy),\n",
    "        'correct_count': int(pipeline_correct_count),\n",
    "        'wrong_count': int(total_samples - pipeline_correct_count),\n",
    "        'per_domain_accuracy': {\n",
    "            row['domain']: float(row['pipeline_accuracy']) \n",
    "            for _, row in df_domain_analysis.iterrows()\n",
    "        }\n",
    "    },\n",
    "    'error_breakdown': {\n",
    "        'stage1_correct_stage2_correct': int(len(df_stage1_correct_stage2_correct)),\n",
    "        'stage1_correct_stage2_wrong': int(len(df_stage1_correct_stage2_wrong)),\n",
    "        'stage1_wrong': int(len(df_stage1_wrong))\n",
    "    },\n",
    "    'confidence_analysis': {\n",
    "        'avg_confidence_correct_domain': float(avg_conf_correct),\n",
    "        'avg_confidence_wrong_domain': float(avg_conf_wrong),\n",
    "        'low_confidence_threshold': 0.7,\n",
    "        'low_confidence_samples': int(len(df_low_conf)),\n",
    "        'high_confidence_threshold': 0.9,\n",
    "        'high_confidence_samples': int(len(df_high_conf))\n",
    "    },\n",
    "    'top_domain_confusions': [\n",
    "        {\n",
    "            'true_domain': row['true_domain_name'],\n",
    "            'pred_domain': row['pred_domain_name'],\n",
    "            'count': int(row['count'])\n",
    "        }\n",
    "        for _, row in domain_confusion_pairs.head(5).iterrows()\n",
    "    ]\n",
    "}\n",
    "\n",
    "json_path = os.path.join(output_folder, 'summary_metrics.json')\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_json, f, indent=2, ensure_ascii=False)\n",
    "print(f\"   âœ… Ã–zet metrikler: {json_path}\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ”Ÿ FINAL SUMMARY DISPLAY\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š KAPSAMLI HÄ°YERARÅÄ°K VALÄ°DASYON TEST SONUÃ‡LARI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ“‹ TEST Ã–ZETÄ°                                 â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   ğŸ“… Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "   ğŸ“Š Toplam Test Ã–rnekleri: {total_samples:,}\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ¯ STAGE 1 - DOMAIN CLASSIFICATION                 â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   âœ… Accuracy: {domain_accuracy*100:.2f}%\n",
    "   âœ… DoÄŸru:    {domain_correct_count:,} Ã¶rnek\n",
    "   âŒ YanlÄ±ÅŸ:   {total_samples - domain_correct_count:,} Ã¶rnek\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ­ STAGE 2 - MASK DETECTION                        â•‘\n",
    "â•‘                  (Sadece Domain DoÄŸru Olanlar Ä°Ã§in)                   â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   âœ… Accuracy: {mask_accuracy_on_correct*100:.2f}%\n",
    "   âœ… DoÄŸru:    {mask_correct_count:,} Ã¶rnek\n",
    "   âŒ YanlÄ±ÅŸ:   {len(df_domain_correct) - mask_correct_count:,} Ã¶rnek\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    ğŸ”— FULL PIPELINE PERFORMANCE                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   âœ… Pipeline Accuracy: {pipeline_accuracy*100:.2f}%\n",
    "   âœ… Tam BaÅŸarÄ±:  {pipeline_correct_count:,} Ã¶rnek ({pipeline_accuracy*100:.1f}%)\n",
    "   âš ï¸ Stage 1 HatasÄ±: {len(df_stage1_wrong):,} Ã¶rnek ({len(df_stage1_wrong)/total_samples*100:.1f}%)\n",
    "   âš ï¸ Stage 2 HatasÄ±: {len(df_stage1_correct_stage2_wrong):,} Ã¶rnek ({len(df_stage1_correct_stage2_wrong)/total_samples*100:.1f}%)\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ“Š DOMAIN BAZINDA                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "print(f\"   {'Domain':<15} {'S1 Acc':>10} {'S2 Acc':>10} {'Pipeline':>10}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "for _, row in df_domain_analysis.iterrows():\n",
    "    print(f\"   {row['domain']:<15} {row['stage1_accuracy']*100:>9.1f}% {row['stage2_accuracy']*100:>9.1f}% {row['pipeline_accuracy']*100:>9.1f}%\")\n",
    "\n",
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ“‚ OLUÅTURULAN DOSYALAR                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   ğŸ“ {output_folder}/\n",
    "      â”œâ”€â”€ ğŸ“„ detailed_results.csv          (Her Ã¶rnek iÃ§in detaylÄ± sonuÃ§lar)\n",
    "      â”œâ”€â”€ ğŸ“„ domain_analysis.csv           (Domain bazÄ±nda analiz)\n",
    "      â”œâ”€â”€ ğŸ“„ domain_confusion_pairs.csv    (Domain karÄ±ÅŸÄ±klÄ±k Ã§iftleri)\n",
    "      â”œâ”€â”€ ğŸ“„ summary_metrics.json          (Ã–zet metrikler)\n",
    "      â”‚\n",
    "      â””â”€â”€ ğŸ“ visualizations/\n",
    "          â”œâ”€â”€ ğŸ“Š stage1_domain_confusion_matrix.png\n",
    "          â”œâ”€â”€ ğŸ“Š stage2_mask_confusion_matrices.png\n",
    "          â”œâ”€â”€ ğŸ“Š per_domain_accuracy.png\n",
    "          â”œâ”€â”€ ğŸ“Š error_flow_diagram.png\n",
    "          â”œâ”€â”€ ğŸ“Š confidence_distributions.png\n",
    "          â”œâ”€â”€ ğŸ“Š error_breakdown.png\n",
    "          â”œâ”€â”€ ğŸ–¼ï¸ misclassified_stage1_errors.png\n",
    "          â”œâ”€â”€ ğŸ–¼ï¸ misclassified_stage2_errors.png\n",
    "          â””â”€â”€ ğŸ–¼ï¸ success_samples.png\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                         ğŸ’¡ Ã–NERÄ°LER                                   â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Recommendations based on results\n",
    "if domain_accuracy < 0.85:\n",
    "    print(f\"   âš ï¸ Stage 1 accuracy dÃ¼ÅŸÃ¼k ({domain_accuracy*100:.1f}% < 85%)\")\n",
    "    print(f\"      â†’ Domain classifier modelini iyileÅŸtir\")\n",
    "    print(f\"      â†’ Daha fazla epoch veya data augmentation dene\")\n",
    "\n",
    "if mask_accuracy_on_correct < 0.75:\n",
    "    print(f\"   âš ï¸ Stage 2 accuracy dÃ¼ÅŸÃ¼k ({mask_accuracy_on_correct*100:.1f}% < 75%)\")\n",
    "    print(f\"      â†’ Domain-specific mask detector'larÄ± iyileÅŸtir\")\n",
    "    print(f\"      â†’ SÄ±nÄ±f dengesizliÄŸi iÃ§in class weighting dene\")\n",
    "\n",
    "if len(df_stage1_wrong) > total_samples * 0.2:\n",
    "    print(f\"   ğŸš¨ YÃ¼ksek Stage 1 hata oranÄ±!\")\n",
    "    print(f\"      â†’ {len(df_stage1_wrong):,} Ã¶rnek yanlÄ±ÅŸ domain'e gitti\")\n",
    "    print(f\"      â†’ En Ã§ok karÄ±ÅŸan domain Ã§iftlerini incele\")\n",
    "\n",
    "if avg_conf_wrong > 0.7:\n",
    "    print(f\"   âš ï¸ YanlÄ±ÅŸ tahminlerin confidence'Ä± yÃ¼ksek ({avg_conf_wrong:.2f})\")\n",
    "    print(f\"      â†’ Model calibration gerekebilir\")\n",
    "    print(f\"      â†’ Label smoothing veya temperature scaling dene\")\n",
    "\n",
    "print(f\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "                    âœ… TEST TAMAMLANDI!\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
