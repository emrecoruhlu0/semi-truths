{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Combined Pipeline Full Test System\n",
    "\n",
    "Bu notebook, 3-stage pipeline'Ä± kullanarak kapsamlÄ± test, gÃ¶rselleÅŸtirme ve analiz yapar:\n",
    "\n",
    "1. **Stage 0**: Real/Fake sÄ±nÄ±flandÄ±rmasÄ±\n",
    "2. **Stage 1** (Fake ise): Domain tespiti\n",
    "3. **Stage 2** (Fake ise): Spesifik mask tÃ¼rÃ¼ tespiti\n",
    "\n",
    "## Workflow\n",
    "```\n",
    "GÃ¶rÃ¼ntÃ¼ â†’ Stage 0 (Real/Fake)? \n",
    "    â”œâ”€ REAL â†’ Bitir âœ…\n",
    "    â””â”€ FAKE â†’ Stage 1 (Domain)?\n",
    "           â†’ Stage 2 (Mask)?\n",
    "           â†’ SonuÃ§ DÃ¶ndÃ¼r ğŸ¯\n",
    "```\n",
    "\n",
    "## Ã–zellikler\n",
    "- âœ… Multi-Expert Routing (Stage 1 iÃ§in)\n",
    "- âœ… KapsamlÄ± gÃ¶rselleÅŸtirmeler (tÃ¼m stage'ler iÃ§in)\n",
    "- âœ… GeliÅŸmiÅŸ hata analizi\n",
    "- âœ… Performans metrikleri (ROC, PR curves, F1 scores)\n",
    "- âœ… Ä°leri dÃ¼zey analizler\n",
    "- âœ… Esnek veri seti kullanÄ±m oranÄ±/miktarÄ± ayarlarÄ±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   VRAM: 8.0 GB\n",
      "ğŸ”¥ GPU Ä±sÄ±ndÄ±rÄ±lÄ±yor...\n",
      "âœ… GPU hazÄ±r!\n",
      "\n",
      "âœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ”§ SETUP VE IMPORTS\n",
    "Her iki notebook'tan gerekli tÃ¼m importlar\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    f1_score, precision_score, recall_score, accuracy_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU OptimizasyonlarÄ±\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# GPU warm-up\n",
    "if device.type == 'cuda':\n",
    "    print(\"ğŸ”¥ GPU Ä±sÄ±ndÄ±rÄ±lÄ±yor...\")\n",
    "    dummy = torch.randn(2000, 2000).cuda()\n",
    "    for _ in range(20):\n",
    "        _ = torch.matmul(dummy, dummy)\n",
    "    torch.cuda.synchronize()\n",
    "    del dummy\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… GPU hazÄ±r!\")\n",
    "\n",
    "print(\"\\nâœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Configuration\n",
    "\n",
    "TÃ¼m test ayarlarÄ±, model path'leri, domain tanÄ±mlarÄ± ve veri seti kullanÄ±m oranÄ±/miktarÄ± ayarlarÄ±.\n",
    "\n",
    "**NOT: TÃ¼m deÄŸiÅŸkenler varsayÄ±lan deÄŸerlere sahiptir. Sadece deÄŸiÅŸtirmek istediÄŸiniz deÄŸiÅŸkenleri ayarlayÄ±n!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 5 Domain tanÄ±mlandÄ±:\n",
      "   â€¢ URBAN: 15 masks\n",
      "   â€¢ HUMAN_BODY: 14 masks\n",
      "   â€¢ CLOTHING: 7 masks\n",
      "   â€¢ INDOOR: 8 masks\n",
      "   â€¢ BACKGROUND: 5 masks\n",
      "   TOPLAM: 49 mask tÃ¼rÃ¼\n",
      "\n",
      "âœ… Configuration yÃ¼klendi!\n",
      "   Model path'leri: âœ…\n",
      "   Veri konumlarÄ±: âœ…\n",
      "   Domain sayÄ±sÄ±: 5\n",
      "   Batch size: 64\n",
      "   Test data ratio: 10%\n",
      "   Multi-Expert Routing: âœ… Aktif\n",
      "   Threshold Pool Test: âœ… Aktif\n",
      "   Threshold Pool Values: [0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
      "   Sampling strategy: stratified\n",
      "   Output folder: combined_pipeline_test_results\\20251225_132940\n",
      "   Visualization folders: âœ… (stage0, stage1, stage2, pipeline, threshold_analysis, routing_analysis, metrics)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "âš™ï¸ CONFIGURATION\n",
    "TÃ¼m ayarlar detaylÄ± aÃ§Ä±klamalarla\n",
    "\"\"\"\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“‚ MODEL PATH'LERÄ°\n",
    "# =============================================\n",
    "\n",
    "# Stage 0: Real/Fake Model\n",
    "REAL_FAKE_MODEL = r\"C:\\Users\\emrec\\OneDrive\\MasaÃ¼stÃ¼\\Bilgisayar_Ders\\Sinif3\\Yapay_Sinir_Aglari\\yeni_modeller\\eÄŸitim_sonuÃ§larÄ±\\real_fake_detection\\models\\best_model.pth\"\n",
    "\n",
    "# Stage 1: Domain Classifier\n",
    "DOMAIN_CLASSIFIER_MODEL = r\"C:\\Users\\emrec\\OneDrive\\MasaÃ¼stÃ¼\\Bilgisayar_Ders\\Sinif3\\Yapay_Sinir_Aglari\\yeni_modeller\\eÄŸitim_sonuÃ§larÄ±\\hierarchical_mask_detection\\stage1_domain\\models\\best_model.pth\"\n",
    "\n",
    "# Stage 2: Domain-specific Mask Detectors (klasÃ¶r yolu)\n",
    "STAGE2_MODELS_DIR = r\"C:\\Users\\emrec\\OneDrive\\MasaÃ¼stÃ¼\\Bilgisayar_Ders\\Sinif3\\Yapay_Sinir_Aglari\\yeni_modeller\\eÄŸitim_sonuÃ§larÄ±\\hierarchical_mask_detection\"\n",
    "\n",
    "# Hierarchical Config (Stage 2 mappings iÃ§in)\n",
    "NOTEBOOK_NAME = \"hierarchical_mask_detection\"\n",
    "HIERARCHICAL_CONFIG_PATH = os.path.join(\"eÄŸitim_sonuÃ§larÄ±\", NOTEBOOK_NAME, \"veriler\", \"hierarchical_config.json\")\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“Š VERÄ° KONUMLARI\n",
    "# =============================================\n",
    "\n",
    "# CSV dosyalarÄ±nÄ±n konumu (meta.csv dosyalarÄ±)\n",
    "CSV_BASE = r\"C:\\AI_DATA\\SEMI_TRUTHS\\inpainting\"\n",
    "\n",
    "# Extract edilmiÅŸ gÃ¶rsellerin konumu (FAKE gÃ¶rÃ¼ntÃ¼ler iÃ§in)\n",
    "EXTRACTED_BASE = r\"C:\\AI_DATA\\SEMI_TRUTHS_extracted\\inpainting\"\n",
    "\n",
    "# Real gÃ¶rÃ¼ntÃ¼lerin konumu (REAL gÃ¶rÃ¼ntÃ¼ler iÃ§in)\n",
    "# NOT: Real gÃ¶rÃ¼ntÃ¼ler iÃ§in CSV yok, sadece gÃ¶rseller var\n",
    "# Real gÃ¶rÃ¼ntÃ¼ler iÃ§in domain ve mask bilgisi yok (Stage 0 iÃ§in kullanÄ±lÄ±r)\n",
    "# Real gÃ¶rÃ¼ntÃ¼ler muhtemelen ÅŸu klasÃ¶rlerden birinde:\n",
    "# - C:\\Users\\DeepLab\\Desktop\\Grup-17\\SEMI_TRUTHS-extracted\\original\n",
    "# - C:\\Users\\DeepLab\\Desktop\\Grup-17\\SEMI_TRUTHS\\original\n",
    "# Path'i kendi sisteminize gÃ¶re gÃ¼ncelleyin\n",
    "REAL_IMAGES_BASE = r\"C:\\Users\\DeepLab\\Desktop\\Grup-17\\SEMI_TRUTHS-extracted\\original\"\n",
    "\n",
    "# Real gÃ¶rÃ¼ntÃ¼leri kullan (True ise real gÃ¶rÃ¼ntÃ¼ler de yÃ¼klenecek)\n",
    "USE_REAL_IMAGES = True  # False yaparsanÄ±z sadece fake gÃ¶rÃ¼ntÃ¼ler kullanÄ±lÄ±r\n",
    "\n",
    "# =============================================\n",
    "# ğŸ¯ DOMAIN TANIMLARI\n",
    "# =============================================\n",
    "\n",
    "DOMAIN_CATEGORIES = {\n",
    "    'URBAN': [\n",
    "        'building', 'car', 'road', 'sidewalk', 'vegetation', 'sky', \n",
    "        'wall', 'window', 'door', 'tree', 'trees', 'pole', 'fence', \n",
    "        'parking', 'terrain'\n",
    "    ],\n",
    "    'HUMAN_BODY': [\n",
    "        'hair', 'face', 'skin', 'neck', 'nose', 'Right-arm', 'Left-arm',\n",
    "        'Right-leg', 'Left-leg', 'left_ear', 'right_ear', 'mouth', \n",
    "        'upper_lip', 'lower_lip'\n",
    "    ],\n",
    "    'CLOTHING': [\n",
    "        'Upper-clothes', 'Pants', 'skirt', 'dress', 'hat', 'bag', 'cloth'\n",
    "    ],\n",
    "    'INDOOR': [\n",
    "        'floor', 'ceiling', 'chair', 'table', 'seat', 'column', 'ground', 'grass'\n",
    "    ],\n",
    "    'BACKGROUND': [\n",
    "        'background', 'out of roi', 'ego vehicle', 'rectification border', 'static'\n",
    "    ]\n",
    "}\n",
    "\n",
    "DOMAIN_NAMES = list(DOMAIN_CATEGORIES.keys())\n",
    "NUM_DOMAINS = len(DOMAIN_NAMES)\n",
    "\n",
    "print(f\"âœ… {NUM_DOMAINS} Domain tanÄ±mlandÄ±:\")\n",
    "total_masks = 0\n",
    "for domain, masks in DOMAIN_CATEGORIES.items():\n",
    "    print(f\"   â€¢ {domain}: {len(masks)} masks\")\n",
    "    total_masks += len(masks)\n",
    "print(f\"   TOPLAM: {total_masks} mask tÃ¼rÃ¼\")\n",
    "\n",
    "# =============================================\n",
    "# âš™ï¸ PERFORMANS AYARLARI\n",
    "# =============================================\n",
    "\n",
    "# Batch size (GPU VRAM'e gÃ¶re ayarlayÄ±n)\n",
    "BATCH_SIZE = 64  # RTX 4060 iÃ§in uygun\n",
    "\n",
    "# DataLoader worker sayÄ±sÄ± (Windows'ta 0 Ã¶nerilir)\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Mixed Precision (AMP) - VRAM tasarrufu iÃ§in\n",
    "USE_AMP = True  # True Ã¶nerilir (%40 VRAM tasarrufu)\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ”„ MULTI-EXPERT ROUTING AYARLARI\n",
    "# =============================================\n",
    "\n",
    "# Multi-expert routing aktif mi? (URBAN/INDOOR/BACKGROUND karÄ±ÅŸÄ±klÄ±ÄŸÄ±nÄ± Ã§Ã¶zmek iÃ§in)\n",
    "USE_MULTI_EXPERT_ROUTING = True\n",
    "\n",
    "# KarÄ±ÅŸan domain'ler (multi-expert routing iÃ§in)\n",
    "CONFUSING_DOMAINS = ['URBAN', 'INDOOR', 'BACKGROUND']\n",
    "\n",
    "# Routing threshold (bu threshold'un altÄ±nda multi-expert devreye girer)\n",
    "ROUTING_THRESHOLD = 0.90\n",
    "\n",
    "# Hybrid scoring parametreleri\n",
    "USE_DYNAMIC_ALPHA = True  # Domain'e gÃ¶re dinamik alpha kullan\n",
    "HYBRID_ALPHA = 0.3  # USE_DYNAMIC_ALPHA=False ise kullanÄ±lÄ±r (Stage1 aÄŸÄ±rlÄ±ÄŸÄ±)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ¯ THRESHOLD POOL TEST AYARLARI\n",
    "# =============================================\n",
    "\n",
    "# Threshold pool testi yapÄ±lsÄ±n mÄ±? (FarklÄ± ROUTING_THRESHOLD deÄŸerleri test edilir)\n",
    "ENABLE_THRESHOLD_POOL_TEST = True  # True ise threshold pool testi yapÄ±lÄ±r\n",
    "\n",
    "# Test edilecek threshold deÄŸerleri (ROUTING_THRESHOLD iÃ§in)\n",
    "# EÄŸer ENABLE_THRESHOLD_POOL_TEST=True ise, bu deÄŸerler test edilir\n",
    "THRESHOLD_POOL_VALUES = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 0.99]  # Ã–rnek deÄŸerler\n",
    "\n",
    "# Threshold pool testi iÃ§in kullanÄ±lacak veri oranÄ± (hÄ±zlÄ± test iÃ§in)\n",
    "THRESHOLD_POOL_TEST_RATIO = 0.05  # Threshold pool testi iÃ§in %5 veri kullan (hÄ±zlÄ± test)\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“Š VERÄ° SETÄ° KULLANIM ORANI/MÄ°KTARI AYARLARI\n",
    "# =============================================\n",
    "\n",
    "# NOT: TÃ¼m deÄŸiÅŸkenler varsayÄ±lan deÄŸerlere sahiptir. \n",
    "# Sadece deÄŸiÅŸtirmek istediÄŸiniz deÄŸiÅŸkenleri ayarlayÄ±n!\n",
    "\n",
    "# Genel kullanÄ±m oranÄ± (0.0-1.0 arasÄ±)\n",
    "# VarsayÄ±lan: 0.25 (tÃ¼m verinin %25'i kullanÄ±lÄ±r)\n",
    "TEST_DATA_RATIO = 0.10\n",
    "\n",
    "# Real/Fake bazÄ±nda oran\n",
    "# VarsayÄ±lan: None â†’ TEST_DATA_RATIO deÄŸerini kullanÄ±r\n",
    "# Ã–rnek: REAL_DATA_RATIO = 0.5  # Real gÃ¶rÃ¼ntÃ¼lerin %50'sini kullan\n",
    "REAL_DATA_RATIO = None  # None ise TEST_DATA_RATIO kullanÄ±lÄ±r\n",
    "FAKE_DATA_RATIO = None  # None ise TEST_DATA_RATIO kullanÄ±lÄ±r\n",
    "\n",
    "# Domain bazÄ±nda oran (dictionary formatÄ±nda)\n",
    "# VarsayÄ±lan: None â†’ TÃ¼m domain'ler iÃ§in TEST_DATA_RATIO kullanÄ±lÄ±r\n",
    "# Ã–rnek: DOMAIN_RATIOS = {'URBAN': 0.3, 'HUMAN_BODY': 0.5, ...}\n",
    "DOMAIN_RATIOS = None  # None ise tÃ¼m domain'ler iÃ§in TEST_DATA_RATIO kullanÄ±lÄ±r\n",
    "\n",
    "# Minimum/Maksimum Ã¶rnek sayÄ±larÄ±\n",
    "MIN_SAMPLES_PER_DOMAIN = None  # None ise limit yok\n",
    "MAX_SAMPLES_PER_DOMAIN = None  # None ise sÄ±nÄ±rsÄ±z\n",
    "MIN_SAMPLES_TOTAL = 1000  # Minimum 1000 Ã¶rnek garanti edilir\n",
    "MAX_SAMPLES_TOTAL = 2000  # None ise sÄ±nÄ±rsÄ±z\n",
    "\n",
    "# Sampling stratejisi\n",
    "# 'random': Rastgele Ã¶rnekleme\n",
    "# 'stratified': Real/Fake ve Domain bazÄ±nda stratifiye Ã¶rnekleme (dengeli daÄŸÄ±lÄ±m) - Ã–NERÄ°LEN\n",
    "# 'balanced': Her domain ve Real/Fake iÃ§in eÅŸit sayÄ±da Ã¶rnek (mÃ¼mkÃ¼nse)\n",
    "SAMPLING_STRATEGY = 'stratified'  # VarsayÄ±lan: 'stratified'\n",
    "\n",
    "# Ã–ncelikli domain/mask seÃ§imi\n",
    "PRIORITY_DOMAINS = None  # None ise Ã¶ncelik yok, Ã¶rnek: ['URBAN', 'HUMAN_BODY']\n",
    "PRIORITY_MASKS = None  # None ise Ã¶ncelik yok\n",
    "\n",
    "# KullanÄ±m modu\n",
    "USE_RATIO_MODE = True  # True ise oran bazlÄ±, False ise miktar bazlÄ±\n",
    "TARGET_SAMPLE_COUNT = None  # USE_RATIO_MODE=False ise hedef Ã¶rnek sayÄ±sÄ±\n",
    "\n",
    "# =============================================\n",
    "# ğŸ“ Ã‡IKTI KLASÃ–RÃœ\n",
    "# =============================================\n",
    "\n",
    "# Ana output klasÃ¶rÃ¼\n",
    "BASE_OUTPUT_FOLDER = \"combined_pipeline_test_results\"\n",
    "\n",
    "# Timestamp oluÅŸtur (her test iÃ§in ayrÄ± klasÃ¶r)\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Zaman damgalÄ± test klasÃ¶rÃ¼ oluÅŸtur\n",
    "OUTPUT_FOLDER = os.path.join(BASE_OUTPUT_FOLDER, TIMESTAMP)\n",
    "VIZ_FOLDER = os.path.join(OUTPUT_FOLDER, \"visualizations\")\n",
    "\n",
    "# GÃ¶rselleÅŸtirme alt klasÃ¶rleri (daha sistematik organizasyon)\n",
    "VIZ_STAGE0 = os.path.join(VIZ_FOLDER, \"stage0\")\n",
    "VIZ_STAGE1 = os.path.join(VIZ_FOLDER, \"stage1\")\n",
    "VIZ_STAGE2 = os.path.join(VIZ_FOLDER, \"stage2\")\n",
    "VIZ_PIPELINE = os.path.join(VIZ_FOLDER, \"pipeline\")\n",
    "VIZ_THRESHOLD = os.path.join(VIZ_FOLDER, \"threshold_analysis\")\n",
    "VIZ_ROUTING = os.path.join(VIZ_FOLDER, \"routing_analysis\")\n",
    "VIZ_METRICS = os.path.join(VIZ_FOLDER, \"metrics\")\n",
    "\n",
    "# TÃ¼m klasÃ¶rleri oluÅŸtur\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(VIZ_FOLDER, exist_ok=True)\n",
    "os.makedirs(VIZ_STAGE0, exist_ok=True)\n",
    "os.makedirs(VIZ_STAGE1, exist_ok=True)\n",
    "os.makedirs(VIZ_STAGE2, exist_ok=True)\n",
    "os.makedirs(VIZ_PIPELINE, exist_ok=True)\n",
    "os.makedirs(VIZ_THRESHOLD, exist_ok=True)\n",
    "os.makedirs(VIZ_ROUTING, exist_ok=True)\n",
    "os.makedirs(VIZ_METRICS, exist_ok=True)\n",
    "\n",
    "print(f\"\\nâœ… Configuration yÃ¼klendi!\")\n",
    "print(f\"   Model path'leri: âœ…\")\n",
    "print(f\"   Veri konumlarÄ±: âœ…\")\n",
    "print(f\"   Domain sayÄ±sÄ±: {NUM_DOMAINS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Test data ratio: {TEST_DATA_RATIO*100:.0f}%\")\n",
    "print(f\"   Multi-Expert Routing: {'âœ… Aktif' if USE_MULTI_EXPERT_ROUTING else 'âŒ KapalÄ±'}\")\n",
    "print(f\"   Threshold Pool Test: {'âœ… Aktif' if ENABLE_THRESHOLD_POOL_TEST else 'âŒ KapalÄ±'}\")\n",
    "if ENABLE_THRESHOLD_POOL_TEST:\n",
    "    print(f\"   Threshold Pool Values: {THRESHOLD_POOL_VALUES}\")\n",
    "print(f\"   Sampling strategy: {SAMPLING_STRATEGY}\")\n",
    "print(f\"   Output folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"   Visualization folders: âœ… (stage0, stage1, stage2, pipeline, threshold_analysis, routing_analysis, metrics)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Model Mimarileri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model mimarileri tanÄ±mlandÄ±\n",
      "   â€¢ RealFakeModel (Stage 0)\n",
      "   â€¢ DomainClassifier (Stage 1)\n",
      "   â€¢ DomainMaskDetector (Stage 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ—ï¸ MODEL MÄ°MARÄ°LERÄ°\n",
    "Stage 0, Stage 1 ve Stage 2 iÃ§in model sÄ±nÄ±flarÄ±\n",
    "\"\"\"\n",
    "\n",
    "class RealFakeModel(nn.Module):\n",
    "    \"\"\"Stage 0: Real/Fake Classification\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.backbone.fc = nn.Linear(2048, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    \"\"\"Stage 1: Domain Classification (5 domain)\"\"\"\n",
    "    def __init__(self, num_domains=5, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        backbone_out = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(backbone_out, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "class DomainMaskDetector(nn.Module):\n",
    "    \"\"\"Stage 2: Domain-specific Mask Detection\"\"\"\n",
    "    def __init__(self, num_masks, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        backbone_out = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(backbone_out, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_masks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.head(features)\n",
    "\n",
    "print(\"âœ… Model mimarileri tanÄ±mlandÄ±\")\n",
    "print(\"   â€¢ RealFakeModel (Stage 0)\")\n",
    "print(\"   â€¢ DomainClassifier (Stage 1)\")\n",
    "print(\"   â€¢ DomainMaskDetector (Stage 2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Model YÃ¼kleme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ”„ MODELLER YÃœKLENÄ°YOR...\n",
      "======================================================================\n",
      "\n",
      "[1/7] Real/Fake model yÃ¼kleniyor...\n",
      "âœ… Real/Fake model yÃ¼klendi\n",
      "\n",
      "[2/7] Domain Classifier yÃ¼kleniyor...\n",
      "âœ… Domain Classifier yÃ¼klendi\n",
      "\n",
      "[3-7/7] Stage 2 Mask Detectors yÃ¼kleniyor...\n",
      "\n",
      "[3/7] URBAN modeli yÃ¼kleniyor...\n",
      "   âœ… URBAN (15 masks)\n",
      "\n",
      "[4/7] HUMAN_BODY modeli yÃ¼kleniyor...\n",
      "   âœ… HUMAN_BODY (14 masks)\n",
      "\n",
      "[5/7] CLOTHING modeli yÃ¼kleniyor...\n",
      "   âœ… CLOTHING (7 masks)\n",
      "\n",
      "[6/7] INDOOR modeli yÃ¼kleniyor...\n",
      "   âœ… INDOOR (8 masks)\n",
      "\n",
      "[7/7] BACKGROUND modeli yÃ¼kleniyor...\n",
      "   âœ… BACKGROUND (5 masks)\n",
      "\n",
      "âœ… TÃ¼m modeller yÃ¼klendi! (5/5 Stage 2 models)\n",
      "\n",
      "ğŸ“Š GPU Memory:\n",
      "   Allocated: 1.39 GB\n",
      "   Reserved: 1.60 GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ“¦ MODEL YÃœKLEME\n",
    "TÃ¼m stage'lerin modellerini yÃ¼kleme\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”„ MODELLER YÃœKLENÄ°YOR...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# STAGE 0: Real/Fake Model\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n[1/7] Real/Fake model yÃ¼kleniyor...\")\n",
    "model_real_fake = RealFakeModel(num_classes=2).to(device)\n",
    "checkpoint = torch.load(REAL_FAKE_MODEL, map_location=device, weights_only=False)\n",
    "\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model_real_fake.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model_real_fake.load_state_dict(checkpoint)\n",
    "\n",
    "model_real_fake.eval()\n",
    "print(\"âœ… Real/Fake model yÃ¼klendi\")\n",
    "\n",
    "# =============================================\n",
    "# STAGE 1: Domain Classifier\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n[2/7] Domain Classifier yÃ¼kleniyor...\")\n",
    "model_domain = DomainClassifier(num_domains=NUM_DOMAINS).to(device)\n",
    "checkpoint_domain = torch.load(DOMAIN_CLASSIFIER_MODEL, map_location=device, weights_only=False)\n",
    "\n",
    "if 'model_state_dict' in checkpoint_domain:\n",
    "    model_domain.load_state_dict(checkpoint_domain['model_state_dict'])\n",
    "    domain_to_label = checkpoint_domain.get('domain_to_label', {})\n",
    "    label_to_domain = checkpoint_domain.get('label_to_domain', {})\n",
    "else:\n",
    "    model_domain.load_state_dict(checkpoint_domain)\n",
    "    domain_to_label = {domain: idx for idx, domain in enumerate(DOMAIN_NAMES)}\n",
    "    label_to_domain = {idx: domain for domain, idx in domain_to_label.items()}\n",
    "\n",
    "model_domain.eval()\n",
    "print(\"âœ… Domain Classifier yÃ¼klendi\")\n",
    "\n",
    "# =============================================\n",
    "# STAGE 2: Domain-specific Mask Detectors\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n[3-7/7] Stage 2 Mask Detectors yÃ¼kleniyor...\")\n",
    "\n",
    "models_stage2 = {}\n",
    "mask_to_label = {}\n",
    "label_to_mask = {}\n",
    "\n",
    "stage2_dir = Path(STAGE2_MODELS_DIR)\n",
    "\n",
    "for idx, domain in enumerate(DOMAIN_NAMES):\n",
    "    domain_lower = domain.lower()\n",
    "    model_path = stage2_dir / f\"stage2_{domain_lower}\" / \"models\" / \"best_model.pth\"\n",
    "    \n",
    "    print(f\"\\n[{idx+3}/7] {domain} modeli yÃ¼kleniyor...\")\n",
    "    \n",
    "    if model_path.exists():\n",
    "        checkpoint_s2 = torch.load(model_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Get masks\n",
    "        masks = DOMAIN_CATEGORIES[domain]\n",
    "        num_masks = len(masks)\n",
    "        \n",
    "        # Create and load model\n",
    "        model_s2 = DomainMaskDetector(num_masks=num_masks).to(device)\n",
    "        \n",
    "        if 'model_state_dict' in checkpoint_s2:\n",
    "            model_s2.load_state_dict(checkpoint_s2['model_state_dict'])\n",
    "        else:\n",
    "            model_s2.load_state_dict(checkpoint_s2)\n",
    "        \n",
    "        model_s2.eval()\n",
    "        models_stage2[domain] = model_s2\n",
    "        \n",
    "        # Create mappings\n",
    "        mask_to_label[domain] = {mask: idx for idx, mask in enumerate(masks)}\n",
    "        label_to_mask[domain] = {idx: mask for mask, idx in mask_to_label[domain].items()}\n",
    "        \n",
    "        print(f\"   âœ… {domain} ({num_masks} masks)\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Model bulunamadÄ±: {model_path}\")\n",
    "\n",
    "print(f\"\\nâœ… TÃ¼m modeller yÃ¼klendi! ({len(models_stage2)}/{len(DOMAIN_NAMES)} Stage 2 models)\")\n",
    "\n",
    "# GPU memory kontrolÃ¼\n",
    "if device.type == 'cuda':\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\nğŸ“Š GPU Memory:\")\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Dataset HazÄ±rlama\n",
    "\n",
    "HierarchicalValidationDataset (Stage 0 label'Ä± da iÃ§erecek ÅŸekilde geniÅŸletilmiÅŸ) + Veri seti sampling fonksiyonlarÄ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“‚ DATASET HAZIRLAMA\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š CSV yÃ¼kleniyor...\n",
      "âœ… Bulunan CSV: 6 dosya\n",
      "   âœ… ADE20K/StableDiffusion_v5: 48,196 satÄ±r\n",
      "   âœ… ADE20K/StableDiffusion_XL: 48,468 satÄ±r\n",
      "   âœ… CelebAHQ/Kandinsky_2_2: 14,730 satÄ±r\n",
      "   âœ… CityScapes/Kandinsky_2_2: 44,649 satÄ±r\n",
      "   âœ… HumanParsing/OpenJourney: 29,061 satÄ±r\n",
      "   âœ… OpenImages/StableDiffusion_XL: 7,332 satÄ±r\n",
      "\n",
      "âœ… Toplam: 192,436 gÃ¶rsel\n",
      "\n",
      "ğŸ“Š Domain DaÄŸÄ±lÄ±mÄ±:\n",
      "   URBAN          : 67,523 ( 55.7%)\n",
      "   HUMAN_BODY     : 20,840 ( 17.2%)\n",
      "   CLOTHING       :  6,417 (  5.3%)\n",
      "   INDOOR         : 16,920 ( 13.9%)\n",
      "   BACKGROUND     :  9,614 (  7.9%)\n",
      "\n",
      "âœ… Domain etiketlendi: 121,314 / 192,436 gÃ¶rsel\n",
      "\n",
      "ğŸ” Real/Fake label'larÄ± hazÄ±rlanÄ±yor...\n",
      "   âœ… Real/Fake label'larÄ± eklendi\n",
      "   VarsayÄ±lan: TÃ¼m gÃ¶rÃ¼ntÃ¼ler 'fake' olarak iÅŸaretlendi\n",
      "   ğŸ’¡ GerÃ§ek uygulamada CSV'den veya ayrÄ± kaynaktan alÄ±nmalÄ±\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ DATASET HAZIRLAMA - BÃ–LÃœM 1\n",
    "CSV yÃ¼kleme, domain etiketleme ve helper fonksiyonlar\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‚ DATASET HAZIRLAMA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================\n",
    "\n",
    "def get_domain(mask_name):\n",
    "    \"\"\"Mask name'den domain'i bul\"\"\"\n",
    "    for domain, masks in DOMAIN_CATEGORIES.items():\n",
    "        if mask_name in masks:\n",
    "            return domain\n",
    "    return None\n",
    "\n",
    "# =============================================\n",
    "# CSV YÃœKLEME VE DOMAIN ETÄ°KETLEME\n",
    "# =============================================\n",
    "\n",
    "print(\"\\nğŸ“Š CSV yÃ¼kleniyor...\")\n",
    "\n",
    "csv_files = glob.glob(os.path.join(CSV_BASE, \"**\", \"*_meta.csv\"), recursive=True)\n",
    "print(f\"âœ… Bulunan CSV: {len(csv_files)} dosya\")\n",
    "\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"âŒ CSV dosyasÄ± bulunamadÄ±: {CSV_BASE}\")\n",
    "\n",
    "all_dfs = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    csv_dir = os.path.dirname(csv_file)\n",
    "    parent_folder = os.path.basename(os.path.dirname(csv_dir))\n",
    "    model_name = os.path.basename(csv_dir)\n",
    "    df['parent_dataset'] = parent_folder\n",
    "    df['model'] = model_name\n",
    "    df['dataset'] = f\"{parent_folder}_{model_name}\"\n",
    "    all_dfs.append(df)\n",
    "    print(f\"   âœ… {parent_folder}/{model_name}: {len(df):,} satÄ±r\")\n",
    "\n",
    "df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\nâœ… Toplam: {len(df_combined):,} gÃ¶rsel\")\n",
    "\n",
    "# Domain etiketleme\n",
    "df_combined['domain'] = df_combined['mask_name'].apply(get_domain)\n",
    "df_with_domain = df_combined[df_combined['domain'].notna()].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Domain DaÄŸÄ±lÄ±mÄ±:\")\n",
    "domain_counts = df_with_domain['domain'].value_counts()\n",
    "for domain in DOMAIN_NAMES:\n",
    "    count = domain_counts.get(domain, 0)\n",
    "    ratio = (count / len(df_with_domain)) * 100\n",
    "    print(f\"   {domain:<15}: {count:>6,} ({ratio:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Domain etiketlendi: {len(df_with_domain):,} / {len(df_combined):,} gÃ¶rsel\")\n",
    "\n",
    "# =============================================\n",
    "# REAL/FAKE LABEL EKLEME (Stage 0 iÃ§in)\n",
    "# =============================================\n",
    "\n",
    "# NOT: Bu aÅŸamada real/fake label'larÄ± eklemek iÃ§in CSV'de bir kolon olmasÄ± gerekir\n",
    "# Veya gÃ¶rÃ¼ntÃ¼ path'inden Ã§Ä±karÄ±labilir. Åimdilik tÃ¼m gÃ¶rÃ¼ntÃ¼leri \"fake\" olarak iÅŸaretliyoruz\n",
    "# Ã§Ã¼nkÃ¼ inpainting dataset'i fake gÃ¶rÃ¼ntÃ¼ler iÃ§eriyor.\n",
    "# GerÃ§ek uygulamada, real gÃ¶rÃ¼ntÃ¼ler iÃ§in ayrÄ± bir dataset veya CSV kolonu olmalÄ±.\n",
    "\n",
    "print(\"\\nğŸ” Real/Fake label'larÄ± hazÄ±rlanÄ±yor...\")\n",
    "# Åimdilik tÃ¼m gÃ¶rÃ¼ntÃ¼ler fake olarak iÅŸaretleniyor\n",
    "# GerÃ§ek uygulamada bu kÄ±sÄ±m CSV'den veya ayrÄ± bir kaynaktan alÄ±nmalÄ±\n",
    "df_with_domain['real_fake'] = 'fake'  # VarsayÄ±lan: fake\n",
    "df_with_domain['real_fake_label'] = 1  # 0=real, 1=fake\n",
    "\n",
    "print(f\"   âœ… Real/Fake label'larÄ± eklendi\")\n",
    "print(f\"   VarsayÄ±lan: TÃ¼m gÃ¶rÃ¼ntÃ¼ler 'fake' olarak iÅŸaretlendi\")\n",
    "print(f\"   ğŸ’¡ GerÃ§ek uygulamada CSV'den veya ayrÄ± kaynaktan alÄ±nmalÄ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Extract edilmiÅŸ dosyalar taranÄ±yor (recursive)...\n",
      "âœ… Etiket dictionary'leri oluÅŸturuldu: 6 dataset\n",
      "\n",
      "âœ… TOPLAM: 81,102 gÃ¶rÃ¼ntÃ¼ bulundu\n",
      "\n",
      "ğŸ“Š Dataset BazÄ±nda Bulunan:\n",
      "   ADE20K_StableDiffusion_XL          : 26,657 gÃ¶rÃ¼ntÃ¼\n",
      "   CelebAHQ_Kandinsky_2_2             : 13,727 gÃ¶rÃ¼ntÃ¼\n",
      "   CityScapes_Kandinsky_2_2           : 27,293 gÃ¶rÃ¼ntÃ¼\n",
      "   HumanParsing_OpenJourney           : 13,215 gÃ¶rÃ¼ntÃ¼\n",
      "   OpenImages_StableDiffusion_XL      :    210 gÃ¶rÃ¼ntÃ¼\n",
      "\n",
      "ğŸ“Š Real/Fake DaÄŸÄ±lÄ±mÄ±:\n",
      "   Real: 0 (0.0%)\n",
      "   Fake: 81,102 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ DATASET HAZIRLAMA - BÃ–LÃœM 2\n",
    "Recursive dosya tarama ve image path â†’ label mapping\n",
    "\"\"\"\n",
    "\n",
    "# Domain label mapping\n",
    "domain_to_label = {domain: idx for idx, domain in enumerate(DOMAIN_NAMES)}\n",
    "label_to_domain = {idx: domain for domain, idx in domain_to_label.items()}\n",
    "\n",
    "# Real/Fake label mapping\n",
    "real_fake_to_label = {'real': 0, 'fake': 1}\n",
    "label_to_real_fake = {0: 'real', 1: 'fake'}\n",
    "\n",
    "print(\"\\nğŸ“‚ Extract edilmiÅŸ dosyalar taranÄ±yor (recursive)...\")\n",
    "\n",
    "# Etiket sÃ¶zlÃ¼kleri (domain ve real/fake iÃ§in)\n",
    "labels_dict_by_dataset = {}\n",
    "real_fake_dict_by_dataset = {}\n",
    "mask_dict_by_dataset = {}\n",
    "\n",
    "for _, row in df_with_domain.iterrows():\n",
    "    img_id = str(row['perturbed_img_id'])\n",
    "    domain = row['domain']\n",
    "    domain_label = domain_to_label[domain]\n",
    "    real_fake = row['real_fake']\n",
    "    real_fake_label = row['real_fake_label']\n",
    "    mask_name = row['mask_name']\n",
    "    dataset_key = row['dataset']\n",
    "    \n",
    "    if dataset_key not in labels_dict_by_dataset:\n",
    "        labels_dict_by_dataset[dataset_key] = {}\n",
    "        real_fake_dict_by_dataset[dataset_key] = {}\n",
    "        mask_dict_by_dataset[dataset_key] = {}\n",
    "    \n",
    "    for ext in ['', '.png', '.jpg', '.jpeg']:\n",
    "        labels_dict_by_dataset[dataset_key][img_id + ext] = domain_label\n",
    "        real_fake_dict_by_dataset[dataset_key][img_id + ext] = real_fake_label\n",
    "        mask_dict_by_dataset[dataset_key][img_id + ext] = mask_name\n",
    "\n",
    "print(f\"âœ… Etiket dictionary'leri oluÅŸturuldu: {len(labels_dict_by_dataset)} dataset\")\n",
    "\n",
    "# Recursive dosya tarama\n",
    "all_image_paths = []\n",
    "all_domain_labels = []\n",
    "all_real_fake_labels = []\n",
    "all_mask_names = []\n",
    "found_by_dataset = {}\n",
    "\n",
    "for parent_dataset in os.listdir(EXTRACTED_BASE):\n",
    "    parent_path = os.path.join(EXTRACTED_BASE, parent_dataset)\n",
    "    if not os.path.isdir(parent_path):\n",
    "        continue\n",
    "    \n",
    "    for model_name in os.listdir(parent_path):\n",
    "        model_path = os.path.join(parent_path, model_name)\n",
    "        if not os.path.isdir(model_path):\n",
    "            continue\n",
    "        \n",
    "        dataset_key = f\"{parent_dataset}_{model_name}\"\n",
    "        \n",
    "        if dataset_key not in labels_dict_by_dataset:\n",
    "            continue\n",
    "        \n",
    "        labels_dict = labels_dict_by_dataset[dataset_key]\n",
    "        rf_dict = real_fake_dict_by_dataset[dataset_key]\n",
    "        mask_dict = mask_dict_by_dataset[dataset_key]\n",
    "        found_by_dataset[dataset_key] = 0\n",
    "        \n",
    "        # Recursive tara\n",
    "        for root, dirs, files in os.walk(model_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    base_name = file\n",
    "                    name_without_ext = os.path.splitext(file)[0]\n",
    "                    \n",
    "                    domain_label = None\n",
    "                    real_fake_label = None\n",
    "                    mask_name = None\n",
    "                    \n",
    "                    for name in [base_name, name_without_ext,\n",
    "                               name_without_ext + '.png',\n",
    "                               name_without_ext + '.jpg',\n",
    "                               name_without_ext + '.jpeg']:\n",
    "                        if name in labels_dict:\n",
    "                            domain_label = labels_dict[name]\n",
    "                            real_fake_label = rf_dict[name]\n",
    "                            mask_name = mask_dict[name]\n",
    "                            break\n",
    "                    \n",
    "                    if domain_label is not None:\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        all_image_paths.append(full_path)\n",
    "                        all_domain_labels.append(domain_label)\n",
    "                        all_real_fake_labels.append(real_fake_label)\n",
    "                        all_mask_names.append(mask_name)\n",
    "                        found_by_dataset[dataset_key] += 1\n",
    "\n",
    "print(f\"\\nâœ… TOPLAM: {len(all_image_paths):,} gÃ¶rÃ¼ntÃ¼ bulundu\")\n",
    "\n",
    "if len(all_image_paths) == 0:\n",
    "    raise FileNotFoundError(\"GÃ¶rÃ¼ntÃ¼ dosyalarÄ± bulunamadÄ±!\")\n",
    "\n",
    "# Dataset bazÄ±nda istatistik\n",
    "print(f\"\\nğŸ“Š Dataset BazÄ±nda Bulunan:\")\n",
    "for dataset_key in sorted(found_by_dataset.keys()):\n",
    "    found = found_by_dataset[dataset_key]\n",
    "    print(f\"   {dataset_key:<35}: {found:>6,} gÃ¶rÃ¼ntÃ¼\")\n",
    "\n",
    "# Real/Fake daÄŸÄ±lÄ±mÄ±\n",
    "real_count = sum(1 for label in all_real_fake_labels if label == 0)\n",
    "fake_count = sum(1 for label in all_real_fake_labels if label == 1)\n",
    "print(f\"\\nğŸ“Š Real/Fake DaÄŸÄ±lÄ±mÄ±:\")\n",
    "print(f\"   Real: {real_count:,} ({real_count/len(all_real_fake_labels)*100:.1f}%)\")\n",
    "print(f\"   Fake: {fake_count:,} ({fake_count/len(all_real_fake_labels)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HierarchicalValidationDataset sÄ±nÄ±fÄ± tanÄ±mlandÄ± (Stage 0 label'Ä± dahil)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ DATASET HAZIRLAMA - BÃ–LÃœM 3\n",
    "HierarchicalValidationDataset (Stage 0 label'Ä± da iÃ§eren geniÅŸletilmiÅŸ versiyon)\n",
    "\"\"\"\n",
    "\n",
    "class HierarchicalValidationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Hem domain hem mask hem de real/fake label'Ä±nÄ± iÃ§eren dataset.\n",
    "    Stage 0, Stage 1 ve Stage 2 testleri iÃ§in gerekli tÃ¼m bilgileri iÃ§erir.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, real_fake_labels, domain_labels, mask_names, mask_labels, \n",
    "                 domain_names_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of image file paths\n",
    "            real_fake_labels: List of real/fake label indices (0=real, 1=fake)\n",
    "            domain_labels: List of domain label indices (0-4)\n",
    "            mask_names: List of actual mask names (string)\n",
    "            mask_labels: List of mask label indices (domain-specific)\n",
    "            domain_names_list: List of domain name strings (for reference)\n",
    "            transform: Image transforms\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.real_fake_labels = real_fake_labels\n",
    "        self.domain_labels = domain_labels\n",
    "        self.mask_names = mask_names\n",
    "        self.mask_labels = mask_labels\n",
    "        self.domain_names_list = domain_names_list\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Validate lengths\n",
    "        assert len(image_paths) == len(real_fake_labels) == len(domain_labels) == len(mask_names) == len(mask_labels), \\\n",
    "            f\"Length mismatch! paths:{len(image_paths)}, rf:{len(real_fake_labels)}, \" \\\n",
    "            f\"domains:{len(domain_labels)}, mask_names:{len(mask_names)}, mask_labels:{len(mask_labels)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            real_fake_label = self.real_fake_labels[idx]\n",
    "            domain_label = self.domain_labels[idx]\n",
    "            mask_name = self.mask_names[idx]\n",
    "            mask_label = self.mask_labels[idx]\n",
    "            \n",
    "            return img, real_fake_label, domain_label, mask_label, idx  # idx for tracking\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.image_paths[idx]}: {e}\")\n",
    "            # Fallback to random valid sample\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "print(\"âœ… HierarchicalValidationDataset sÄ±nÄ±fÄ± tanÄ±mlandÄ± (Stage 0 label'Ä± dahil)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sampling fonksiyonlarÄ± tanÄ±mlandÄ±\n",
      "   â€¢ sample_dataset_stratified: Dengeli daÄŸÄ±lÄ±m (Ã¶nerilen)\n",
      "   â€¢ sample_dataset_balanced: EÅŸit sayÄ±da Ã¶rnek\n",
      "   â€¢ sample_dataset_random: Rastgele Ã¶rnekleme\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ DATASET HAZIRLAMA - BÃ–LÃœM 4\n",
    "Veri seti sampling fonksiyonlarÄ± (tÃ¼m ayarlara gÃ¶re)\n",
    "\"\"\"\n",
    "\n",
    "def sample_dataset_stratified(image_paths, real_fake_labels, domain_labels, mask_names, \n",
    "                              target_count, real_ratio=None, fake_ratio=None, \n",
    "                              domain_ratios=None, min_per_domain=None, max_per_domain=None,\n",
    "                              min_total=None, max_total=None, priority_domains=None):\n",
    "    \"\"\"\n",
    "    Stratified sampling: Real/Fake ve Domain bazÄ±nda dengeli daÄŸÄ±lÄ±m\n",
    "    \n",
    "    Returns:\n",
    "        sampled_indices: SeÃ§ilen Ã¶rneklerin index'leri\n",
    "    \"\"\"\n",
    "    # OranlarÄ± belirle\n",
    "    if real_ratio is None:\n",
    "        real_ratio = TEST_DATA_RATIO\n",
    "    if fake_ratio is None:\n",
    "        fake_ratio = TEST_DATA_RATIO\n",
    "    \n",
    "    # TÃ¼m Ã¶rnekleri kategorilere ayÄ±r\n",
    "    samples_by_category = defaultdict(list)\n",
    "    for idx, (rf_label, dom_label) in enumerate(zip(real_fake_labels, domain_labels)):\n",
    "        rf_name = 'real' if rf_label == 0 else 'fake'\n",
    "        dom_name = label_to_domain.get(dom_label, 'UNKNOWN')\n",
    "        key = (rf_name, dom_name)\n",
    "        samples_by_category[key].append(idx)\n",
    "    \n",
    "    # Her kategori iÃ§in hedef sayÄ±yÄ± hesapla\n",
    "    sampled_indices = []\n",
    "    \n",
    "    for (rf_name, dom_name), indices in samples_by_category.items():\n",
    "        # Domain oranÄ±nÄ± belirle\n",
    "        if domain_ratios and dom_name in domain_ratios:\n",
    "            dom_ratio = domain_ratios[dom_name]\n",
    "        else:\n",
    "            dom_ratio = TEST_DATA_RATIO\n",
    "        \n",
    "        # Real/Fake oranÄ±nÄ± belirle\n",
    "        if rf_name == 'real':\n",
    "            rf_ratio = real_ratio\n",
    "        else:\n",
    "            rf_ratio = fake_ratio\n",
    "        \n",
    "        # Hedef sayÄ±yÄ± hesapla\n",
    "        category_count = len(indices)\n",
    "        target = int(category_count * min(dom_ratio, rf_ratio))\n",
    "        \n",
    "        # Minimum/maksimum limitleri uygula\n",
    "        if min_per_domain:\n",
    "            target = max(target, min_per_domain)\n",
    "        if max_per_domain:\n",
    "            target = min(target, max_per_domain)\n",
    "        \n",
    "        # Ã–ncelik kontrolÃ¼\n",
    "        if priority_domains and dom_name in priority_domains:\n",
    "            target = int(target * 1.5)  # Ã–ncelikli domain'lerden %50 daha fazla\n",
    "        \n",
    "        target = min(target, category_count)\n",
    "        \n",
    "        # Random sample\n",
    "        if target > 0:\n",
    "            random.seed(42)\n",
    "            sampled = random.sample(indices, target)\n",
    "            sampled_indices.extend(sampled)\n",
    "    \n",
    "    # Toplam limitleri uygula\n",
    "    if min_total and len(sampled_indices) < min_total:\n",
    "        # Eksik Ã¶rnekleri rastgele ekle\n",
    "        remaining = [i for i in range(len(image_paths)) if i not in sampled_indices]\n",
    "        needed = min_total - len(sampled_indices)\n",
    "        if needed > 0 and remaining:\n",
    "            random.seed(42)\n",
    "            additional = random.sample(remaining, min(needed, len(remaining)))\n",
    "            sampled_indices.extend(additional)\n",
    "    \n",
    "    if max_total and len(sampled_indices) > max_total:\n",
    "        random.seed(42)\n",
    "        sampled_indices = random.sample(sampled_indices, max_total)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "def sample_dataset_balanced(image_paths, real_fake_labels, domain_labels, mask_names,\n",
    "                           target_count, min_per_domain=None, max_per_domain=None):\n",
    "    \"\"\"\n",
    "    Balanced sampling: Her kategori iÃ§in eÅŸit sayÄ±da Ã¶rnek (mÃ¼mkÃ¼nse)\n",
    "    \"\"\"\n",
    "    # Kategorilere ayÄ±r\n",
    "    samples_by_category = defaultdict(list)\n",
    "    for idx, (rf_label, dom_label) in enumerate(zip(real_fake_labels, domain_labels)):\n",
    "        rf_name = 'real' if rf_label == 0 else 'fake'\n",
    "        dom_name = label_to_domain.get(dom_label, 'UNKNOWN')\n",
    "        key = (rf_name, dom_name)\n",
    "        samples_by_category[key].append(idx)\n",
    "    \n",
    "    # Her kategori iÃ§in eÅŸit sayÄ± hesapla\n",
    "    num_categories = len(samples_by_category)\n",
    "    samples_per_category = target_count // num_categories if num_categories > 0 else 0\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for (rf_name, dom_name), indices in samples_by_category.items():\n",
    "        target = samples_per_category\n",
    "        \n",
    "        # Domain limitleri\n",
    "        if min_per_domain:\n",
    "            target = max(target, min_per_domain)\n",
    "        if max_per_domain:\n",
    "            target = min(target, max_per_domain)\n",
    "        \n",
    "        target = min(target, len(indices))\n",
    "        \n",
    "        if target > 0:\n",
    "            random.seed(42)\n",
    "            sampled = random.sample(indices, target)\n",
    "            sampled_indices.extend(sampled)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "def sample_dataset_random(image_paths, target_count):\n",
    "    \"\"\"\n",
    "    Random sampling: Rastgele Ã¶rnekleme\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    indices = list(range(len(image_paths)))\n",
    "    sampled = random.sample(indices, min(target_count, len(indices)))\n",
    "    return sampled\n",
    "\n",
    "print(\"âœ… Sampling fonksiyonlarÄ± tanÄ±mlandÄ±\")\n",
    "print(\"   â€¢ sample_dataset_stratified: Dengeli daÄŸÄ±lÄ±m (Ã¶nerilen)\")\n",
    "print(\"   â€¢ sample_dataset_balanced: EÅŸit sayÄ±da Ã¶rnek\")\n",
    "print(\"   â€¢ sample_dataset_random: Rastgele Ã¶rnekleme\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Stage 2 mappings yÃ¼kleniyor...\n",
      "   âœ… Config yÃ¼klendi: 5 domain\n",
      "\n",
      "ğŸ” Mask label'larÄ± hazÄ±rlanÄ±yor...\n",
      "   âœ… 81,102 mask label hazÄ±rlandÄ±\n",
      "\n",
      "ğŸ“Š Sampling uygulanÄ±yor...\n",
      "   Strateji: stratified\n",
      "   Hedef oran: 10%\n",
      "   Hedef Ã¶rnek sayÄ±sÄ±: 8,110\n",
      "\n",
      "âœ… Sampling tamamlandÄ±: 2,000 Ã¶rnek seÃ§ildi\n",
      "\n",
      "ğŸ“Š SeÃ§ilen Ã–rneklerin DaÄŸÄ±lÄ±mÄ±:\n",
      "   Real: 0 (0.0%)\n",
      "   Fake: 2,000 (100.0%)\n",
      "\n",
      "   Domain DaÄŸÄ±lÄ±mÄ±:\n",
      "      URBAN          :    976 ( 48.8%)\n",
      "      HUMAN_BODY     :    524 ( 26.2%)\n",
      "      CLOTHING       :    157 (  7.8%)\n",
      "      INDOOR         :    204 ( 10.2%)\n",
      "      BACKGROUND     :    139 (  7.0%)\n",
      "\n",
      "âœ… Dataset ve DataLoader hazÄ±r!\n",
      "   Dataset size: 2,000\n",
      "   Batch count: 32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ“‚ DATASET HAZIRLAMA - BÃ–LÃœM 5\n",
    "Stage 2 mask label'larÄ±nÄ± hazÄ±rlama ve final dataset oluÅŸturma\n",
    "\"\"\"\n",
    "\n",
    "# Stage 2 mappings yÃ¼kle (hierarchical config'den)\n",
    "print(\"\\nğŸ“¦ Stage 2 mappings yÃ¼kleniyor...\")\n",
    "\n",
    "if os.path.exists(HIERARCHICAL_CONFIG_PATH):\n",
    "    with open(HIERARCHICAL_CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        hierarchical_config = json.load(f)\n",
    "    \n",
    "    stage2_mappings = {}\n",
    "    for domain_name, domain_info in hierarchical_config.get('stage2', {}).items():\n",
    "        stage2_mappings[domain_name] = {\n",
    "            'mask_to_label': domain_info.get('mask_to_label', {}),\n",
    "            'label_to_mask': {int(k): v for k, v in domain_info.get('label_to_mask', {}).items()},\n",
    "            'num_masks': domain_info.get('num_masks', 0)\n",
    "        }\n",
    "    print(f\"   âœ… Config yÃ¼klendi: {len(stage2_mappings)} domain\")\n",
    "else:\n",
    "    # Fallback: DOMAIN_CATEGORIES'den oluÅŸtur\n",
    "    print(f\"   âš ï¸  Config bulunamadÄ±, DOMAIN_CATEGORIES'den oluÅŸturuluyor...\")\n",
    "    stage2_mappings = {}\n",
    "    for domain_name, masks in DOMAIN_CATEGORIES.items():\n",
    "        stage2_mappings[domain_name] = {\n",
    "            'mask_to_label': {mask: idx for idx, mask in enumerate(masks)},\n",
    "            'label_to_mask': {idx: mask for idx, mask in enumerate(masks)},\n",
    "            'num_masks': len(masks)\n",
    "        }\n",
    "\n",
    "def get_mask_label_for_domain(mask_name, domain_name, stage2_mappings):\n",
    "    \"\"\"Belirli bir domain iÃ§in mask label'Ä± dÃ¶ndÃ¼r\"\"\"\n",
    "    if domain_name not in stage2_mappings:\n",
    "        return None\n",
    "    mapping = stage2_mappings[domain_name]\n",
    "    if mask_name in mapping['mask_to_label']:\n",
    "        return mapping['mask_to_label'][mask_name]\n",
    "    return None\n",
    "\n",
    "# Mask label'larÄ±nÄ± hazÄ±rla\n",
    "print(\"\\nğŸ” Mask label'larÄ± hazÄ±rlanÄ±yor...\")\n",
    "all_mask_labels = []\n",
    "\n",
    "for idx, (mask_name, domain_label) in enumerate(zip(all_mask_names, all_domain_labels)):\n",
    "    domain_name = label_to_domain.get(domain_label, 'UNKNOWN')\n",
    "    mask_label = get_mask_label_for_domain(mask_name, domain_name, stage2_mappings)\n",
    "    \n",
    "    if mask_label is None:\n",
    "        # Fallback: 0\n",
    "        mask_label = 0\n",
    "    \n",
    "    all_mask_labels.append(mask_label)\n",
    "\n",
    "print(f\"   âœ… {len(all_mask_labels):,} mask label hazÄ±rlandÄ±\")\n",
    "\n",
    "# Sampling uygula\n",
    "print(f\"\\nğŸ“Š Sampling uygulanÄ±yor...\")\n",
    "print(f\"   Strateji: {SAMPLING_STRATEGY}\")\n",
    "print(f\"   Hedef oran: {TEST_DATA_RATIO*100:.0f}%\")\n",
    "\n",
    "if USE_RATIO_MODE:\n",
    "    target_count = int(len(all_image_paths) * TEST_DATA_RATIO)\n",
    "    target_count = max(MIN_SAMPLES_TOTAL if MIN_SAMPLES_TOTAL else 1000, target_count)\n",
    "else:\n",
    "    target_count = TARGET_SAMPLE_COUNT if TARGET_SAMPLE_COUNT else int(len(all_image_paths) * TEST_DATA_RATIO)\n",
    "\n",
    "print(f\"   Hedef Ã¶rnek sayÄ±sÄ±: {target_count:,}\")\n",
    "\n",
    "# Sampling stratejisine gÃ¶re Ã¶rnekle\n",
    "if SAMPLING_STRATEGY == 'stratified':\n",
    "    sampled_indices = sample_dataset_stratified(\n",
    "        all_image_paths, all_real_fake_labels, all_domain_labels, all_mask_names,\n",
    "        target_count,\n",
    "        real_ratio=REAL_DATA_RATIO,\n",
    "        fake_ratio=FAKE_DATA_RATIO,\n",
    "        domain_ratios=DOMAIN_RATIOS,\n",
    "        min_per_domain=MIN_SAMPLES_PER_DOMAIN,\n",
    "        max_per_domain=MAX_SAMPLES_PER_DOMAIN,\n",
    "        min_total=MIN_SAMPLES_TOTAL,\n",
    "        max_total=MAX_SAMPLES_TOTAL,\n",
    "        priority_domains=PRIORITY_DOMAINS\n",
    "    )\n",
    "elif SAMPLING_STRATEGY == 'balanced':\n",
    "    sampled_indices = sample_dataset_balanced(\n",
    "        all_image_paths, all_real_fake_labels, all_domain_labels, all_mask_names,\n",
    "        target_count,\n",
    "        min_per_domain=MIN_SAMPLES_PER_DOMAIN,\n",
    "        max_per_domain=MAX_SAMPLES_PER_DOMAIN\n",
    "    )\n",
    "else:  # random\n",
    "    sampled_indices = sample_dataset_random(all_image_paths, target_count)\n",
    "\n",
    "# Sampled data\n",
    "val_image_paths = [all_image_paths[i] for i in sampled_indices]\n",
    "val_real_fake_labels = [all_real_fake_labels[i] for i in sampled_indices]\n",
    "val_domain_labels = [all_domain_labels[i] for i in sampled_indices]\n",
    "val_mask_names = [all_mask_names[i] for i in sampled_indices]\n",
    "val_mask_labels = [all_mask_labels[i] for i in sampled_indices]\n",
    "\n",
    "print(f\"\\nâœ… Sampling tamamlandÄ±: {len(val_image_paths):,} Ã¶rnek seÃ§ildi\")\n",
    "\n",
    "# Ä°statistikler\n",
    "print(f\"\\nğŸ“Š SeÃ§ilen Ã–rneklerin DaÄŸÄ±lÄ±mÄ±:\")\n",
    "real_count = sum(1 for label in val_real_fake_labels if label == 0)\n",
    "fake_count = sum(1 for label in val_real_fake_labels if label == 1)\n",
    "print(f\"   Real: {real_count:,} ({real_count/len(val_real_fake_labels)*100:.1f}%)\")\n",
    "print(f\"   Fake: {fake_count:,} ({fake_count/len(val_real_fake_labels)*100:.1f}%)\")\n",
    "\n",
    "domain_dist = defaultdict(int)\n",
    "for dom_label in val_domain_labels:\n",
    "    dom_name = label_to_domain.get(dom_label, 'UNKNOWN')\n",
    "    domain_dist[dom_name] += 1\n",
    "\n",
    "print(f\"\\n   Domain DaÄŸÄ±lÄ±mÄ±:\")\n",
    "for domain in DOMAIN_NAMES:\n",
    "    count = domain_dist.get(domain, 0)\n",
    "    pct = (count / len(val_domain_labels)) * 100 if len(val_domain_labels) > 0 else 0\n",
    "    print(f\"      {domain:<15}: {count:>6,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Final dataset oluÅŸtur\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "hierarchical_val_dataset = HierarchicalValidationDataset(\n",
    "    image_paths=val_image_paths,\n",
    "    real_fake_labels=val_real_fake_labels,\n",
    "    domain_labels=val_domain_labels,\n",
    "    mask_names=val_mask_names,\n",
    "    mask_labels=val_mask_labels,\n",
    "    domain_names_list=DOMAIN_NAMES,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "hierarchical_val_loader = DataLoader(\n",
    "    hierarchical_val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Dataset ve DataLoader hazÄ±r!\")\n",
    "print(f\"   Dataset size: {len(hierarchical_val_dataset):,}\")\n",
    "print(f\"   Batch count: {len(hierarchical_val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Test Pipeline\n",
    "\n",
    "Stage 0 â†’ Stage 1 â†’ Stage 2 pipeline (Multi-Expert Routing ile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper fonksiyonlar tanÄ±mlandÄ±\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ”„ TEST PIPELINE - HELPER FUNCTIONS\n",
    "Hybrid scoring, entropy, dynamic alpha fonksiyonlarÄ±\n",
    "\"\"\"\n",
    "\n",
    "def calculate_entropy(probs):\n",
    "    \"\"\"Probability daÄŸÄ±lÄ±mÄ±nÄ±n entropy'sini hesapla\"\"\"\n",
    "    probs = np.clip(probs, 1e-10, 1.0)\n",
    "    entropy = -np.sum(probs * np.log(probs))\n",
    "    return entropy\n",
    "\n",
    "def calculate_hybrid_score(domain_prob, mask_probs, alpha=0.3):\n",
    "    \"\"\"\n",
    "    Hybrid scoring: Stage1 + Stage2 + Entropy kombinasyonu\n",
    "    \n",
    "    Args:\n",
    "        domain_prob: Stage1'den gelen bu domain'in olasÄ±lÄ±ÄŸÄ±\n",
    "        mask_probs: Stage2'den gelen mask olasÄ±lÄ±klarÄ± (numpy array)\n",
    "        alpha: Stage1 aÄŸÄ±rlÄ±ÄŸÄ± (0-1 arasÄ±)\n",
    "    \n",
    "    Returns:\n",
    "        hybrid_score, mask_confidence, mask_pred, confidence_factor\n",
    "    \"\"\"\n",
    "    mask_confidence = np.max(mask_probs)\n",
    "    mask_pred = np.argmax(mask_probs)\n",
    "    \n",
    "    entropy = calculate_entropy(mask_probs)\n",
    "    max_entropy = np.log(len(mask_probs))\n",
    "    confidence_factor = 1 - (entropy / max_entropy) if max_entropy > 0 else 1.0\n",
    "    \n",
    "    hybrid_score = alpha * domain_prob + (1 - alpha) * mask_confidence * confidence_factor\n",
    "    \n",
    "    return hybrid_score, mask_confidence, mask_pred, confidence_factor\n",
    "\n",
    "def get_dynamic_alpha(domain_name, stage2_models):\n",
    "    \"\"\"\n",
    "    Domain'in Stage1 ve Stage2 accuracy'sine gÃ¶re dinamik alpha belirle\n",
    "    \"\"\"\n",
    "    if domain_name in stage2_models:\n",
    "        stage2_acc = stage2_models[domain_name].get('val_acc', 0.7)\n",
    "    else:\n",
    "        stage2_acc = 0.7\n",
    "    \n",
    "    # Domain'e gÃ¶re strateji\n",
    "    if domain_name == 'INDOOR':\n",
    "        return 0.2  # Stage2'ye Ã§ok gÃ¼ven\n",
    "    elif domain_name == 'URBAN':\n",
    "        return 0.6  # Stage1'e gÃ¼ven\n",
    "    elif domain_name == 'CLOTHING':\n",
    "        return 0.1  # Stage2'ye Ã§ok gÃ¼ven\n",
    "    elif domain_name == 'HUMAN_BODY':\n",
    "        return 0.4  # Dengeli\n",
    "    elif domain_name == 'BACKGROUND':\n",
    "        return 0.7  # Stage1'e gÃ¼ven\n",
    "    else:\n",
    "        if stage2_acc > 0.85:\n",
    "            return 0.2\n",
    "        elif stage2_acc > 0.70:\n",
    "            return 0.4\n",
    "        else:\n",
    "            return 0.6\n",
    "\n",
    "print(\"âœ… Helper fonksiyonlar tanÄ±mlandÄ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ”„ COMBINED PIPELINE TEST BAÅLATILIYOR...\n",
      "======================================================================\n",
      "   Toplam Ã¶rnek: 2,000\n",
      "   Batch sayÄ±sÄ±: 32\n",
      "   Multi-Expert Routing: âœ… Aktif\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68e1ad465274328af9f8eedf1772d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ Combined Pipeline Testing:   0%|                                       | 0/32 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, true_rf_labels, true_domain_labels, true_mask_labels, sample_indices) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m     39\u001b[0m         tqdm(hierarchical_val_loader, \n\u001b[0;32m     40\u001b[0m              desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”„ Combined Pipeline Testing\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     41\u001b[0m              total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(hierarchical_val_loader),\n\u001b[0;32m     42\u001b[0m              ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     43\u001b[0m              leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m              unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m     ):\n\u001b[0;32m     46\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\emrec\\anaconda3\\Lib\\site-packages\\tqdm\\notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emrec\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emrec\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\emrec\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:759\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m--> 759\u001b[0m     data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\emrec\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:98\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     96\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m---> 98\u001b[0m         clone[i] \u001b[38;5;241m=\u001b[39m pin_memory(item, device)\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emrec\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:64\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mpin_memory(device)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ğŸ”„ TEST PIPELINE - ANA TEST DÃ–NGÃœSÃœ\n",
    "Stage 0 â†’ Stage 1 â†’ Stage 2 pipeline (Multi-Expert Routing ile)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”„ COMBINED PIPELINE TEST BAÅLATILIYOR...\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Toplam Ã¶rnek: {len(hierarchical_val_dataset):,}\")\n",
    "print(f\"   Batch sayÄ±sÄ±: {len(hierarchical_val_loader)}\")\n",
    "print(f\"   Multi-Expert Routing: {'âœ… Aktif' if USE_MULTI_EXPERT_ROUTING else 'âŒ KapalÄ±'}\")\n",
    "\n",
    "# SonuÃ§larÄ± saklamak iÃ§in\n",
    "all_results = []\n",
    "\n",
    "# Ä°statistikler\n",
    "stats = {\n",
    "    'stage0_real': 0,\n",
    "    'stage0_fake': 0,\n",
    "    'stage0_correct': 0,\n",
    "    'stage0_wrong': 0,\n",
    "    'normal_routing': 0,\n",
    "    'multi_expert_routing': 0,\n",
    "    'multi_expert_changed': 0,\n",
    "    'multi_expert_same': 0\n",
    "}\n",
    "\n",
    "# Stage 2 models dict (val_acc bilgisi iÃ§in)\n",
    "stage2_models_dict = {}\n",
    "for domain_name in DOMAIN_NAMES:\n",
    "    if domain_name in models_stage2:\n",
    "        stage2_models_dict[domain_name] = {'val_acc': 0.7}  # VarsayÄ±lan, gerÃ§ek deÄŸer config'den alÄ±nabilir\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, true_rf_labels, true_domain_labels, true_mask_labels, sample_indices) in enumerate(\n",
    "        tqdm(hierarchical_val_loader, \n",
    "             desc=\"ğŸ”„ Combined Pipeline Testing\",\n",
    "             total=len(hierarchical_val_loader),\n",
    "             ncols=100,\n",
    "             leave=True,\n",
    "             unit=\"batch\")\n",
    "    ):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # ============================================\n",
    "        # STAGE 0: Real/Fake Classification\n",
    "        # ============================================\n",
    "        \n",
    "        with autocast(enabled=USE_AMP):\n",
    "            rf_outputs = model_real_fake(images)\n",
    "        \n",
    "        rf_probs = torch.softmax(rf_outputs, dim=1)\n",
    "        rf_confidences, pred_rf = torch.max(rf_probs, 1)\n",
    "        \n",
    "        # CPU'ya taÅŸÄ±\n",
    "        rf_probs_np = rf_probs.cpu().numpy()\n",
    "        pred_rf_np = pred_rf.cpu().numpy()\n",
    "        rf_confidences_np = rf_confidences.cpu().numpy()\n",
    "        true_rf_np = true_rf_labels.numpy()\n",
    "        true_domain_np = true_domain_labels.numpy()\n",
    "        true_mask_np = true_mask_labels.numpy()\n",
    "        sample_indices_np = sample_indices.numpy()\n",
    "        \n",
    "        # ============================================\n",
    "        # STAGE 1: Domain Classification (sadece fake olanlar iÃ§in)\n",
    "        # ============================================\n",
    "        \n",
    "        with autocast(enabled=USE_AMP):\n",
    "            domain_outputs = model_domain(images)\n",
    "        \n",
    "        domain_probs = torch.softmax(domain_outputs, dim=1)\n",
    "        domain_confidences, pred_domains = torch.max(domain_probs, 1)\n",
    "        \n",
    "        domain_probs_np = domain_probs.cpu().numpy()\n",
    "        pred_domains_np = pred_domains.cpu().numpy()\n",
    "        domain_confidences_np = domain_confidences.cpu().numpy()\n",
    "        \n",
    "        # ============================================\n",
    "        # Her Ã¶rnek iÃ§in iÅŸle\n",
    "        # ============================================\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            img = images[i:i+1]\n",
    "            \n",
    "            # True labels\n",
    "            true_rf_label = true_rf_np[i]\n",
    "            true_domain_label = true_domain_np[i]\n",
    "            true_mask_label = true_mask_np[i]\n",
    "            sample_idx = sample_indices_np[i]\n",
    "            \n",
    "            # Stage 0 predictions\n",
    "            pred_rf_label = pred_rf_np[i]\n",
    "            rf_confidence = rf_confidences_np[i]\n",
    "            \n",
    "            # Stage 0 doÄŸruluÄŸu\n",
    "            stage0_correct = (pred_rf_label == true_rf_label)\n",
    "            if stage0_correct:\n",
    "                stats['stage0_correct'] += 1\n",
    "            else:\n",
    "                stats['stage0_wrong'] += 1\n",
    "            \n",
    "            if pred_rf_label == 0:  # real\n",
    "                stats['stage0_real'] += 1\n",
    "            else:  # fake\n",
    "                stats['stage0_fake'] += 1\n",
    "            \n",
    "            # Real/Fake names\n",
    "            true_rf_name = 'real' if true_rf_label == 0 else 'fake'\n",
    "            pred_rf_name = 'real' if pred_rf_label == 0 else 'fake'\n",
    "            \n",
    "            # Domain names (Stage 1 tahminleri)\n",
    "            pred_domain_label = pred_domains_np[i]\n",
    "            domain_conf = domain_confidences_np[i]\n",
    "            all_domain_probs = domain_probs_np[i]\n",
    "            \n",
    "            true_domain_name = label_to_domain.get(true_domain_label) or label_to_domain.get(str(true_domain_label))\n",
    "            original_pred_domain_name = label_to_domain.get(pred_domain_label) or label_to_domain.get(str(pred_domain_label))\n",
    "            \n",
    "            # True mask name\n",
    "            true_mask_name = hierarchical_val_dataset.mask_names[sample_idx]\n",
    "            \n",
    "            # EÄŸer REAL tahmin edildiyse, Stage 1 ve Stage 2'ye geÃ§me\n",
    "            if pred_rf_label == 0:  # real\n",
    "                # Real gÃ¶rÃ¼ntÃ¼ler iÃ§in Stage 1 ve Stage 2 yok\n",
    "                result = {\n",
    "                    'sample_idx': sample_idx,\n",
    "                    'image_path': hierarchical_val_dataset.image_paths[sample_idx],\n",
    "                    'true_rf_label': true_rf_label,\n",
    "                    'pred_rf_label': pred_rf_label,\n",
    "                    'true_rf_name': true_rf_name,\n",
    "                    'pred_rf_name': pred_rf_name,\n",
    "                    'rf_confidence': rf_confidence,\n",
    "                    'stage0_correct': stage0_correct,\n",
    "                    'stage1_domain': None,  # Real iÃ§in yok\n",
    "                    'stage2_mask': None,    # Real iÃ§in yok\n",
    "                    'pipeline_correct': stage0_correct  # Sadece Stage 0 doÄŸruysa pipeline doÄŸru\n",
    "                }\n",
    "                all_results.append(result)\n",
    "                continue\n",
    "            \n",
    "            # FAKE ise Stage 1 ve Stage 2 devam eder\n",
    "            \n",
    "            # ============================================\n",
    "            # MULTI-EXPERT ROUTING (Stage 1 iÃ§in)\n",
    "            # ============================================\n",
    "            \n",
    "            use_multi_expert = False\n",
    "            final_pred_domain_name = original_pred_domain_name\n",
    "            routing_method = \"normal\"\n",
    "            \n",
    "            if USE_MULTI_EXPERT_ROUTING:\n",
    "                if domain_conf < ROUTING_THRESHOLD and original_pred_domain_name in CONFUSING_DOMAINS:\n",
    "                    use_multi_expert = True\n",
    "            \n",
    "            if use_multi_expert:\n",
    "                # Multi-expert: TÃ¼m karÄ±ÅŸan domain'lere sor\n",
    "                candidates = []\n",
    "                \n",
    "                for domain_name in CONFUSING_DOMAINS:\n",
    "                    if domain_name not in models_stage2:\n",
    "                        continue\n",
    "                    \n",
    "                    s2_model = models_stage2[domain_name]\n",
    "                    with autocast(enabled=USE_AMP):\n",
    "                        s2_outputs = s2_model(img)\n",
    "                    s2_probs = torch.softmax(s2_outputs, dim=1).cpu().numpy()[0]\n",
    "                    \n",
    "                    domain_idx = domain_to_label[domain_name]\n",
    "                    domain_prob = all_domain_probs[domain_idx]\n",
    "                    \n",
    "                    # Hybrid score\n",
    "                    if USE_DYNAMIC_ALPHA:\n",
    "                        dynamic_alpha = get_dynamic_alpha(domain_name, stage2_models_dict)\n",
    "                    else:\n",
    "                        dynamic_alpha = HYBRID_ALPHA\n",
    "                    \n",
    "                    hybrid_score, mask_conf, mask_pred, conf_factor = calculate_hybrid_score(\n",
    "                        domain_prob, s2_probs, alpha=dynamic_alpha\n",
    "                    )\n",
    "                    \n",
    "                    l2m = label_to_mask[domain_name]\n",
    "                    mask_name = l2m.get(int(mask_pred)) or l2m.get(str(mask_pred), \"UNKNOWN\")\n",
    "                    \n",
    "                    candidates.append({\n",
    "                        'domain': domain_name,\n",
    "                        'domain_prob': domain_prob,\n",
    "                        'mask_pred': int(mask_pred),\n",
    "                        'mask_name': mask_name,\n",
    "                        'mask_confidence': mask_conf,\n",
    "                        'hybrid_score': hybrid_score\n",
    "                    })\n",
    "                \n",
    "                if candidates:\n",
    "                    best = max(candidates, key=lambda x: x['hybrid_score'])\n",
    "                    final_pred_domain_name = best['domain']\n",
    "                    pred_mask_label = best['mask_pred']\n",
    "                    pred_mask_name = best['mask_name']\n",
    "                    mask_confidence = best['mask_confidence']\n",
    "                    routing_method = \"multi_expert\"\n",
    "                    \n",
    "                    stats['multi_expert_routing'] += 1\n",
    "                    if final_pred_domain_name != original_pred_domain_name:\n",
    "                        stats['multi_expert_changed'] += 1\n",
    "                    else:\n",
    "                        stats['multi_expert_same'] += 1\n",
    "                else:\n",
    "                    use_multi_expert = False\n",
    "            \n",
    "            if not use_multi_expert:\n",
    "                # Normal routing: Sadece tahmin edilen domain'e sor\n",
    "                pred_mask_label = -1\n",
    "                pred_mask_name = \"UNKNOWN\"\n",
    "                mask_confidence = 0.0\n",
    "                \n",
    "                if original_pred_domain_name in models_stage2:\n",
    "                    s2_model = models_stage2[original_pred_domain_name]\n",
    "                    with autocast(enabled=USE_AMP):\n",
    "                        s2_outputs = s2_model(img)\n",
    "                    s2_probs = torch.softmax(s2_outputs, dim=1)\n",
    "                    mask_conf, mask_pred = torch.max(s2_probs, 1)\n",
    "                    \n",
    "                    pred_mask_label = mask_pred.item()\n",
    "                    mask_confidence = mask_conf.item()\n",
    "                    \n",
    "                    l2m = label_to_mask[original_pred_domain_name]\n",
    "                    pred_mask_name = l2m.get(pred_mask_label) or l2m.get(str(pred_mask_label), \"UNKNOWN\")\n",
    "                \n",
    "                final_pred_domain_name = original_pred_domain_name\n",
    "                routing_method = \"normal\"\n",
    "                stats['normal_routing'] += 1\n",
    "            \n",
    "            # Final domain label - GÃ¼venli mapping ile fallback mekanizmasÄ±\n",
    "            if final_pred_domain_name and final_pred_domain_name in domain_to_label:\n",
    "                final_pred_domain_label = domain_to_label[final_pred_domain_name]\n",
    "            elif original_pred_domain_name and original_pred_domain_name in domain_to_label:\n",
    "                # Fallback: original prediction domain name'ini kullan\n",
    "                final_pred_domain_label = domain_to_label[original_pred_domain_name]\n",
    "            else:\n",
    "                # Son fallback: original prediction label'Ä±nÄ± kullan\n",
    "                final_pred_domain_label = int(pred_domain_label)\n",
    "            \n",
    "            # DoÄŸruluk kontrolleri - label'larÄ± integer'a Ã§evirerek karÅŸÄ±laÅŸtÄ±r\n",
    "            # true_domain_label zaten integer olmalÄ± ama gÃ¼venli olmasÄ± iÃ§in int() kullanÄ±yoruz\n",
    "            domain_correct = (int(final_pred_domain_label) == int(true_domain_label))\n",
    "            \n",
    "            if domain_correct:\n",
    "                expected_mask_label = get_mask_label_for_domain(true_mask_name, true_domain_name, stage2_mappings)\n",
    "                mask_correct = (pred_mask_label == expected_mask_label) if expected_mask_label is not None else False\n",
    "            else:\n",
    "                mask_correct = False\n",
    "            \n",
    "            # SonuÃ§\n",
    "            result = {\n",
    "                'sample_idx': sample_idx,\n",
    "                'image_path': hierarchical_val_dataset.image_paths[sample_idx],\n",
    "                'true_rf_label': true_rf_label,\n",
    "                'pred_rf_label': pred_rf_label,\n",
    "                'true_rf_name': true_rf_name,\n",
    "                'pred_rf_name': pred_rf_name,\n",
    "                'rf_confidence': rf_confidence,\n",
    "                'stage0_correct': stage0_correct,\n",
    "                'true_domain_label': true_domain_label,\n",
    "                'original_pred_domain_label': pred_domain_label,\n",
    "                'final_pred_domain_label': final_pred_domain_label,\n",
    "                'true_domain_name': true_domain_name,\n",
    "                'original_pred_domain_name': original_pred_domain_name,\n",
    "                'final_pred_domain_name': final_pred_domain_name,\n",
    "                'domain_confidence': domain_conf,\n",
    "                'domain_correct': domain_correct,\n",
    "                'true_mask_label': true_mask_label,\n",
    "                'pred_mask_label': pred_mask_label,\n",
    "                'true_mask_name': true_mask_name,\n",
    "                'pred_mask_name': pred_mask_name,\n",
    "                'mask_confidence': mask_confidence,\n",
    "                'mask_correct': mask_correct,\n",
    "                'pipeline_correct': stage0_correct and domain_correct and mask_correct,\n",
    "                'routing_method': routing_method\n",
    "            }\n",
    "            \n",
    "            all_results.append(result)\n",
    "\n",
    "# DataFrame oluÅŸtur\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\nâœ… Test tamamlandÄ±: {len(df_results):,} Ã¶rnek\")\n",
    "\n",
    "# Ä°statistikler\n",
    "print(f\"\\nğŸ“Š Ä°STATÄ°STÄ°KLER:\")\n",
    "print(f\"   Stage 0 Real: {stats['stage0_real']:,}\")\n",
    "print(f\"   Stage 0 Fake: {stats['stage0_fake']:,}\")\n",
    "print(f\"   Stage 0 Correct: {stats['stage0_correct']:,}\")\n",
    "print(f\"   Stage 0 Wrong: {stats['stage0_wrong']:,}\")\n",
    "if USE_MULTI_EXPERT_ROUTING:\n",
    "    print(f\"   Multi-Expert Routing: {stats['multi_expert_routing']:,}\")\n",
    "    print(f\"      â†’ Domain deÄŸiÅŸti: {stats['multi_expert_changed']:,}\")\n",
    "    print(f\"      â†’ Domain aynÄ±: {stats['multi_expert_same']:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š TEMEL METRÄ°KLER\n",
    "TÃ¼m stage'ler iÃ§in accuracy hesaplama\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š TEMEL METRÄ°KLER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_samples = len(df_results)\n",
    "\n",
    "# Stage 0 metrikleri\n",
    "stage0_correct = df_results['stage0_correct'].sum()\n",
    "stage0_accuracy = stage0_correct / total_samples\n",
    "\n",
    "print(f\"\\nğŸ¯ STAGE 0 - Real/Fake Classification:\")\n",
    "print(f\"   Accuracy: {stage0_accuracy*100:.2f}%\")\n",
    "print(f\"   Correct: {stage0_correct:,} / {total_samples:,}\")\n",
    "\n",
    "# Stage 1 metrikleri (sadece fake olanlar iÃ§in)\n",
    "df_fake = df_results[df_results['pred_rf_name'] == 'fake'].copy()\n",
    "if len(df_fake) > 0:\n",
    "    stage1_correct = df_fake['domain_correct'].sum()\n",
    "    stage1_accuracy = stage1_correct / len(df_fake)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ STAGE 1 - Domain Classification (Fake gÃ¶rÃ¼ntÃ¼ler iÃ§in):\")\n",
    "    print(f\"   Accuracy: {stage1_accuracy*100:.2f}%\")\n",
    "    print(f\"   Correct: {stage1_correct:,} / {len(df_fake):,}\")\n",
    "    \n",
    "    # Stage 2 metrikleri (sadece domain doÄŸru olanlar iÃ§in)\n",
    "    df_domain_correct = df_fake[df_fake['domain_correct'] == True].copy()\n",
    "    if len(df_domain_correct) > 0:\n",
    "        stage2_correct = df_domain_correct['mask_correct'].sum()\n",
    "        stage2_accuracy = stage2_correct / len(df_domain_correct)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ STAGE 2 - Mask Detection (Domain doÄŸru olanlar iÃ§in):\")\n",
    "        print(f\"   Accuracy: {stage2_accuracy*100:.2f}%\")\n",
    "        print(f\"   Correct: {stage2_correct:,} / {len(df_domain_correct):,}\")\n",
    "    else:\n",
    "        stage2_accuracy = 0.0\n",
    "        print(f\"\\nğŸ¯ STAGE 2 - Mask Detection:\")\n",
    "        print(f\"   âš ï¸  Domain doÄŸru Ã¶rnek yok, Stage 2 test edilemedi\")\n",
    "else:\n",
    "    stage1_accuracy = 0.0\n",
    "    stage2_accuracy = 0.0\n",
    "    print(f\"\\nğŸ¯ STAGE 1 & 2:\")\n",
    "    print(f\"   âš ï¸  Fake gÃ¶rÃ¼ntÃ¼ yok, Stage 1 ve Stage 2 test edilemedi\")\n",
    "\n",
    "# Full pipeline accuracy\n",
    "pipeline_correct = df_results['pipeline_correct'].sum()\n",
    "pipeline_accuracy = pipeline_correct / total_samples\n",
    "\n",
    "print(f\"\\nğŸ”— FULL PIPELINE (Stage 0 + Stage 1 + Stage 2):\")\n",
    "print(f\"   Accuracy: {pipeline_accuracy*100:.2f}%\")\n",
    "print(f\"   Correct: {pipeline_correct:,} / {total_samples:,}\")\n",
    "\n",
    "# Ã–zet dict\n",
    "test_summary = {\n",
    "    'total_samples': total_samples,\n",
    "    'stage0_accuracy': stage0_accuracy,\n",
    "    'stage0_correct': int(stage0_correct),\n",
    "    'stage1_accuracy': stage1_accuracy if len(df_fake) > 0 else 0.0,\n",
    "    'stage1_correct': int(stage1_correct) if len(df_fake) > 0 else 0,\n",
    "    'stage2_accuracy': stage2_accuracy if len(df_fake) > 0 and len(df_domain_correct) > 0 else 0.0,\n",
    "    'stage2_correct': int(stage2_correct) if len(df_fake) > 0 and len(df_domain_correct) > 0 else 0,\n",
    "    'pipeline_accuracy': pipeline_accuracy,\n",
    "    'pipeline_correct': int(pipeline_correct),\n",
    "    'multi_expert_stats': stats\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… Metrikler hesaplandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Stage 0 GÃ¶rselleÅŸtirmeleri\n",
    "\n",
    "Real/Fake analizleri: Confusion Matrix, Confidence DaÄŸÄ±lÄ±mÄ±, Domain bazÄ±nda doÄŸruluk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š STAGE 0 GÃ–RSELLEÅTÄ°RMELERÄ°\n",
    "Real/Fake Confusion Matrix, Confidence DaÄŸÄ±lÄ±mÄ±, Domain bazÄ±nda analiz\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š STAGE 0 GÃ–RSELLEÅTÄ°RMELERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Style ayarlarÄ±\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "\n",
    "COLORS = {\n",
    "    'correct': '#2ecc71',\n",
    "    'wrong': '#e74c3c',\n",
    "    'real': '#3498db',\n",
    "    'fake': '#f39c12',\n",
    "    'neutral': '#95a5a6'\n",
    "}\n",
    "\n",
    "# =============================================\n",
    "# 1. Real/Fake Confusion Matrix\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Real/Fake Confusion Matrix oluÅŸturuluyor...\")\n",
    "\n",
    "true_rf = df_results['true_rf_label'].values\n",
    "pred_rf = df_results['pred_rf_label'].values\n",
    "\n",
    "cm_rf = confusion_matrix(true_rf, pred_rf, labels=[0, 1])\n",
    "cm_rf_normalized = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Sol: Raw counts\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'},\n",
    "            annot_kws={'size': 14, 'weight': 'bold'})\n",
    "axes[0].set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('True', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Real/Fake Classification\\n(Absolute Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# SaÄŸ: Percentages\n",
    "sns.heatmap(cm_rf_normalized, annot=True, fmt='.1f', cmap='Greens',\n",
    "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Percentage (%)'},\n",
    "            annot_kws={'size': 14, 'weight': 'bold'}, vmin=0, vmax=100)\n",
    "axes[1].set_xlabel('Predicted', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('True', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('Real/Fake Classification\\n(Row-Normalized %)', fontsize=14, fontweight='bold')\n",
    "\n",
    "overall_acc = np.trace(cm_rf) / cm_rf.sum()\n",
    "fig.suptitle(f'Stage 0: Real/Fake Classification Accuracy: {overall_acc*100:.2f}%', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_STAGE0, 'stage0_real_fake_confusion_matrix.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Confusion matrix kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 2. Real/Fake Confidence DaÄŸÄ±lÄ±mÄ±\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Confidence daÄŸÄ±lÄ±mÄ± oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Sol: Histogram\n",
    "correct_confs = df_results[df_results['stage0_correct'] == True]['rf_confidence'].values\n",
    "wrong_confs = df_results[df_results['stage0_correct'] == False]['rf_confidence'].values\n",
    "\n",
    "axes[0].hist(correct_confs, bins=50, alpha=0.7, color=COLORS['correct'], \n",
    "            label=f'Correct ({len(correct_confs):,})', density=True, edgecolor='white')\n",
    "axes[0].hist(wrong_confs, bins=50, alpha=0.7, color=COLORS['wrong'], \n",
    "            label=f'Wrong ({len(wrong_confs):,})', density=True, edgecolor='white')\n",
    "axes[0].axvline(x=correct_confs.mean(), color=COLORS['correct'], linestyle=':', linewidth=2, label='Mean (Correct)')\n",
    "if len(wrong_confs) > 0:\n",
    "    axes[0].axvline(x=wrong_confs.mean(), color=COLORS['wrong'], linestyle=':', linewidth=2, label='Mean (Wrong)')\n",
    "axes[0].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Stage 0: Confidence Distribution\\n(Correct vs Wrong)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# SaÄŸ: Real vs Fake confidence\n",
    "real_confs = df_results[df_results['true_rf_name'] == 'real']['rf_confidence'].values\n",
    "fake_confs = df_results[df_results['true_rf_name'] == 'fake']['rf_confidence'].values\n",
    "\n",
    "if len(real_confs) > 0:\n",
    "    axes[1].hist(real_confs, bins=50, alpha=0.7, color=COLORS['real'], \n",
    "                label=f'Real ({len(real_confs):,})', density=True, edgecolor='white')\n",
    "if len(fake_confs) > 0:\n",
    "    axes[1].hist(fake_confs, bins=50, alpha=0.7, color=COLORS['fake'], \n",
    "                label=f'Fake ({len(fake_confs):,})', density=True, edgecolor='white')\n",
    "axes[1].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Density', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Stage 0: Confidence Distribution\\n(Real vs Fake)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_STAGE0, 'stage0_confidence_distribution.png'), \n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Confidence daÄŸÄ±lÄ±mÄ± kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 3. Domain bazÄ±nda Real/Fake doÄŸruluÄŸu\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Domain bazÄ±nda Real/Fake analizi oluÅŸturuluyor...\")\n",
    "\n",
    "# Fake gÃ¶rÃ¼ntÃ¼lerin domain daÄŸÄ±lÄ±mÄ±\n",
    "df_fake_only = df_results[df_results['true_rf_name'] == 'fake'].copy()\n",
    "if len(df_fake_only) > 0:\n",
    "    domain_rf_accuracy = []\n",
    "    for domain in DOMAIN_NAMES:\n",
    "        df_dom = df_fake_only[df_fake_only['true_domain_name'] == domain]\n",
    "        if len(df_dom) > 0:\n",
    "            acc = df_dom['stage0_correct'].mean()\n",
    "            domain_rf_accuracy.append({\n",
    "                'domain': domain,\n",
    "                'accuracy': acc,\n",
    "                'count': len(df_dom)\n",
    "            })\n",
    "    \n",
    "    if domain_rf_accuracy:\n",
    "        df_domain_rf = pd.DataFrame(domain_rf_accuracy)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        x = np.arange(len(df_domain_rf))\n",
    "        bars = ax.bar(x, df_domain_rf['accuracy'], color=COLORS['fake'], \n",
    "                     edgecolor='white', linewidth=2, alpha=0.85)\n",
    "        \n",
    "        # Bar labels\n",
    "        for bar, acc, count in zip(bars, df_domain_rf['accuracy'], df_domain_rf['count']):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{acc*100:.1f}%\\\\n({count:,})',\n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Domain', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Stage 0 Accuracy', fontsize=13, fontweight='bold')\n",
    "        ax.set_title('Stage 0: Real/Fake Accuracy by Domain\\\\n(Fake gÃ¶rÃ¼ntÃ¼ler iÃ§in)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(df_domain_rf['domain'], fontsize=11, fontweight='bold')\n",
    "        ax.set_ylim([0, 1.1])\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIZ_STAGE0, 'stage0_domain_accuracy.png'), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"   âœ… Domain bazÄ±nda analiz kaydedildi\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Fake gÃ¶rÃ¼ntÃ¼ bulunamadÄ±, analiz atlandÄ±\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Fake gÃ¶rÃ¼ntÃ¼ bulunamadÄ±, analiz atlandÄ±\")\n",
    "\n",
    "print(f\"\\nâœ… Stage 0 gÃ¶rselleÅŸtirmeleri tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Stage 1 & 2 GÃ¶rselleÅŸtirmeleri\n",
    "\n",
    "Confusion matrices, error flow, confidence distributions, per-domain accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Multi-Expert Routing Analizi\n",
    "\n",
    "Multi-expert routing performansÄ± ve confusion matrix analizi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”„ MULTI-EXPERT ROUTING ANALÄ°ZÄ°\n",
    "Multi-expert routing confusion matrix ve performans analizi\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”„ MULTI-EXPERT ROUTING ANALÄ°ZÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not USE_MULTI_EXPERT_ROUTING:\n",
    "    print(\"\\nâš ï¸  Multi-Expert Routing kapalÄ±, analiz atlanÄ±yor\")\n",
    "else:\n",
    "    df_fake = df_results[df_results['pred_rf_name'] == 'fake'].copy()\n",
    "    \n",
    "    if len(df_fake) == 0:\n",
    "        print(\"\\nâš ï¸  Fake gÃ¶rÃ¼ntÃ¼ bulunamadÄ±, routing analizi atlanÄ±yor\")\n",
    "    else:\n",
    "        # =============================================\n",
    "        # 1. Multi-Expert Routing Confusion Matrix\n",
    "        # =============================================\n",
    "        \n",
    "        print(\"\\n1ï¸âƒ£ Multi-Expert Routing Confusion Matrix oluÅŸturuluyor...\")\n",
    "        \n",
    "        # Normal routing vs Multi-expert routing iÃ§in ayrÄ± confusion matrix\n",
    "        df_normal = df_fake[df_fake['routing_method'] == 'normal'].copy()\n",
    "        df_multi = df_fake[df_fake['routing_method'] == 'multi_expert'].copy()\n",
    "        \n",
    "        if len(df_multi) > 0:\n",
    "            # Multi-expert routing iÃ§in confusion matrix\n",
    "            true_domains_multi = df_multi['true_domain_label'].values\n",
    "            pred_domains_multi = df_multi['final_pred_domain_label'].values\n",
    "            \n",
    "            cm_multi = confusion_matrix(true_domains_multi, pred_domains_multi, labels=range(NUM_DOMAINS))\n",
    "            cm_multi_normalized = cm_multi.astype('float') / (cm_multi.sum(axis=1)[:, np.newaxis] + 1e-10) * 100\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "            \n",
    "            # Sol: Raw counts\n",
    "            matrix_size = len(DOMAIN_NAMES)\n",
    "            font_size = max(12, 16 - matrix_size // 3)\n",
    "            sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Purples',\n",
    "                        xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "                        ax=axes[0], cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
    "                        annot_kws={'size': font_size, 'weight': 'bold', 'color': 'white'},\n",
    "                        linewidths=2, linecolor='white')\n",
    "            axes[0].set_xlabel('Predicted Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "            axes[0].set_ylabel('True Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "            axes[0].set_title('Multi-Expert Routing Confusion Matrix\\n(Absolute Counts)', \n",
    "                             fontsize=15, fontweight='bold', pad=15)\n",
    "            axes[0].tick_params(axis='x', rotation=45, labelsize=12)\n",
    "            axes[0].tick_params(axis='y', rotation=0, labelsize=12)\n",
    "            \n",
    "            # SaÄŸ: Percentages\n",
    "            sns.heatmap(cm_multi_normalized, annot=True, fmt='.1f', cmap='Oranges',\n",
    "                        xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "                        ax=axes[1], cbar_kws={'label': 'Percentage (%)', 'shrink': 0.8},\n",
    "                        annot_kws={'size': font_size, 'weight': 'bold', 'color': 'white'},\n",
    "                        linewidths=2, linecolor='white', vmin=0, vmax=100)\n",
    "            axes[1].set_xlabel('Predicted Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "            axes[1].set_ylabel('True Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "            axes[1].set_title('Multi-Expert Routing Confusion Matrix\\n(Row-Normalized %)', \n",
    "                             fontsize=15, fontweight='bold', pad=15)\n",
    "            axes[1].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "            axes[1].tick_params(axis='y', rotation=0, labelsize=11)\n",
    "            \n",
    "            multi_acc = np.trace(cm_multi) / cm_multi.sum() if cm_multi.sum() > 0 else 0\n",
    "            fig.suptitle(f'Multi-Expert Routing Accuracy: {multi_acc*100:.2f}% ({len(df_multi):,} samples)', \n",
    "                        fontsize=16, fontweight='bold', y=1.02)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(VIZ_ROUTING, 'multi_expert_routing_confusion_matrix.png'), \n",
    "                       dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"   âœ… Multi-expert routing confusion matrix kaydedildi\")\n",
    "            print(f\"      Accuracy: {multi_acc*100:.2f}%\")\n",
    "            print(f\"      Samples: {len(df_multi):,}\")\n",
    "        \n",
    "        # =============================================\n",
    "        # 2. Routing Method Comparison\n",
    "        # =============================================\n",
    "        \n",
    "        print(\"\\n2ï¸âƒ£ Routing method karÅŸÄ±laÅŸtÄ±rmasÄ± oluÅŸturuluyor...\")\n",
    "        \n",
    "        if len(df_normal) > 0 and len(df_multi) > 0:\n",
    "            normal_acc = df_normal['domain_correct'].mean()\n",
    "            multi_acc = df_multi['domain_correct'].mean()\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            methods = ['Normal Routing', 'Multi-Expert Routing']\n",
    "            accuracies = [normal_acc, multi_acc]\n",
    "            counts = [len(df_normal), len(df_multi)]\n",
    "            colors = ['#3498db', '#9b59b6']\n",
    "            \n",
    "            bars = ax.bar(methods, accuracies, color=colors, edgecolor='white', linewidth=2, alpha=0.85)\n",
    "            \n",
    "            for bar, acc, count in zip(bars, accuracies, counts):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{acc*100:.2f}%\\\\n({count:,} samples)',\n",
    "                       ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            ax.set_ylabel('Domain Classification Accuracy', fontsize=13, fontweight='bold')\n",
    "            ax.set_title('Routing Method Comparison\\\\n(Domain Classification Accuracy)', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            ax.set_ylim([0, 1.1])\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(VIZ_ROUTING, 'routing_method_comparison.png'), \n",
    "                       dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"   âœ… Routing method karÅŸÄ±laÅŸtÄ±rmasÄ± kaydedildi\")\n",
    "            print(f\"      Normal Routing: {normal_acc*100:.2f}% ({len(df_normal):,} samples)\")\n",
    "            print(f\"      Multi-Expert Routing: {multi_acc*100:.2f}% ({len(df_multi):,} samples)\")\n",
    "        \n",
    "        print(f\"\\nâœ… Multi-Expert Routing analizi tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Threshold Pool Testi\n",
    "\n",
    "FarklÄ± ROUTING_THRESHOLD deÄŸerleri iÃ§in performans analizi ve gÃ¶rselleÅŸtirmeler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ¯ THRESHOLD POOL TESTÄ°\n",
    "FarklÄ± ROUTING_THRESHOLD deÄŸerleri iÃ§in performans analizi\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ THRESHOLD POOL TESTÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not ENABLE_THRESHOLD_POOL_TEST:\n",
    "    print(\"\\nâš ï¸  Threshold Pool Testi kapalÄ±, analiz atlanÄ±yor\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š Test edilecek threshold deÄŸerleri: {THRESHOLD_POOL_VALUES}\")\n",
    "    print(f\"ğŸ“Š Threshold pool testi iÃ§in veri oranÄ±: {THRESHOLD_POOL_TEST_RATIO*100:.0f}%\")\n",
    "    \n",
    "    # Threshold pool testi iÃ§in veri hazÄ±rlama\n",
    "    # Mevcut df_results'tan fake gÃ¶rÃ¼ntÃ¼leri al ve threshold pool testi iÃ§in kullan\n",
    "    df_fake_all = df_results[df_results['pred_rf_name'] == 'fake'].copy()\n",
    "    \n",
    "    if len(df_fake_all) == 0:\n",
    "        print(\"\\nâš ï¸  Fake gÃ¶rÃ¼ntÃ¼ bulunamadÄ±, threshold pool testi atlanÄ±yor\")\n",
    "    else:\n",
    "        # Threshold pool testi iÃ§in Ã¶rnekleme (hÄ±zlÄ± test iÃ§in)\n",
    "        n_samples_pool = max(100, int(len(df_fake_all) * THRESHOLD_POOL_TEST_RATIO))\n",
    "        if len(df_fake_all) > n_samples_pool:\n",
    "            df_fake_pool = df_fake_all.sample(n=n_samples_pool, random_state=42).copy()\n",
    "        else:\n",
    "            df_fake_pool = df_fake_all.copy()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Threshold pool testi iÃ§in {len(df_fake_pool):,} Ã¶rnek kullanÄ±lacak\")\n",
    "        \n",
    "        # Her threshold deÄŸeri iÃ§in analiz\n",
    "        threshold_results = []\n",
    "        \n",
    "        for threshold_val in THRESHOLD_POOL_VALUES:\n",
    "            print(f\"\\nğŸ” Threshold = {threshold_val:.2f} test ediliyor...\")\n",
    "            \n",
    "            # Bu threshold deÄŸerine gÃ¶re multi-expert routing kullanÄ±mÄ±nÄ± simÃ¼le et\n",
    "            # Mevcut domain_confidence deÄŸerlerini kullanarak hangi Ã¶rneklerin multi-expert kullanacaÄŸÄ±nÄ± belirle\n",
    "            df_pool_test = df_fake_pool.copy()\n",
    "            \n",
    "            # Multi-expert routing kullanÄ±mÄ± simÃ¼lasyonu\n",
    "            # domain_confidence < threshold_val ve domain CONFUSING_DOMAINS iÃ§indeyse multi-expert kullan\n",
    "            use_multi_expert = (\n",
    "                (df_pool_test['domain_confidence'] < threshold_val) & \n",
    "                (df_pool_test['original_pred_domain_name'].isin(CONFUSING_DOMAINS))\n",
    "            )\n",
    "            \n",
    "            df_normal_test = df_pool_test[~use_multi_expert].copy()\n",
    "            df_multi_test = df_pool_test[use_multi_expert].copy()\n",
    "            \n",
    "            # Normal routing accuracy\n",
    "            normal_acc = df_normal_test['domain_correct'].mean() if len(df_normal_test) > 0 else 0.0\n",
    "            \n",
    "            # Multi-expert routing accuracy (mevcut sonuÃ§lardan)\n",
    "            # Not: GerÃ§ek multi-expert routing sonuÃ§larÄ±nÄ± kullan\n",
    "            if len(df_multi_test) > 0:\n",
    "                # Mevcut df_results'tan bu threshold'a yakÄ±n Ã¶rnekleri bul\n",
    "                # BasitleÅŸtirilmiÅŸ: Mevcut routing_method='multi_expert' olanlarÄ± kullan\n",
    "                df_multi_actual = df_fake_all[\n",
    "                    (df_fake_all['routing_method'] == 'multi_expert') &\n",
    "                    (df_fake_all['domain_confidence'] < threshold_val + 0.05) &\n",
    "                    (df_fake_all['domain_confidence'] > threshold_val - 0.05)\n",
    "                ].copy()\n",
    "                \n",
    "                if len(df_multi_actual) > 0:\n",
    "                    multi_acc = df_multi_actual['domain_correct'].mean()\n",
    "                else:\n",
    "                    # Fallback: TÃ¼m multi-expert Ã¶rneklerini kullan\n",
    "                    df_multi_all = df_fake_all[df_fake_all['routing_method'] == 'multi_expert'].copy()\n",
    "                    multi_acc = df_multi_all['domain_correct'].mean() if len(df_multi_all) > 0 else 0.0\n",
    "            else:\n",
    "                multi_acc = 0.0\n",
    "            \n",
    "            # Genel accuracy (aÄŸÄ±rlÄ±klÄ± ortalama)\n",
    "            total_samples = len(df_normal_test) + len(df_multi_test)\n",
    "            if total_samples > 0:\n",
    "                overall_acc = (normal_acc * len(df_normal_test) + multi_acc * len(df_multi_test)) / total_samples\n",
    "            else:\n",
    "                overall_acc = 0.0\n",
    "            \n",
    "            threshold_results.append({\n",
    "                'threshold': threshold_val,\n",
    "                'normal_routing_count': len(df_normal_test),\n",
    "                'normal_routing_accuracy': normal_acc,\n",
    "                'multi_expert_count': len(df_multi_test),\n",
    "                'multi_expert_accuracy': multi_acc,\n",
    "                'overall_accuracy': overall_acc,\n",
    "                'total_samples': total_samples\n",
    "            })\n",
    "            \n",
    "            print(f\"   Normal Routing: {normal_acc*100:.2f}% ({len(df_normal_test):,} samples)\")\n",
    "            print(f\"   Multi-Expert Routing: {multi_acc*100:.2f}% ({len(df_multi_test):,} samples)\")\n",
    "            print(f\"   Overall Accuracy: {overall_acc*100:.2f}%\")\n",
    "        \n",
    "        # SonuÃ§larÄ± DataFrame'e Ã§evir\n",
    "        df_threshold_results = pd.DataFrame(threshold_results)\n",
    "        \n",
    "        # =============================================\n",
    "        # 1. Threshold vs Accuracy Visualization\n",
    "        # =============================================\n",
    "        \n",
    "        print(\"\\n1ï¸âƒ£ Threshold vs Accuracy gÃ¶rselleÅŸtirmesi oluÅŸturuluyor...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Ãœst: Accuracy vs Threshold\n",
    "        ax1 = axes[0]\n",
    "        ax1.plot(df_threshold_results['threshold'], df_threshold_results['overall_accuracy'], \n",
    "                marker='o', linewidth=2.5, markersize=8, label='Overall Accuracy', color='#2ecc71')\n",
    "        ax1.plot(df_threshold_results['threshold'], df_threshold_results['normal_routing_accuracy'], \n",
    "                marker='s', linewidth=2, markersize=6, label='Normal Routing', color='#3498db', linestyle='--')\n",
    "        ax1.plot(df_threshold_results['threshold'], df_threshold_results['multi_expert_accuracy'], \n",
    "                marker='^', linewidth=2, markersize=6, label='Multi-Expert Routing', color='#9b59b6', linestyle='--')\n",
    "        \n",
    "        ax1.set_xlabel('Routing Threshold', fontsize=13, fontweight='bold')\n",
    "        ax1.set_ylabel('Accuracy', fontsize=13, fontweight='bold')\n",
    "        ax1.set_title('Accuracy vs Routing Threshold', fontsize=14, fontweight='bold')\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim([0, 1.05])\n",
    "        \n",
    "        # Alt: Sample Count vs Threshold\n",
    "        ax2 = axes[1]\n",
    "        x = df_threshold_results['threshold']\n",
    "        width = (x.max() - x.min()) / len(x) * 0.8\n",
    "        \n",
    "        ax2.bar(x - width/2, df_threshold_results['normal_routing_count'], \n",
    "               width, label='Normal Routing', color='#3498db', alpha=0.7)\n",
    "        ax2.bar(x + width/2, df_threshold_results['multi_expert_count'], \n",
    "               width, label='Multi-Expert Routing', color='#9b59b6', alpha=0.7)\n",
    "        \n",
    "        ax2.set_xlabel('Routing Threshold', fontsize=13, fontweight='bold')\n",
    "        ax2.set_ylabel('Sample Count', fontsize=13, fontweight='bold')\n",
    "        ax2.set_title('Sample Distribution vs Routing Threshold', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIZ_THRESHOLD, 'threshold_vs_accuracy.png'), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"   âœ… Threshold vs Accuracy gÃ¶rselleÅŸtirmesi kaydedildi\")\n",
    "        \n",
    "        # =============================================\n",
    "        # 2. Best Threshold Analysis\n",
    "        # =============================================\n",
    "        \n",
    "        print(\"\\n2ï¸âƒ£ En iyi threshold analizi oluÅŸturuluyor...\")\n",
    "        \n",
    "        best_idx = df_threshold_results['overall_accuracy'].idxmax()\n",
    "        best_threshold = df_threshold_results.loc[best_idx, 'threshold']\n",
    "        best_accuracy = df_threshold_results.loc[best_idx, 'overall_accuracy']\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        bars = ax.bar(range(len(df_threshold_results)), df_threshold_results['overall_accuracy'], \n",
    "                     color=['#e74c3c' if i == best_idx else '#3498db' for i in range(len(df_threshold_results))],\n",
    "                     edgecolor='white', linewidth=2, alpha=0.85)\n",
    "        \n",
    "        for i, (bar, acc, thresh) in enumerate(zip(bars, df_threshold_results['overall_accuracy'], \n",
    "                                                   df_threshold_results['threshold'])):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{acc*100:.2f}%\\\\n({thresh:.2f})',\n",
    "                   ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Threshold Index', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Overall Accuracy', fontsize=13, fontweight='bold')\n",
    "        ax.set_title(f'Threshold Pool Results\\\\nBest: {best_threshold:.2f} (Accuracy: {best_accuracy*100:.2f}%)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(range(len(df_threshold_results)))\n",
    "        ax.set_xticklabels([f'{t:.2f}' for t in df_threshold_results['threshold']], rotation=45)\n",
    "        ax.set_ylim([0, 1.1])\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIZ_THRESHOLD, 'best_threshold_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"   âœ… En iyi threshold analizi kaydedildi\")\n",
    "        print(f\"      En iyi threshold: {best_threshold:.2f}\")\n",
    "        print(f\"      En iyi accuracy: {best_accuracy*100:.2f}%\")\n",
    "        \n",
    "        # =============================================\n",
    "        # 3. Save Threshold Results to CSV\n",
    "        # =============================================\n",
    "        \n",
    "        threshold_csv_path = os.path.join(OUTPUT_FOLDER, f'threshold_pool_results_{TIMESTAMP}.csv')\n",
    "        df_threshold_results.to_csv(threshold_csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n   âœ… Threshold pool sonuÃ§larÄ± CSV'ye kaydedildi: {threshold_csv_path}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Threshold Pool Testi tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š STAGE 1 & 2 GÃ–RSELLEÅTÄ°RMELERÄ° - BÃ–LÃœM 1\n",
    "Confusion Matrices ve Performance Charts\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š STAGE 1 & 2 GÃ–RSELLEÅTÄ°RMELERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fake gÃ¶rÃ¼ntÃ¼ler iÃ§in analiz\n",
    "df_fake = df_results[df_results['pred_rf_name'] == 'fake'].copy()\n",
    "\n",
    "if len(df_fake) == 0:\n",
    "    print(\"\\nâš ï¸  Fake gÃ¶rÃ¼ntÃ¼ bulunamadÄ±, Stage 1 & 2 gÃ¶rselleÅŸtirmeleri atlanÄ±yor\")\n",
    "else:\n",
    "    # =============================================\n",
    "    # 1. Stage 1 Domain Confusion Matrix\n",
    "    # =============================================\n",
    "    \n",
    "    print(\"\\n1ï¸âƒ£ Stage 1 Domain Confusion Matrix oluÅŸturuluyor...\")\n",
    "    \n",
    "    true_domains = df_fake['true_domain_label'].values\n",
    "    pred_domains = df_fake['final_pred_domain_label'].values\n",
    "    \n",
    "    cm_domain = confusion_matrix(true_domains, pred_domains, labels=range(NUM_DOMAINS))\n",
    "    cm_domain_normalized = cm_domain.astype('float') / (cm_domain.sum(axis=1)[:, np.newaxis] + 1e-10) * 100\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # Sol: Raw counts - Ä°yileÅŸtirilmiÅŸ annotation\n",
    "    matrix_size = len(DOMAIN_NAMES)\n",
    "    font_size = max(12, 16 - matrix_size // 3)\n",
    "    sns.heatmap(cm_domain, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "                ax=axes[0], cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
    "                annot_kws={'size': font_size, 'weight': 'bold', 'color': 'white'},\n",
    "                linewidths=2, linecolor='white')\n",
    "    axes[0].set_xlabel('Predicted Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    axes[0].set_ylabel('True Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    axes[0].set_title('Stage 1: Domain Classification (Multi-Expert)\\n(Absolute Counts)', \n",
    "                     fontsize=15, fontweight='bold', pad=15)\n",
    "    axes[0].tick_params(axis='x', rotation=45, labelsize=12)\n",
    "    axes[0].tick_params(axis='y', rotation=0, labelsize=12)\n",
    "    \n",
    "    # SaÄŸ: Percentages - Ä°yileÅŸtirilmiÅŸ annotation\n",
    "    sns.heatmap(cm_domain_normalized, annot=True, fmt='.1f', cmap='Greens',\n",
    "                xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "                ax=axes[1], cbar_kws={'label': 'Percentage (%)', 'shrink': 0.8},\n",
    "                annot_kws={'size': font_size, 'weight': 'bold', 'color': 'white'},\n",
    "                linewidths=2, linecolor='white', vmin=0, vmax=100)\n",
    "    axes[1].set_xlabel('Predicted Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    axes[1].set_ylabel('True Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    axes[1].set_title('Stage 1: Domain Classification (Multi-Expert)\\n(Row-Normalized %)', \n",
    "                     fontsize=15, fontweight='bold', pad=15)\n",
    "    axes[1].tick_params(axis='x', rotation=45, labelsize=11)\n",
    "    axes[1].tick_params(axis='y', rotation=0, labelsize=11)\n",
    "    \n",
    "    overall_acc = np.trace(cm_domain) / cm_domain.sum() if cm_domain.sum() > 0 else 0\n",
    "    fig.suptitle(f'Stage 1: Domain Classification Accuracy (Multi-Expert): {overall_acc*100:.2f}%', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_STAGE1, 'stage1_domain_confusion_matrix.png'), \n",
    "               dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ… Stage 1 confusion matrix kaydedildi\")\n",
    "    \n",
    "    # =============================================\n",
    "    # 2. Stage 2 Confusion Matrices (Her domain iÃ§in)\n",
    "    # =============================================\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£ Stage 2 Confusion Matrices oluÅŸturuluyor...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 14))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, domain in enumerate(DOMAIN_NAMES):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        df_domain_correct = df_fake[(df_fake['true_domain_name'] == domain) & \n",
    "                                    (df_fake['domain_correct'] == True)]\n",
    "        \n",
    "        if len(df_domain_correct) == 0:\n",
    "            ax.text(0.5, 0.5, f'{domain}\\\\nNo data', ha='center', va='center', fontsize=14)\n",
    "            ax.set_title(f'{domain} - No Data')\n",
    "            continue\n",
    "        \n",
    "        true_masks = df_domain_correct['true_mask_label'].values\n",
    "        pred_masks = df_domain_correct['pred_mask_label'].values\n",
    "        \n",
    "        unique_labels = sorted(set(true_masks) | set(pred_masks))\n",
    "        \n",
    "        if len(unique_labels) == 0:\n",
    "            ax.text(0.5, 0.5, f'{domain}\\\\nNo valid masks', ha='center', va='center', fontsize=14)\n",
    "            continue\n",
    "        \n",
    "        l2m = label_to_mask.get(domain, {})\n",
    "        mask_names = [l2m.get(l) or l2m.get(str(l), f\"Mask{l}\") for l in unique_labels]\n",
    "        \n",
    "        cm_mask = confusion_matrix(true_masks, pred_masks, labels=unique_labels)\n",
    "        cm_mask_norm = cm_mask.astype('float') / (cm_mask.sum(axis=1)[:, np.newaxis] + 1e-10) * 100\n",
    "        \n",
    "        # Dinamik font size\n",
    "        mask_font_size = max(10, 14 - len(unique_labels)//3)\n",
    "        sns.heatmap(cm_mask_norm, annot=True, fmt='.0f', cmap='YlOrRd',\n",
    "                   xticklabels=mask_names, yticklabels=mask_names,\n",
    "                   ax=ax, cbar_kws={'shrink': 0.6},\n",
    "                   annot_kws={'size': mask_font_size, 'weight': 'bold', 'color': 'white'},\n",
    "                   linewidths=1.5, linecolor='white', vmin=0, vmax=100)\n",
    "        \n",
    "        domain_mask_acc = np.trace(cm_mask) / cm_mask.sum() if cm_mask.sum() > 0 else 0\n",
    "        \n",
    "        ax.set_xlabel('Predicted Mask', fontsize=11, fontweight='bold', labelpad=8)\n",
    "        ax.set_ylabel('True Mask', fontsize=11, fontweight='bold', labelpad=8)\n",
    "        ax.set_title(f'{domain}\\\\nMask Accuracy: {domain_mask_acc*100:.1f}%\\\\n({len(df_domain_correct):,} samples)', \n",
    "                    fontsize=13, fontweight='bold', pad=12)\n",
    "        tick_font_size = max(8, 11 - len(unique_labels)//3)\n",
    "        ax.tick_params(axis='x', rotation=90, labelsize=tick_font_size)\n",
    "        ax.tick_params(axis='y', rotation=0, labelsize=tick_font_size)\n",
    "    \n",
    "    axes[5].axis('off')\n",
    "    \n",
    "    fig.suptitle('Stage 2: Mask Detection Confusion Matrices by Domain\\\\n(Only Domain-Correct Samples)', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_STAGE2, 'stage2_mask_confusion_matrices.png'), \n",
    "               dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ… Stage 2 confusion matrices kaydedildi\")\n",
    "    \n",
    "    # =============================================\n",
    "    # 3. Per-Domain Accuracy Bar Chart\n",
    "    # =============================================\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£ Per-domain accuracy chart oluÅŸturuluyor...\")\n",
    "    \n",
    "    # Domain bazÄ±nda accuracy hesapla\n",
    "    domain_analysis = []\n",
    "    for domain in DOMAIN_NAMES:\n",
    "        df_dom = df_fake[df_fake['true_domain_name'] == domain]\n",
    "        if len(df_dom) == 0:\n",
    "            continue\n",
    "        \n",
    "        s1_acc = df_dom['domain_correct'].mean()\n",
    "        df_dom_correct = df_dom[df_dom['domain_correct'] == True]\n",
    "        s2_acc = df_dom_correct['mask_correct'].mean() if len(df_dom_correct) > 0 else 0.0\n",
    "        pipeline_acc = df_dom['pipeline_correct'].mean() if 'pipeline_correct' in df_dom.columns else 0.0\n",
    "        \n",
    "        domain_analysis.append({\n",
    "            'domain': domain,\n",
    "            'stage1_accuracy': s1_acc,\n",
    "            'stage2_accuracy': s2_acc,\n",
    "            'pipeline_accuracy': pipeline_acc\n",
    "        })\n",
    "    \n",
    "    if domain_analysis:\n",
    "        df_domain_analysis = pd.DataFrame(domain_analysis)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "        \n",
    "        x = np.arange(len(df_domain_analysis))\n",
    "        width = 0.28\n",
    "        \n",
    "        s1_accs = df_domain_analysis['stage1_accuracy'].values\n",
    "        s2_accs = df_domain_analysis['stage2_accuracy'].values\n",
    "        pipeline_accs = df_domain_analysis['pipeline_accuracy'].values\n",
    "        \n",
    "        bars1 = ax.bar(x - width, s1_accs, width, label='Stage 1 (Domain)', \n",
    "                      color=COLORS['real'], edgecolor='white', linewidth=2, alpha=0.85)\n",
    "        bars2 = ax.bar(x, s2_accs, width, label='Stage 2 (Mask)', \n",
    "                      color=COLORS['correct'], edgecolor='white', linewidth=2, alpha=0.85)\n",
    "        bars3 = ax.bar(x + width, pipeline_accs, width, label='Full Pipeline', \n",
    "                      color=COLORS['fake'], edgecolor='white', linewidth=2, alpha=0.85)\n",
    "        \n",
    "        ax.axhline(y=0.85, color=COLORS['wrong'], linestyle='--', linewidth=2, \n",
    "                  alpha=0.7, label='Target (85%)')\n",
    "        \n",
    "        ax.set_xlabel('Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "        ax.set_ylabel('Accuracy', fontsize=14, fontweight='bold', labelpad=10)\n",
    "        ax.set_title('Hierarchical Pipeline Performance by Domain', fontsize=16, fontweight='bold', pad=15)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(df_domain_analysis['domain'], fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=11, loc='upper right', framealpha=0.9)\n",
    "        ax.set_ylim([0, 1.05])\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Ä°yileÅŸtirilmiÅŸ bar labels\n",
    "        max_value = max(s1_accs.max() if len(s1_accs) > 0 else 0, \n",
    "                       s2_accs.max() if len(s2_accs) > 0 else 0,\n",
    "                       pipeline_accs.max() if len(pipeline_accs) > 0 else 0)\n",
    "        \n",
    "        for bars, values in [(bars1, s1_accs), (bars2, s2_accs), (bars3, pipeline_accs)]:\n",
    "            for bar, val in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                # Yeterli yÃ¼kseklik kontrolÃ¼\n",
    "                if height > max_value * 0.05:\n",
    "                    text_y = height + max_value * 0.015\n",
    "                    text_color = 'black'\n",
    "                    va_pos = 'bottom'\n",
    "                    bbox_props = None\n",
    "                else:\n",
    "                    text_y = height / 2\n",
    "                    text_color = 'white'\n",
    "                    va_pos = 'center'\n",
    "                    bbox_props = dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7, edgecolor='none')\n",
    "                \n",
    "                ax.text(bar.get_x() + bar.get_width()/2., text_y,\n",
    "                       f'{val*100:.1f}%',\n",
    "                       ha='center', va=va_pos,\n",
    "                       fontsize=11, fontweight='bold', color=text_color,\n",
    "                       bbox=bbox_props)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIZ_METRICS, 'per_domain_accuracy.png'), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"   âœ… Per-domain accuracy chart kaydedildi\")\n",
    "\n",
    "print(f\"\\nâœ… Stage 1 & 2 gÃ¶rselleÅŸtirmeleri tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š PIPELINE FLOW GÃ–RSELLEÅTÄ°RMELERÄ°\n",
    "Sankey Diagram, Waterfall Chart, Decision Tree\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š PIPELINE FLOW GÃ–RSELLEÅTÄ°RMELERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Hesaplamalar\n",
    "n_total = len(df_results)\n",
    "n_stage0_real = len(df_results[df_results['pred_rf_name'] == 'real'])\n",
    "n_stage0_fake = len(df_results[df_results['pred_rf_name'] == 'fake'])\n",
    "\n",
    "df_fake = df_results[df_results['pred_rf_name'] == 'fake'].copy()\n",
    "if len(df_fake) > 0:\n",
    "    n_stage1_correct = df_fake['domain_correct'].sum()\n",
    "    n_stage1_wrong = len(df_fake) - n_stage1_correct\n",
    "    \n",
    "    df_stage1_correct = df_fake[df_fake['domain_correct'] == True].copy()\n",
    "    if len(df_stage1_correct) > 0:\n",
    "        n_stage2_correct = df_stage1_correct['mask_correct'].sum()\n",
    "        n_stage2_wrong = len(df_stage1_correct) - n_stage2_correct\n",
    "    else:\n",
    "        n_stage2_correct = 0\n",
    "        n_stage2_wrong = 0\n",
    "else:\n",
    "    n_stage1_correct = 0\n",
    "    n_stage1_wrong = 0\n",
    "    n_stage2_correct = 0\n",
    "    n_stage2_wrong = 0\n",
    "\n",
    "n_final_correct = n_stage2_correct  # Full pipeline success\n",
    "n_final_wrong = n_stage0_real + n_stage1_wrong + n_stage2_wrong  # All errors\n",
    "\n",
    "# =============================================\n",
    "# 1. Waterfall Chart\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Waterfall Chart oluÅŸturuluyor...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "stages = ['Total\\\\nInput', 'Stage 0\\\\n(Real)', 'Stage 0\\\\n(Fake)', \n",
    "         'Stage 1\\\\n(Correct)', 'Stage 1\\\\n(Wrong)', \n",
    "         'Stage 2\\\\n(Correct)', 'Stage 2\\\\n(Wrong)', 'Final\\\\nSuccess', 'Final\\\\nFailed']\n",
    "values = [n_total, -n_stage0_real, n_stage0_fake, \n",
    "         n_stage1_correct, -n_stage1_wrong,\n",
    "         n_stage2_correct, -n_stage2_wrong,\n",
    "         n_final_correct, -n_final_wrong]\n",
    "\n",
    "colors_waterfall = [COLORS['neutral'], COLORS['real'], COLORS['fake'],\n",
    "                   COLORS['correct'], COLORS['wrong'],\n",
    "                   COLORS['correct'], COLORS['wrong'],\n",
    "                   COLORS['correct'], COLORS['wrong']]\n",
    "\n",
    "cumulative = np.cumsum([0] + values[:-1])\n",
    "cumulative = np.append(cumulative, cumulative[-1] + values[-1])\n",
    "\n",
    "# Ä°yileÅŸtirilmiÅŸ waterfall chart - deÄŸerlerin net gÃ¶zÃ¼kmesi\n",
    "max_abs_val = max(abs(v) for v in values)\n",
    "for i, (stage, val, color, cum) in enumerate(zip(stages, values, colors_waterfall, cumulative[:-1])):\n",
    "    abs_val = abs(val)\n",
    "    bar_height = abs_val\n",
    "    \n",
    "    if val > 0:\n",
    "        ax.bar(i, val, bottom=cum, color=color, alpha=0.85, edgecolor='white', linewidth=2)\n",
    "        # Bar yeterince yÃ¼ksekse iÃ§te, deÄŸilse Ã¼stte\n",
    "        if bar_height > max_abs_val * 0.1:\n",
    "            text_y = cum + val/2\n",
    "            text_color = 'white'\n",
    "            va_pos = 'center'\n",
    "            bbox_props = None\n",
    "        else:\n",
    "            text_y = cum + val + max_abs_val * 0.01\n",
    "            text_color = 'black'\n",
    "            va_pos = 'bottom'\n",
    "            bbox_props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.9, edgecolor='none')\n",
    "    else:\n",
    "        ax.bar(i, val, bottom=cum, color=color, alpha=0.85, edgecolor='white', linewidth=2)\n",
    "        # Negatif bar'lar iÃ§in\n",
    "        if bar_height > max_abs_val * 0.1:\n",
    "            text_y = cum + val/2\n",
    "            text_color = 'white'\n",
    "            va_pos = 'center'\n",
    "            bbox_props = None\n",
    "        else:\n",
    "            text_y = cum + val - max_abs_val * 0.01\n",
    "            text_color = 'black'\n",
    "            va_pos = 'top'\n",
    "            bbox_props = dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.9, edgecolor='none')\n",
    "    \n",
    "    ax.text(i, text_y, f'{abs_val:,}', \n",
    "           ha='center', va=va_pos,\n",
    "           fontsize=12, fontweight='bold', color=text_color,\n",
    "           bbox=bbox_props)\n",
    "\n",
    "ax.set_xticks(range(len(stages)))\n",
    "ax.set_xticklabels(stages, fontsize=12, fontweight='bold', rotation=45, ha='right')\n",
    "ax.set_ylabel('Sample Count', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_title('Pipeline Flow: Waterfall Chart\\\\n(Stage 0 â†’ Stage 1 â†’ Stage 2)', \n",
    "            fontsize=16, fontweight='bold', pad=15)\n",
    "ax.grid(True, alpha=0.3, axis='y', linewidth=1.5)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_PIPELINE, 'pipeline_waterfall_chart.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Waterfall chart kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 2. Error Flow Diagram (Sankey-style)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Error Flow Diagram oluÅŸturuluyor...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "def draw_box(ax, x, y, width, height, color, label, count, pct):\n",
    "    \"\"\"Box Ã§iz - Ä°yileÅŸtirilmiÅŸ text\"\"\"\n",
    "    rect = plt.Rectangle((x - width/2, y - height/2), width, height, \n",
    "                        facecolor=color, edgecolor='white', linewidth=2, alpha=0.85)\n",
    "    ax.add_patch(rect)\n",
    "    # Font size ve text iyileÅŸtirmesi\n",
    "    ax.text(x, y, f'{label}\\\\n{count:,}\\\\n({pct:.1f}%)', \n",
    "           ha='center', va='center', fontsize=11, fontweight='bold', color='white',\n",
    "           bbox=dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "def draw_flow(ax, x1, y1, x2, y2, width, color, alpha=0.4):\n",
    "    \"\"\"AkÄ±ÅŸ Ã§izgisi\"\"\"\n",
    "    ax.fill([x1, x2, x2, x1], [y1-width/2, y2-width/2, y2+width/2, y1+width/2], \n",
    "           color=color, alpha=alpha, linewidth=0)\n",
    "\n",
    "# Box'lar\n",
    "draw_box(ax, 0, 0.5, 0.3, 0.35, COLORS['neutral'], 'Total Input', n_total, 100)\n",
    "\n",
    "# Stage 0\n",
    "s0_real_pct = n_stage0_real/n_total*100\n",
    "s0_fake_pct = n_stage0_fake/n_total*100\n",
    "draw_box(ax, 1, 0.75, 0.3, 0.25, COLORS['real'], 'Real', n_stage0_real, s0_real_pct)\n",
    "draw_box(ax, 1, 0.25, 0.3, 0.25, COLORS['fake'], 'Fake', n_stage0_fake, s0_fake_pct)\n",
    "\n",
    "# Stage 1 (sadece fake iÃ§in)\n",
    "if n_stage0_fake > 0:\n",
    "    s1_correct_pct = n_stage1_correct/n_stage0_fake*100\n",
    "    s1_wrong_pct = n_stage1_wrong/n_stage0_fake*100\n",
    "    draw_box(ax, 2, 0.7, 0.3, 0.2, COLORS['correct'], 'Domain\\\\nCorrect', n_stage1_correct, s1_correct_pct)\n",
    "    draw_box(ax, 2, 0.3, 0.3, 0.2, COLORS['wrong'], 'Domain\\\\nWrong', n_stage1_wrong, s1_wrong_pct)\n",
    "\n",
    "# Stage 2 (sadece domain correct iÃ§in)\n",
    "if n_stage1_correct > 0:\n",
    "    s2_correct_pct = n_stage2_correct/n_stage1_correct*100\n",
    "    s2_wrong_pct = n_stage2_wrong/n_stage1_correct*100\n",
    "    draw_box(ax, 3, 0.8, 0.3, 0.15, COLORS['correct'], 'Mask\\\\nCorrect', n_stage2_correct, s2_correct_pct)\n",
    "    draw_box(ax, 3, 0.55, 0.3, 0.15, COLORS['wrong'], 'Mask\\\\nWrong', n_stage2_wrong, s2_wrong_pct)\n",
    "\n",
    "# Final\n",
    "final_correct_pct = n_final_correct/n_total*100\n",
    "final_wrong_pct = n_final_wrong/n_total*100\n",
    "draw_box(ax, 4, 0.8, 0.3, 0.15, COLORS['correct'], 'SUCCESS', n_final_correct, final_correct_pct)\n",
    "draw_box(ax, 4, 0.3, 0.3, 0.35, COLORS['wrong'], 'FAILED', n_final_wrong, final_wrong_pct)\n",
    "\n",
    "# AkÄ±ÅŸ Ã§izgileri\n",
    "draw_flow(ax, 0.15, 0.65, 0.85, 0.75, 0.12, COLORS['real'])\n",
    "draw_flow(ax, 0.15, 0.35, 0.85, 0.25, 0.12, COLORS['fake'])\n",
    "if n_stage0_fake > 0:\n",
    "    draw_flow(ax, 1.15, 0.7, 1.85, 0.7, 0.08, COLORS['correct'])\n",
    "    draw_flow(ax, 1.15, 0.3, 1.85, 0.3, 0.08, COLORS['wrong'])\n",
    "if n_stage1_correct > 0:\n",
    "    draw_flow(ax, 2.15, 0.8, 2.85, 0.8, 0.06, COLORS['correct'])\n",
    "    draw_flow(ax, 2.15, 0.55, 2.85, 0.35, 0.06, COLORS['wrong'])\n",
    "draw_flow(ax, 3.15, 0.8, 3.85, 0.8, 0.06, COLORS['correct'])\n",
    "draw_flow(ax, 1.15, 0.3, 3.85, 0.3, 0.06, COLORS['wrong'], alpha=0.2)\n",
    "\n",
    "# Stage labels\n",
    "for i, stage in enumerate(['Input', 'Stage 0', 'Stage 1', 'Stage 2', 'Output']):\n",
    "    ax.text(i, -0.05, stage, ha='center', va='top', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-0.3, 4.3)\n",
    "ax.set_ylim(-0.1, 1.05)\n",
    "ax.axis('off')\n",
    "ax.set_title('Pipeline Error Flow Diagram\\\\n(Stage 0 â†’ Stage 1 â†’ Stage 2)', \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_PIPELINE, 'pipeline_error_flow_diagram.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Error flow diagram kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 3. Decision Tree (BasitleÅŸtirilmiÅŸ)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Decision Tree oluÅŸturuluyor...\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Decision tree yapÄ±sÄ±\n",
    "tree_nodes = {\n",
    "    'root': (2, 0.5, 'Input\\\\nAll Samples', n_total),\n",
    "    's0_real': (1, 0.8, 'Real', n_stage0_real),\n",
    "    's0_fake': (1, 0.2, 'Fake', n_stage0_fake),\n",
    "    's1_correct': (0, 0.3, 'Domain\\\\nCorrect', n_stage1_correct),\n",
    "    's1_wrong': (0, 0.1, 'Domain\\\\nWrong', n_stage1_wrong),\n",
    "    's2_correct': (-1, 0.35, 'Mask\\\\nCorrect', n_stage2_correct),\n",
    "    's2_wrong': (-1, 0.15, 'Mask\\\\nWrong', n_stage2_wrong),\n",
    "    'success': (-2, 0.35, 'SUCCESS', n_final_correct),\n",
    "    'failed': (-2, 0.15, 'FAILED', n_final_wrong)\n",
    "}\n",
    "\n",
    "# Node'larÄ± Ã§iz\n",
    "for node_id, (x, y, label, count) in tree_nodes.items():\n",
    "    if 'correct' in node_id or node_id == 'success':\n",
    "        color = COLORS['correct']\n",
    "    elif 'wrong' in node_id or node_id == 'failed':\n",
    "        color = COLORS['wrong']\n",
    "    elif 'real' in node_id:\n",
    "        color = COLORS['real']\n",
    "    elif 'fake' in node_id:\n",
    "        color = COLORS['fake']\n",
    "    else:\n",
    "        color = COLORS['neutral']\n",
    "    \n",
    "    circle = plt.Circle((x, y), 0.15, color=color, alpha=0.85, edgecolor='white', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    # Ä°yileÅŸtirilmiÅŸ text gÃ¶sterimi\n",
    "    ax.text(x, y, f'{label}\\\\n{count:,}', ha='center', va='center', \n",
    "           fontsize=11, fontweight='bold', color='white',\n",
    "           bbox=dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.6, edgecolor='white', linewidth=1))\n",
    "\n",
    "# BaÄŸlantÄ±lar\n",
    "connections = [\n",
    "    ('root', 's0_real'), ('root', 's0_fake'),\n",
    "    ('s0_fake', 's1_correct'), ('s0_fake', 's1_wrong'),\n",
    "    ('s1_correct', 's2_correct'), ('s1_correct', 's2_wrong'),\n",
    "    ('s2_correct', 'success'), ('s1_wrong', 'failed'), ('s2_wrong', 'failed')\n",
    "]\n",
    "\n",
    "for start, end in connections:\n",
    "    x1, y1, _, _ = tree_nodes[start]\n",
    "    x2, y2, _, _ = tree_nodes[end]\n",
    "    ax.plot([x1, x2], [y1, y2], color='gray', linewidth=2, alpha=0.5, zorder=0)\n",
    "\n",
    "ax.set_xlim(-2.5, 2.5)\n",
    "ax.set_ylim(-0.1, 1.0)\n",
    "ax.axis('off')\n",
    "ax.set_title('Pipeline Decision Tree\\\\n(Stage 0 â†’ Stage 1 â†’ Stage 2)', \n",
    "            fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_PIPELINE, 'pipeline_decision_tree.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Decision tree kaydedildi\")\n",
    "\n",
    "print(f\"\\nâœ… Pipeline flow gÃ¶rselleÅŸtirmeleri tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ GeliÅŸmiÅŸ Hata Analizi\n",
    "\n",
    "3D Error Heatmap, Error Sample Grids, Error Timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ” GELÄ°ÅMÄ°Å HATA ANALÄ°ZÄ°\n",
    "3D Error Heatmap, Error Timeline, Hata kategorileri\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” GELÄ°ÅMÄ°Å HATA ANALÄ°ZÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Hata kategorileri\n",
    "df_stage0_correct = df_results[df_results['stage0_correct'] == True]\n",
    "df_stage0_wrong = df_results[df_results['stage0_correct'] == False]\n",
    "\n",
    "df_fake = df_results[df_results['pred_rf_name'] == 'fake'].copy()\n",
    "if len(df_fake) > 0:\n",
    "    df_stage1_correct = df_fake[df_fake['domain_correct'] == True]\n",
    "    df_stage1_wrong = df_fake[df_fake['domain_correct'] == False]\n",
    "    \n",
    "    df_stage1_correct_s2_correct = df_stage1_correct[df_stage1_correct['mask_correct'] == True]\n",
    "    df_stage1_correct_s2_wrong = df_stage1_correct[df_stage1_correct['mask_correct'] == False]\n",
    "else:\n",
    "    df_stage1_correct = pd.DataFrame()\n",
    "    df_stage1_wrong = pd.DataFrame()\n",
    "    df_stage1_correct_s2_correct = pd.DataFrame()\n",
    "    df_stage1_correct_s2_wrong = pd.DataFrame()\n",
    "\n",
    "# =============================================\n",
    "# 1. 3D Error Heatmap (Stage 0 Ã— Stage 1 Ã— Stage 2)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ 3D Error Heatmap oluÅŸturuluyor...\")\n",
    "\n",
    "# Hata kombinasyonlarÄ±\n",
    "error_combinations = {\n",
    "    'S0âœ“ S1âœ“ S2âœ“': len(df_stage1_correct_s2_correct),\n",
    "    'S0âœ“ S1âœ“ S2âœ—': len(df_stage1_correct_s2_wrong),\n",
    "    'S0âœ“ S1âœ—': len(df_stage1_wrong),\n",
    "    'S0âœ—': len(df_stage0_wrong)\n",
    "}\n",
    "\n",
    "# 2D heatmap (3D yerine 2D matrix)\n",
    "error_matrix = np.zeros((3, 3))  # Stage 0, Stage 1, Stage 2\n",
    "\n",
    "# Matrix doldur\n",
    "error_matrix[0, 0] = len(df_stage1_correct_s2_correct)  # S0âœ“ S1âœ“ S2âœ“\n",
    "error_matrix[0, 1] = len(df_stage1_correct_s2_wrong)    # S0âœ“ S1âœ“ S2âœ—\n",
    "error_matrix[0, 2] = len(df_stage1_wrong)                # S0âœ“ S1âœ—\n",
    "error_matrix[1, 0] = len(df_stage0_wrong)                # S0âœ—\n",
    "error_matrix[1, 1] = 0\n",
    "error_matrix[1, 2] = 0\n",
    "error_matrix[2, 0] = 0\n",
    "error_matrix[2, 1] = 0\n",
    "error_matrix[2, 2] = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "labels = ['S0âœ“S1âœ“S2âœ“', 'S0âœ“S1âœ“S2âœ—', 'S0âœ“S1âœ—', 'S0âœ—', '', '', '', '', '']\n",
    "labels_2d = np.array(labels).reshape(3, 3)\n",
    "\n",
    "sns.heatmap(error_matrix, annot=True, fmt='.0f', cmap='Reds',\n",
    "           xticklabels=['Success', 'S2 Error', 'S1 Error'],\n",
    "           yticklabels=['S0 Success', 'S0 Error', ''],\n",
    "           ax=ax, cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
    "           annot_kws={'size': 16, 'weight': 'bold', 'color': 'white'},\n",
    "           linewidths=2, linecolor='white')\n",
    "\n",
    "ax.set_xlabel('Stage 1 & 2 Status', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_ylabel('Stage 0 Status', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_title('3D Error Heatmap\\\\n(Stage 0 Ã— Stage 1 Ã— Stage 2 Error Combinations)', \n",
    "            fontsize=15, fontweight='bold', pad=15)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_PIPELINE, 'error_3d_heatmap.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… 3D error heatmap kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 2. Error Timeline (Confidence threshold'a gÃ¶re)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Error Timeline oluÅŸturuluyor...\")\n",
    "\n",
    "# Confidence threshold'larÄ±\n",
    "conf_thresholds = np.linspace(0.5, 0.99, 20)\n",
    "stage0_error_rates = []\n",
    "stage1_error_rates = []\n",
    "stage2_error_rates = []\n",
    "\n",
    "for threshold in conf_thresholds:\n",
    "    # Stage 0\n",
    "    df_above = df_results[df_results['rf_confidence'] >= threshold]\n",
    "    if len(df_above) > 0:\n",
    "        s0_err = 1 - df_above['stage0_correct'].mean()\n",
    "    else:\n",
    "        s0_err = 0\n",
    "    stage0_error_rates.append(s0_err)\n",
    "    \n",
    "    # Stage 1\n",
    "    df_fake_above = df_fake[df_fake['domain_confidence'] >= threshold] if len(df_fake) > 0 else pd.DataFrame()\n",
    "    if len(df_fake_above) > 0:\n",
    "        s1_err = 1 - df_fake_above['domain_correct'].mean()\n",
    "    else:\n",
    "        s1_err = 0\n",
    "    stage1_error_rates.append(s1_err)\n",
    "    \n",
    "    # Stage 2\n",
    "    df_s1_correct_above = df_stage1_correct[df_stage1_correct['mask_confidence'] >= threshold] if len(df_stage1_correct) > 0 else pd.DataFrame()\n",
    "    if len(df_s1_correct_above) > 0:\n",
    "        s2_err = 1 - df_s1_correct_above['mask_correct'].mean()\n",
    "    else:\n",
    "        s2_err = 0\n",
    "    stage2_error_rates.append(s2_err)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax.plot(conf_thresholds, stage0_error_rates, marker='o', linewidth=2.5, \n",
    "       label='Stage 0 (Real/Fake)', color=COLORS['real'], markersize=8)\n",
    "if len(df_fake) > 0:\n",
    "    ax.plot(conf_thresholds, stage1_error_rates, marker='s', linewidth=2.5, \n",
    "           label='Stage 1 (Domain)', color=COLORS['fake'], markersize=8)\n",
    "if len(df_stage1_correct) > 0:\n",
    "    ax.plot(conf_thresholds, stage2_error_rates, marker='^', linewidth=2.5, \n",
    "           label='Stage 2 (Mask)', color=COLORS['correct'], markersize=8)\n",
    "\n",
    "ax.set_xlabel('Confidence Threshold', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_ylabel('Error Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_title('Error Rate vs Confidence Threshold\\\\n(Lower threshold = Higher error rate)', \n",
    "            fontsize=15, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=12, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, linewidth=1.5)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_PIPELINE, 'error_timeline.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Error timeline kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 3. Error Breakdown Pie Chart\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Error Breakdown Pie Chart oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Sol: Genel breakdown\n",
    "ax = axes[0]\n",
    "labels = ['S0âœ“S1âœ“S2âœ“\\\\n(Full Success)', 'S0âœ“S1âœ“S2âœ—\\\\n(Mask Error)', \n",
    "         'S0âœ“S1âœ—\\\\n(Domain Error)', 'S0âœ—\\\\n(Real/Fake Error)']\n",
    "sizes = [len(df_stage1_correct_s2_correct), len(df_stage1_correct_s2_wrong), \n",
    "        len(df_stage1_wrong), len(df_stage0_wrong)]\n",
    "colors_pie = [COLORS['correct'], COLORS['fake'], COLORS['wrong'], COLORS['real']]\n",
    "explode = (0.02, 0.02, 0.05, 0.05)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=colors_pie,\n",
    "                                 autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                                 textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
    "                                 wedgeprops={'edgecolor': 'white', 'linewidth': 2})\n",
    "# Autopct text'lerini iyileÅŸtir\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontsize(13)\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_color('white')\n",
    "ax.set_title(f'Pipeline Result Distribution\\\\n(Total: {n_total:,} samples)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# SaÄŸ: Domain bazÄ±nda error breakdown\n",
    "ax = axes[1]\n",
    "domain_errors = []\n",
    "for domain in DOMAIN_NAMES:\n",
    "    df_dom = df_fake[df_fake['true_domain_name'] == domain] if len(df_fake) > 0 else pd.DataFrame()\n",
    "    domain_errors.append(len(df_dom[df_dom['domain_correct'] == False]) if len(df_dom) > 0 else 0)\n",
    "\n",
    "bars = ax.barh(DOMAIN_NAMES, domain_errors, color=COLORS['wrong'], \n",
    "              edgecolor='white', alpha=0.85)\n",
    "ax.set_xlabel('Number of Domain Errors', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Domain', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Stage 1 Domain Errors by True Domain', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, err in zip(bars, domain_errors):\n",
    "    width = bar.get_width()\n",
    "    if width > 0:\n",
    "        ax.text(width + max(domain_errors)*0.02, bar.get_y() + bar.get_height()/2.,\n",
    "               f'{err:,}', ha='left', va='center', \n",
    "               fontsize=12, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.9, edgecolor='none'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_PIPELINE, 'error_breakdown.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Error breakdown kaydedildi\")\n",
    "\n",
    "print(f\"\\nâœ… GeliÅŸmiÅŸ hata analizi tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Performans Metrikleri\n",
    "\n",
    "ROC Curves, Precision-Recall Curves, F1-Score by Domain (tÃ¼m stage'ler iÃ§in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ“Š PERFORMANS METRÄ°KLERÄ°\n",
    "ROC Curves, Precision-Recall Curves, F1-Score by Domain\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š PERFORMANS METRÄ°KLERÄ°\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 1. Stage 0 ROC Curve (Binary: Real vs Fake)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Stage 0 ROC Curve oluÅŸturuluyor...\")\n",
    "\n",
    "# Real/Fake iÃ§in probability'leri al (fake class probability)\n",
    "rf_probs_all = []\n",
    "rf_labels_all = []\n",
    "\n",
    "# TÃ¼m batch'lerden probability'leri topla (ÅŸimdilik basit yaklaÅŸÄ±m)\n",
    "# GerÃ§ek uygulamada test sÄ±rasÄ±nda probability'leri kaydetmek gerekir\n",
    "# Åimdilik confidence'Ä± probability olarak kullanÄ±yoruz\n",
    "rf_probs_all = df_results['rf_confidence'].values\n",
    "rf_labels_all = df_results['true_rf_label'].values\n",
    "\n",
    "# Fake class iÃ§in probability (eÄŸer pred fake ise confidence, deÄŸilse 1-confidence)\n",
    "rf_probs_fake = np.where(df_results['pred_rf_label'] == 1, \n",
    "                        df_results['rf_confidence'].values,\n",
    "                        1 - df_results['rf_confidence'].values)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(rf_labels_all, rf_probs_fake, pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(fpr, tpr, color=COLORS['fake'], lw=3, \n",
    "       label=f'ROC curve (AUC = {roc_auc:.3f})', marker='o', markersize=4, markevery=50)\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
    "ax.set_title('Stage 0: Real/Fake ROC Curve', fontsize=16, fontweight='bold', pad=15)\n",
    "ax.legend(fontsize=12, loc='lower right', framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3, linewidth=1.5)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_STAGE0, 'stage0_roc_curve.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Stage 0 ROC curve kaydedildi (AUC: {roc_auc:.3f})\")\n",
    "\n",
    "# =============================================\n",
    "# 2. Stage 1 ROC Curves (Multi-class: One-vs-Rest)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Stage 1 ROC Curves oluÅŸturuluyor...\")\n",
    "\n",
    "if len(df_fake) > 0:\n",
    "    # Domain probability'leri (ÅŸimdilik confidence kullanÄ±yoruz)\n",
    "    # GerÃ§ek uygulamada tÃ¼m probability'leri kaydetmek gerekir\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Her domain iÃ§in one-vs-rest ROC\n",
    "    domain_aucs = {}\n",
    "    for domain_idx, domain_name in enumerate(DOMAIN_NAMES):\n",
    "        # Binary labels: bu domain mi deÄŸil mi?\n",
    "        y_true_binary = (df_fake['true_domain_label'] == domain_idx).astype(int).values\n",
    "        # Probability: bu domain'in confidence'Ä± (basitleÅŸtirilmiÅŸ)\n",
    "        y_score = np.where(df_fake['final_pred_domain_label'] == domain_idx,\n",
    "                          df_fake['domain_confidence'].values,\n",
    "                          1 - df_fake['domain_confidence'].values / NUM_DOMAINS)\n",
    "        \n",
    "        if len(np.unique(y_true_binary)) > 1:  # En az bir positive ve negative var\n",
    "            fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
    "            auc_score = auc(fpr, tpr)\n",
    "            domain_aucs[domain_name] = auc_score\n",
    "            \n",
    "            ax.plot(fpr, tpr, lw=2.5, label=f'{domain_name} (AUC = {auc_score:.3f})', marker='o', markersize=3, markevery=30)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    ax.set_title('Stage 1: Domain Classification ROC Curves\\\\n(One-vs-Rest)', \n",
    "                fontsize=16, fontweight='bold', pad=15)\n",
    "    ax.legend(fontsize=11, loc='lower right', framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3, linewidth=1.5)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_STAGE1, 'stage1_roc_curves.png'), \n",
    "               dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ… Stage 1 ROC curves kaydedildi\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Fake gÃ¶rÃ¼ntÃ¼ yok, Stage 1 ROC atlandÄ±\")\n",
    "\n",
    "# =============================================\n",
    "# 3. Stage 2 ROC Curves (Domain-specific)\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Stage 2 ROC Curves oluÅŸturuluyor...\")\n",
    "\n",
    "if len(df_stage1_correct) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, domain in enumerate(DOMAIN_NAMES):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        df_dom = df_stage1_correct[df_stage1_correct['true_domain_name'] == domain]\n",
    "        if len(df_dom) == 0:\n",
    "            ax.text(0.5, 0.5, f'{domain}\\\\nNo data', ha='center', va='center', fontsize=12)\n",
    "            ax.set_title(f'{domain}')\n",
    "            continue\n",
    "        \n",
    "        # Her mask iÃ§in one-vs-rest ROC (basitleÅŸtirilmiÅŸ)\n",
    "        # GerÃ§ek uygulamada tÃ¼m mask probability'lerini kaydetmek gerekir\n",
    "        true_masks = df_dom['true_mask_label'].values\n",
    "        mask_confs = df_dom['mask_confidence'].values\n",
    "        \n",
    "        # En yaygÄ±n mask iÃ§in binary ROC\n",
    "        if len(np.unique(true_masks)) > 1:\n",
    "            most_common_mask = int(np.bincount(true_masks.astype(int)).argmax())\n",
    "            y_true_binary = (true_masks == most_common_mask).astype(int)\n",
    "            y_score = mask_confs\n",
    "            \n",
    "            if len(np.unique(y_true_binary)) > 1:\n",
    "                fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
    "                auc_score = auc(fpr, tpr)\n",
    "                \n",
    "                ax.plot(fpr, tpr, lw=2.5, color=COLORS['correct'], \n",
    "                       label=f'AUC = {auc_score:.3f}', marker='o', markersize=3, markevery=20)\n",
    "                ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random')\n",
    "                ax.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold', labelpad=8)\n",
    "                ax.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold', labelpad=8)\n",
    "                ax.set_title(f'{domain}\\\\nMask Detection ROC', fontsize=13, fontweight='bold', pad=12)\n",
    "                ax.legend(fontsize=10, framealpha=0.9)\n",
    "                ax.grid(True, alpha=0.3, linewidth=1.2)\n",
    "                ax.tick_params(labelsize=10)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f'{domain}\\\\nInsufficient data', ha='center', va='center', fontsize=12)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'{domain}\\\\nSingle class', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    axes[5].axis('off')\n",
    "    \n",
    "    fig.suptitle('Stage 2: Mask Detection ROC Curves by Domain', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_STAGE2, 'stage2_roc_curves.png'), \n",
    "               dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ… Stage 2 ROC curves kaydedildi\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Domain correct Ã¶rnek yok, Stage 2 ROC atlandÄ±\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Precision-Recall Curves\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Precision-Recall Curves oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Stage 0 PR Curve\n",
    "rf_probs_fake = np.where(df_results['pred_rf_label'] == 1, \n",
    "                        df_results['rf_confidence'].values,\n",
    "                        1 - df_results['rf_confidence'].values)\n",
    "precision, recall, _ = precision_recall_curve(rf_labels_all, rf_probs_fake, pos_label=1)\n",
    "ap = average_precision_score(rf_labels_all, rf_probs_fake, pos_label=1)\n",
    "\n",
    "axes[0].plot(recall, precision, lw=3, color=COLORS['fake'], label=f'AP = {ap:.3f}', marker='o', markersize=4, markevery=50)\n",
    "axes[0].set_xlabel('Recall', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_ylabel('Precision', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_title('Stage 0: Real/Fake PR Curve', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[0].legend(fontsize=11, framealpha=0.9)\n",
    "axes[0].grid(True, alpha=0.3, linewidth=1.5)\n",
    "axes[0].tick_params(labelsize=11)\n",
    "\n",
    "# Stage 1 PR Curve (macro-averaged)\n",
    "if len(df_fake) > 0:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for domain_idx in range(NUM_DOMAINS):\n",
    "        y_true_binary = (df_fake['true_domain_label'] == domain_idx).astype(int).values\n",
    "        y_score = np.where(df_fake['final_pred_domain_label'] == domain_idx,\n",
    "                          df_fake['domain_confidence'].values,\n",
    "                          1 - df_fake['domain_confidence'].values / NUM_DOMAINS)\n",
    "        \n",
    "        if len(np.unique(y_true_binary)) > 1:\n",
    "            prec, rec, _ = precision_recall_curve(y_true_binary, y_score)\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "    \n",
    "    if precisions:\n",
    "        # Interpolate to common recall values\n",
    "        recall_common = np.linspace(0, 1, 100)\n",
    "        precisions_interp = []\n",
    "        for prec, rec in zip(precisions, recalls):\n",
    "            prec_interp = np.interp(recall_common, rec[::-1], prec[::-1])\n",
    "            precisions_interp.append(prec_interp)\n",
    "        \n",
    "        prec_mean = np.mean(precisions_interp, axis=0)\n",
    "        axes[1].plot(recall_common, prec_mean, lw=3, color=COLORS['real'], label='Macro-Averaged', marker='s', markersize=4, markevery=20)\n",
    "        axes[1].set_xlabel('Recall', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[1].set_ylabel('Precision', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[1].set_title('Stage 1: Domain PR Curve (Macro-Avg)', fontsize=14, fontweight='bold', pad=15)\n",
    "        axes[1].legend(fontsize=11, framealpha=0.9)\n",
    "        axes[1].grid(True, alpha=0.3, linewidth=1.5)\n",
    "        axes[1].tick_params(labelsize=11)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'Insufficient data', ha='center', va='center', fontsize=12)\n",
    "        axes[1].set_title('Stage 1: Domain PR Curve', fontsize=13, fontweight='bold')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No fake images', ha='center', va='center', fontsize=12)\n",
    "    axes[1].set_title('Stage 1: Domain PR Curve', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Stage 2 PR Curve (basitleÅŸtirilmiÅŸ)\n",
    "if len(df_stage1_correct) > 0:\n",
    "    mask_confs = df_stage1_correct['mask_confidence'].values\n",
    "    mask_correct = df_stage1_correct['mask_correct'].astype(int).values\n",
    "    \n",
    "    if len(np.unique(mask_correct)) > 1:\n",
    "        prec, rec, _ = precision_recall_curve(mask_correct, mask_confs)\n",
    "        ap = average_precision_score(mask_correct, mask_confs)\n",
    "        \n",
    "        axes[2].plot(rec, prec, lw=3, color=COLORS['correct'], label=f'AP = {ap:.3f}', marker='^', markersize=4, markevery=30)\n",
    "        axes[2].set_xlabel('Recall', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[2].set_ylabel('Precision', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[2].set_title('Stage 2: Mask PR Curve', fontsize=14, fontweight='bold', pad=15)\n",
    "        axes[2].legend(fontsize=11, framealpha=0.9)\n",
    "        axes[2].grid(True, alpha=0.3, linewidth=1.5)\n",
    "        axes[2].tick_params(labelsize=11)\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'Insufficient data', ha='center', va='center', fontsize=12)\n",
    "        axes[2].set_title('Stage 2: Mask PR Curve', fontsize=13, fontweight='bold')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'No domain-correct samples', ha='center', va='center', fontsize=12)\n",
    "    axes[2].set_title('Stage 2: Mask PR Curve', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_METRICS, 'precision_recall_curves.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Precision-Recall curves kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 5. F1-Score by Domain\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ F1-Score by Domain oluÅŸturuluyor...\")\n",
    "\n",
    "if len(df_fake) > 0:\n",
    "    domain_metrics = []\n",
    "    \n",
    "    for domain in DOMAIN_NAMES:\n",
    "        df_dom = df_fake[df_fake['true_domain_name'] == domain]\n",
    "        if len(df_dom) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Stage 1 metrics\n",
    "        y_true_s1 = df_dom['true_domain_label'].values\n",
    "        y_pred_s1 = df_dom['final_pred_domain_label'].values\n",
    "        s1_precision = precision_score(y_true_s1, y_pred_s1, average='weighted', zero_division=0)\n",
    "        s1_recall = recall_score(y_true_s1, y_pred_s1, average='weighted', zero_division=0)\n",
    "        s1_f1 = f1_score(y_true_s1, y_pred_s1, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Stage 2 metrics\n",
    "        df_dom_correct = df_dom[df_dom['domain_correct'] == True]\n",
    "        if len(df_dom_correct) > 0:\n",
    "            y_true_s2 = df_dom_correct['true_mask_label'].values\n",
    "            y_pred_s2 = df_dom_correct['pred_mask_label'].values\n",
    "            s2_precision = precision_score(y_true_s2, y_pred_s2, average='weighted', zero_division=0)\n",
    "            s2_recall = recall_score(y_true_s2, y_pred_s2, average='weighted', zero_division=0)\n",
    "            s2_f1 = f1_score(y_true_s2, y_pred_s2, average='weighted', zero_division=0)\n",
    "        else:\n",
    "            s2_precision = s2_recall = s2_f1 = 0.0\n",
    "        \n",
    "        domain_metrics.append({\n",
    "            'domain': domain,\n",
    "            'stage1_precision': s1_precision,\n",
    "            'stage1_recall': s1_recall,\n",
    "            'stage1_f1': s1_f1,\n",
    "            'stage2_precision': s2_precision,\n",
    "            'stage2_recall': s2_recall,\n",
    "            'stage2_f1': s2_f1\n",
    "        })\n",
    "    \n",
    "    if domain_metrics:\n",
    "        df_metrics = pd.DataFrame(domain_metrics)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        x = np.arange(len(df_metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Precision - Ä°yileÅŸtirilmiÅŸ bar labels\n",
    "        bars1_p = axes[0].bar(x - width/2, df_metrics['stage1_precision'], width, \n",
    "                   label='Stage 1', color=COLORS['real'], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "        bars2_p = axes[0].bar(x + width/2, df_metrics['stage2_precision'], width, \n",
    "                   label='Stage 2', color=COLORS['correct'], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "        max_val_p = max(df_metrics['stage1_precision'].max(), df_metrics['stage2_precision'].max())\n",
    "        for bars, values in [(bars1_p, df_metrics['stage1_precision']), (bars2_p, df_metrics['stage2_precision'])]:\n",
    "            for bar, val in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                if height > max_val_p * 0.05:\n",
    "                    text_y = height + max_val_p * 0.015\n",
    "                    text_color = 'black'\n",
    "                    bbox_props = None\n",
    "                else:\n",
    "                    text_y = height / 2\n",
    "                    text_color = 'white'\n",
    "                    bbox_props = dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7, edgecolor='none')\n",
    "                axes[0].text(bar.get_x() + bar.get_width()/2., text_y,\n",
    "                           f'{val:.3f}', ha='center', va='bottom' if height > max_val_p * 0.05 else 'center',\n",
    "                           fontsize=10, fontweight='bold', color=text_color, bbox=bbox_props)\n",
    "        axes[0].set_xlabel('Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[0].set_ylabel('Precision', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[0].set_title('Precision by Domain', fontsize=14, fontweight='bold', pad=15)\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(df_metrics['domain'], rotation=45, ha='right', fontsize=11)\n",
    "        axes[0].legend(fontsize=11, framealpha=0.9)\n",
    "        axes[0].grid(True, alpha=0.3, axis='y', linewidth=1.2)\n",
    "        axes[0].tick_params(labelsize=11)\n",
    "        \n",
    "        # Recall - Ä°yileÅŸtirilmiÅŸ bar labels\n",
    "        bars1_r = axes[1].bar(x - width/2, df_metrics['stage1_recall'], width, \n",
    "                   label='Stage 1', color=COLORS['real'], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "        bars2_r = axes[1].bar(x + width/2, df_metrics['stage2_recall'], width, \n",
    "                   label='Stage 2', color=COLORS['correct'], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "        max_val_r = max(df_metrics['stage1_recall'].max(), df_metrics['stage2_recall'].max())\n",
    "        for bars, values in [(bars1_r, df_metrics['stage1_recall']), (bars2_r, df_metrics['stage2_recall'])]:\n",
    "            for bar, val in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                if height > max_val_r * 0.05:\n",
    "                    text_y = height + max_val_r * 0.015\n",
    "                    text_color = 'black'\n",
    "                    bbox_props = None\n",
    "                else:\n",
    "                    text_y = height / 2\n",
    "                    text_color = 'white'\n",
    "                    bbox_props = dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7, edgecolor='none')\n",
    "                axes[1].text(bar.get_x() + bar.get_width()/2., text_y,\n",
    "                           f'{val:.3f}', ha='center', va='bottom' if height > max_val_r * 0.05 else 'center',\n",
    "                           fontsize=10, fontweight='bold', color=text_color, bbox=bbox_props)\n",
    "        axes[1].set_xlabel('Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[1].set_ylabel('Recall', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[1].set_title('Recall by Domain', fontsize=14, fontweight='bold', pad=15)\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(df_metrics['domain'], rotation=45, ha='right', fontsize=11)\n",
    "        axes[1].legend(fontsize=11, framealpha=0.9)\n",
    "        axes[1].grid(True, alpha=0.3, axis='y', linewidth=1.2)\n",
    "        axes[1].tick_params(labelsize=11)\n",
    "        \n",
    "        # F1-Score - Ä°yileÅŸtirilmiÅŸ bar labels\n",
    "        bars1_f = axes[2].bar(x - width/2, df_metrics['stage1_f1'], width, \n",
    "                   label='Stage 1', color=COLORS['real'], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "        bars2_f = axes[2].bar(x + width/2, df_metrics['stage2_f1'], width, \n",
    "                   label='Stage 2', color=COLORS['correct'], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "        max_val_f = max(df_metrics['stage1_f1'].max(), df_metrics['stage2_f1'].max())\n",
    "        for bars, values in [(bars1_f, df_metrics['stage1_f1']), (bars2_f, df_metrics['stage2_f1'])]:\n",
    "            for bar, val in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                if height > max_val_f * 0.05:\n",
    "                    text_y = height + max_val_f * 0.015\n",
    "                    text_color = 'black'\n",
    "                    bbox_props = None\n",
    "                else:\n",
    "                    text_y = height / 2\n",
    "                    text_color = 'white'\n",
    "                    bbox_props = dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7, edgecolor='none')\n",
    "                axes[2].text(bar.get_x() + bar.get_width()/2., text_y,\n",
    "                           f'{val:.3f}', ha='center', va='bottom' if height > max_val_f * 0.05 else 'center',\n",
    "                           fontsize=10, fontweight='bold', color=text_color, bbox=bbox_props)\n",
    "        axes[2].set_xlabel('Domain', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[2].set_ylabel('F1-Score', fontsize=13, fontweight='bold', labelpad=10)\n",
    "        axes[2].set_title('F1-Score by Domain', fontsize=14, fontweight='bold', pad=15)\n",
    "        axes[2].set_xticks(x)\n",
    "        axes[2].set_xticklabels(df_metrics['domain'], rotation=45, ha='right', fontsize=11)\n",
    "        axes[2].legend(fontsize=11, framealpha=0.9)\n",
    "        axes[2].grid(True, alpha=0.3, axis='y', linewidth=1.2)\n",
    "        axes[2].tick_params(labelsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(VIZ_METRICS, 'f1_score_by_domain.png'), \n",
    "                   dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"   âœ… F1-Score by domain kaydedildi\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Domain metrikleri hesaplanamadÄ±\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Fake gÃ¶rÃ¼ntÃ¼ yok, F1-Score atlandÄ±\")\n",
    "\n",
    "print(f\"\\nâœ… Performans metrikleri tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ Ä°leri DÃ¼zey Analizler\n",
    "\n",
    "Confidence Calibration, Domain Transition Matrix, Mask Distribution, Real vs Fake Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ”¬ Ä°LERÄ° DÃœZEY ANALÄ°ZLER\n",
    "Confidence Calibration, Domain Transition Matrix, Mask Distribution, Real vs Fake Statistics\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”¬ Ä°LERÄ° DÃœZEY ANALÄ°ZLER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 1. Confidence Calibration Plot\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ Confidence Calibration Plot oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Stage 0 Calibration\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "rf_confs = df_results['rf_confidence'].values\n",
    "rf_correct = df_results['stage0_correct'].astype(int).values\n",
    "\n",
    "bin_indices = np.digitize(rf_confs, bins) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, len(bins)-2)\n",
    "\n",
    "calibrated_acc = []\n",
    "for i in range(len(bins)-1):\n",
    "    mask = (bin_indices == i)\n",
    "    if mask.sum() > 0:\n",
    "        calibrated_acc.append(rf_correct[mask].mean())\n",
    "    else:\n",
    "        calibrated_acc.append(0)\n",
    "\n",
    "axes[0].plot(bin_centers, calibrated_acc, 'o-', linewidth=3, markersize=10, \n",
    "            color=COLORS['real'], label='Actual Accuracy', markeredgecolor='white', markeredgewidth=1.5)\n",
    "axes[0].plot([0, 1], [0, 1], '--', color='gray', linewidth=2, label='Perfect Calibration')\n",
    "axes[0].set_xlabel('Predicted Confidence', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_ylabel('Actual Accuracy', fontsize=13, fontweight='bold', labelpad=10)\n",
    "axes[0].set_title('Stage 0: Real/Fake Calibration', fontsize=14, fontweight='bold', pad=15)\n",
    "axes[0].legend(fontsize=11, framealpha=0.9)\n",
    "axes[0].grid(True, alpha=0.3, linewidth=1.5)\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].tick_params(labelsize=11)\n",
    "\n",
    "# Stage 1 Calibration\n",
    "if len(df_fake) > 0:\n",
    "    domain_confs = df_fake['domain_confidence'].values\n",
    "    domain_correct = df_fake['domain_correct'].astype(int).values\n",
    "    \n",
    "    bin_indices = np.digitize(domain_confs, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, len(bins)-2)\n",
    "    \n",
    "    calibrated_acc = []\n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (bin_indices == i)\n",
    "        if mask.sum() > 0:\n",
    "            calibrated_acc.append(domain_correct[mask].mean())\n",
    "        else:\n",
    "            calibrated_acc.append(0)\n",
    "    \n",
    "    axes[1].plot(bin_centers, calibrated_acc, 'o-', linewidth=3, markersize=10, \n",
    "                color=COLORS['fake'], label='Actual Accuracy', markeredgecolor='white', markeredgewidth=1.5)\n",
    "    axes[1].plot([0, 1], [0, 1], '--', color='gray', linewidth=2, label='Perfect Calibration')\n",
    "    axes[1].set_xlabel('Predicted Confidence', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    axes[1].set_ylabel('Actual Accuracy', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    axes[1].set_title('Stage 1: Domain Calibration', fontsize=14, fontweight='bold', pad=15)\n",
    "    axes[1].legend(fontsize=11, framealpha=0.9)\n",
    "    axes[1].grid(True, alpha=0.3, linewidth=1.5)\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    axes[1].tick_params(labelsize=11)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No fake images', ha='center', va='center', fontsize=12)\n",
    "    axes[1].set_title('Stage 1: Domain Calibration', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Stage 2 Calibration\n",
    "if len(df_stage1_correct) > 0:\n",
    "    mask_confs = df_stage1_correct['mask_confidence'].values\n",
    "    mask_correct = df_stage1_correct['mask_correct'].astype(int).values\n",
    "    \n",
    "    bin_indices = np.digitize(mask_confs, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, len(bins)-2)\n",
    "    \n",
    "    calibrated_acc = []\n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (bin_indices == i)\n",
    "        if mask.sum() > 0:\n",
    "            calibrated_acc.append(mask_correct[mask].mean())\n",
    "        else:\n",
    "            calibrated_acc.append(0)\n",
    "    \n",
    "    axes[2].plot(bin_centers, calibrated_acc, 'o-', linewidth=3, markersize=10, \n",
    "                color=COLORS['correct'], label='Actual Accuracy', markeredgecolor='white', markeredgewidth=1.5)\n",
    "    axes[2].plot([0, 1], [0, 1], '--', color='gray', linewidth=2, label='Perfect Calibration')\n",
    "    axes[2].set_xlabel('Predicted Confidence', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    axes[2].set_ylabel('Actual Accuracy', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    axes[2].set_title('Stage 2: Mask Calibration', fontsize=14, fontweight='bold', pad=15)\n",
    "    axes[2].legend(fontsize=11, framealpha=0.9)\n",
    "    axes[2].grid(True, alpha=0.3, linewidth=1.5)\n",
    "    axes[2].set_xlim([0, 1])\n",
    "    axes[2].set_ylim([0, 1])\n",
    "    axes[2].tick_params(labelsize=11)\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'No domain-correct samples', ha='center', va='center', fontsize=12)\n",
    "    axes[2].set_title('Stage 2: Mask Calibration', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_METRICS, 'confidence_calibration.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Confidence calibration plot kaydedildi\")\n",
    "\n",
    "# =============================================\n",
    "# 2. Domain Transition Matrix\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Domain Transition Matrix oluÅŸturuluyor...\")\n",
    "\n",
    "if len(df_fake) > 0:\n",
    "    transition_matrix = np.zeros((NUM_DOMAINS, NUM_DOMAINS))\n",
    "    \n",
    "    for i in range(NUM_DOMAINS):\n",
    "        for j in range(NUM_DOMAINS):\n",
    "            count = len(df_fake[(df_fake['true_domain_label'] == i) & \n",
    "                               (df_fake['final_pred_domain_label'] == j)])\n",
    "            transition_matrix[i, j] = count\n",
    "    \n",
    "    # Normalize by row\n",
    "    transition_matrix_norm = transition_matrix / (transition_matrix.sum(axis=1, keepdims=True) + 1e-10) * 100\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # Raw counts - Ä°yileÅŸtirilmiÅŸ\n",
    "    matrix_size = len(DOMAIN_NAMES)\n",
    "    font_size = max(12, 16 - matrix_size // 3)\n",
    "    sns.heatmap(transition_matrix, annot=True, fmt='.0f', cmap='Blues',\n",
    "               xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "               ax=axes[0], cbar_kws={'label': 'Count', 'shrink': 0.8},\n",
    "               annot_kws={'size': font_size, 'weight': 'bold', 'color': 'white'},\n",
    "               linewidths=2, linecolor='white')\n",
    "    axes[0].set_xlabel('Predicted Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    axes[0].set_ylabel('True Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    axes[0].set_title('Domain Transition Matrix\\\\n(Absolute Counts)', \n",
    "                     fontsize=15, fontweight='bold', pad=15)\n",
    "    axes[0].tick_params(axis='x', rotation=45, labelsize=12)\n",
    "    axes[0].tick_params(axis='y', rotation=0, labelsize=12)\n",
    "    \n",
    "    # Normalized percentages - Ä°yileÅŸtirilmiÅŸ\n",
    "    sns.heatmap(transition_matrix_norm, annot=True, fmt='.1f', cmap='Greens',\n",
    "               xticklabels=DOMAIN_NAMES, yticklabels=DOMAIN_NAMES,\n",
    "               ax=axes[1], cbar_kws={'label': 'Percentage (%)', 'shrink': 0.8},\n",
    "               annot_kws={'size': font_size, 'weight': 'bold', 'color': 'white'},\n",
    "               linewidths=2, linecolor='white', vmin=0, vmax=100)\n",
    "    axes[1].set_xlabel('Predicted Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    axes[1].set_ylabel('True Domain', fontsize=14, fontweight='bold', labelpad=10)\n",
    "    axes[1].set_title('Domain Transition Matrix\\\\n(Row-Normalized %)', \n",
    "                     fontsize=15, fontweight='bold', pad=15)\n",
    "    axes[1].tick_params(axis='x', rotation=45, labelsize=12)\n",
    "    axes[1].tick_params(axis='y', rotation=0, labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_METRICS, 'domain_transition_matrix.png'), \n",
    "               dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ… Domain transition matrix kaydedildi\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Fake gÃ¶rÃ¼ntÃ¼ yok, transition matrix atlandÄ±\")\n",
    "\n",
    "# =============================================\n",
    "# 3. Mask Distribution Analysis\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Mask Distribution Analysis oluÅŸturuluyor...\")\n",
    "\n",
    "if len(df_stage1_correct) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, domain in enumerate(DOMAIN_NAMES):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        df_dom = df_stage1_correct[df_stage1_correct['true_domain_name'] == domain]\n",
    "        if len(df_dom) == 0:\n",
    "            ax.text(0.5, 0.5, f'{domain}\\\\nNo data', ha='center', va='center', fontsize=12)\n",
    "            ax.set_title(f'{domain}')\n",
    "            continue\n",
    "        \n",
    "        # True mask distribution\n",
    "        true_mask_counts = df_dom['true_mask_name'].value_counts()\n",
    "        pred_mask_counts = df_dom['pred_mask_name'].value_counts()\n",
    "        \n",
    "        # Combine all masks\n",
    "        all_masks = sorted(set(true_mask_counts.index) | set(pred_mask_counts.index))\n",
    "        \n",
    "        true_counts = [true_mask_counts.get(m, 0) for m in all_masks]\n",
    "        pred_counts = [pred_mask_counts.get(m, 0) for m in all_masks]\n",
    "        \n",
    "        x = np.arange(len(all_masks))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, true_counts, width, label='True', \n",
    "                      color=COLORS['correct'], alpha=0.85)\n",
    "        bars2 = ax.bar(x + width/2, pred_counts, width, label='Predicted', \n",
    "                      color=COLORS['fake'], alpha=0.85)\n",
    "        \n",
    "        ax.set_xlabel('Mask Type', fontsize=11, fontweight='bold', labelpad=8)\n",
    "        ax.set_ylabel('Count', fontsize=11, fontweight='bold', labelpad=8)\n",
    "        ax.set_title(f'{domain}\\\\nMask Distribution', fontsize=13, fontweight='bold', pad=12)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(all_masks, rotation=45, ha='right', fontsize=9)\n",
    "        ax.legend(fontsize=10, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3, axis='y', linewidth=1.2)\n",
    "        ax.tick_params(labelsize=9)\n",
    "    \n",
    "    axes[5].axis('off')\n",
    "    \n",
    "    fig.suptitle('Mask Distribution Analysis by Domain\\\\n(True vs Predicted)', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_METRICS, 'mask_distribution_analysis.png'), \n",
    "               dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   âœ… Mask distribution analysis kaydedildi\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Domain correct Ã¶rnek yok, mask distribution atlandÄ±\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Real vs Fake Statistics\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Real vs Fake Statistics oluÅŸturuluyor...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Sol: Real/Fake distribution\n",
    "ax = axes[0]\n",
    "rf_counts = df_results['pred_rf_name'].value_counts()\n",
    "colors_rf = [COLORS['real'] if name == 'real' else COLORS['fake'] for name in rf_counts.index]\n",
    "\n",
    "bars = ax.bar(rf_counts.index, rf_counts.values, color=colors_rf, \n",
    "             edgecolor='white', linewidth=2, alpha=0.85)\n",
    "ax.set_xlabel('Predicted Class', fontsize=13, fontweight='bold', labelpad=10)\n",
    "ax.set_ylabel('Count', fontsize=13, fontweight='bold', labelpad=10)\n",
    "ax.set_title('Real vs Fake Distribution\\\\n(Predicted)', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.grid(True, alpha=0.3, axis='y', linewidth=1.2)\n",
    "ax.tick_params(labelsize=11)\n",
    "\n",
    "max_count = rf_counts.values.max()\n",
    "for bar, count in zip(bars, rf_counts.values):\n",
    "    height = bar.get_height()\n",
    "    pct = count/len(df_results)*100\n",
    "    if height > max_count * 0.1:\n",
    "        text_y = height + max_count * 0.02\n",
    "        text_color = 'black'\n",
    "        bbox_props = None\n",
    "    else:\n",
    "        text_y = height / 2\n",
    "        text_color = 'white'\n",
    "        bbox_props = dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7, edgecolor='none')\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., text_y,\n",
    "           f'{count:,}\\\\n({pct:.1f}%)',\n",
    "           ha='center', va='bottom' if height > max_count * 0.1 else 'center',\n",
    "           fontsize=12, fontweight='bold', color=text_color, bbox=bbox_props)\n",
    "\n",
    "# SaÄŸ: Real/Fake accuracy comparison\n",
    "ax = axes[1]\n",
    "rf_stats = []\n",
    "for rf_name in ['real', 'fake']:\n",
    "    df_rf = df_results[df_results['pred_rf_name'] == rf_name]\n",
    "    if len(df_rf) > 0:\n",
    "        accuracy = df_rf['stage0_correct'].mean() if 'stage0_correct' in df_rf.columns else 0.0\n",
    "        avg_conf = df_rf['rf_confidence'].mean()\n",
    "        rf_stats.append({\n",
    "            'class': rf_name,\n",
    "            'count': len(df_rf),\n",
    "            'accuracy': accuracy,\n",
    "            'avg_confidence': avg_conf\n",
    "        })\n",
    "\n",
    "if rf_stats:\n",
    "    df_rf_stats = pd.DataFrame(rf_stats)\n",
    "    \n",
    "    x = np.arange(len(df_rf_stats))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, df_rf_stats['accuracy'], width, \n",
    "                  label='Accuracy', color=COLORS['correct'], alpha=0.85)\n",
    "    bars2 = ax.bar(x + width/2, df_rf_stats['avg_confidence'], width, \n",
    "                  label='Avg Confidence', color=COLORS['neutral'], alpha=0.85)\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    ax.set_ylabel('Value', fontsize=13, fontweight='bold', labelpad=10)\n",
    "    ax.set_title('Real vs Fake: Accuracy & Confidence', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_rf_stats['class'], fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=11, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3, axis='y', linewidth=1.2)\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.tick_params(labelsize=11)\n",
    "    \n",
    "    for bars, values in [(bars1, df_rf_stats['accuracy']), (bars2, df_rf_stats['avg_confidence'])]:\n",
    "        max_val = max(df_rf_stats['accuracy'].max(), df_rf_stats['avg_confidence'].max())\n",
    "        for bars, values in [(bars1, df_rf_stats['accuracy']), (bars2, df_rf_stats['avg_confidence'])]:\n",
    "            for bar, val in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                if height > max_val * 0.05:\n",
    "                    text_y = height + max_val * 0.02\n",
    "                    text_color = 'black'\n",
    "                    bbox_props = None\n",
    "                else:\n",
    "                    text_y = height / 2\n",
    "                    text_color = 'white'\n",
    "                    bbox_props = dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7, edgecolor='none')\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., text_y,\n",
    "                       f'{val:.3f}',\n",
    "                       ha='center', va='bottom' if height > max_val * 0.05 else 'center',\n",
    "                       fontsize=11, fontweight='bold', color=text_color, bbox=bbox_props)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No data', ha='center', va='center', fontsize=12)\n",
    "    ax.set_title('Real vs Fake: Accuracy & Confidence', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(VIZ_METRICS, 'real_vs_fake_statistics.png'), \n",
    "           dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ… Real vs Fake statistics kaydedildi\")\n",
    "\n",
    "print(f\"\\nâœ… Ä°leri dÃ¼zey analizler tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£3ï¸âƒ£ Export: CSV ve JSON\n",
    "\n",
    "TÃ¼m sonuÃ§larÄ± CSV ve JSON formatÄ±nda export etme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ’¾ EXPORT: CSV ve JSON\n",
    "TÃ¼m sonuÃ§larÄ± ve metrikleri export etme\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ’¾ EXPORT: CSV ve JSON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================\n",
    "# 0. Metrikleri Hesapla (EÄŸer henÃ¼z hesaplanmadÄ±ysa)\n",
    "# =============================================\n",
    "\n",
    "# Stage 0 accuracy\n",
    "if 'stage0_accuracy' not in locals():\n",
    "    stage0_correct = df_results['stage0_correct'].sum()\n",
    "    total_samples = len(df_results)\n",
    "    stage0_accuracy = stage0_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# Stage 1 accuracy (fake gÃ¶rÃ¼ntÃ¼ler iÃ§in)\n",
    "df_fake = df_results[df_results['pred_rf_name'] == 'fake'].copy()\n",
    "if len(df_fake) > 0:\n",
    "    if 'domain_accuracy' not in locals():\n",
    "        domain_correct = df_fake['domain_correct'].sum()\n",
    "        domain_accuracy = domain_correct / len(df_fake) if len(df_fake) > 0 else 0.0\n",
    "else:\n",
    "    domain_accuracy = 0.0\n",
    "\n",
    "# Stage 2 accuracy (domain-correct fake gÃ¶rÃ¼ntÃ¼ler iÃ§in)\n",
    "df_stage1_correct = df_fake[df_fake['domain_correct'] == True].copy() if len(df_fake) > 0 else pd.DataFrame()\n",
    "if len(df_stage1_correct) > 0:\n",
    "    if 'mask_accuracy_on_correct' not in locals():\n",
    "        mask_correct = df_stage1_correct['mask_correct'].sum()\n",
    "        mask_accuracy_on_correct = mask_correct / len(df_stage1_correct) if len(df_stage1_correct) > 0 else 0.0\n",
    "else:\n",
    "    mask_accuracy_on_correct = 0.0\n",
    "\n",
    "# Pipeline accuracy\n",
    "if 'pipeline_accuracy' not in locals():\n",
    "    pipeline_correct = df_results['pipeline_correct'].sum() if 'pipeline_correct' in df_results.columns else 0\n",
    "    total_samples = len(df_results)\n",
    "    pipeline_accuracy = pipeline_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# =============================================\n",
    "# 1. DetaylÄ± SonuÃ§lar CSV\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ DetaylÄ± sonuÃ§lar CSV'ye kaydediliyor...\")\n",
    "\n",
    "# df_results zaten hazÄ±r, sadece export et (timestamp ile)\n",
    "csv_filename = f'detailed_results_{TIMESTAMP}.csv'\n",
    "csv_path = os.path.join(OUTPUT_FOLDER, csv_filename)\n",
    "df_results.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"   âœ… DetaylÄ± sonuÃ§lar kaydedildi: {csv_path}\")\n",
    "print(f\"      Toplam {len(df_results):,} satÄ±r, {len(df_results.columns)} sÃ¼tun\")\n",
    "print(f\"      OluÅŸturulma zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# =============================================\n",
    "# 2. Ã–zet Metrikler JSON\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Ã–zet metrikler JSON'a kaydediliyor...\")\n",
    "\n",
    "# Ã–zet metrikler\n",
    "summary_metrics = {\n",
    "    'test_info': {\n",
    "        'total_samples': int(len(df_results)),\n",
    "        'test_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_workers': NUM_WORKERS,\n",
    "        'use_amp': USE_AMP,\n",
    "        'use_multi_expert_routing': USE_MULTI_EXPERT_ROUTING,\n",
    "        'routing_threshold': ROUTING_THRESHOLD if USE_MULTI_EXPERT_ROUTING else None\n",
    "    },\n",
    "    'stage0_metrics': {\n",
    "        'accuracy': float(stage0_accuracy),\n",
    "        'total_samples': int(len(df_results)),\n",
    "        'real_count': int(len(df_results[df_results['pred_rf_name'] == 'real'])),\n",
    "        'fake_count': int(len(df_results[df_results['pred_rf_name'] == 'fake'])),\n",
    "        'real_accuracy': float(df_results[df_results['pred_rf_name'] == 'real']['stage0_correct'].mean()) if len(df_results[df_results['pred_rf_name'] == 'real']) > 0 else 0.0,\n",
    "        'fake_accuracy': float(df_results[df_results['pred_rf_name'] == 'fake']['stage0_correct'].mean()) if len(df_results[df_results['pred_rf_name'] == 'fake']) > 0 else 0.0,\n",
    "        'avg_confidence': float(df_results['rf_confidence'].mean())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Stage 1 metrikleri\n",
    "# df_fake zaten yukarÄ±da tanÄ±mlandÄ±\n",
    "if len(df_fake) > 0:\n",
    "    summary_metrics['stage1_metrics'] = {\n",
    "        'accuracy': float(domain_accuracy),\n",
    "        'total_samples': int(len(df_fake)),\n",
    "        'avg_confidence': float(df_fake['domain_confidence'].mean()),\n",
    "        'multi_expert_usage': int(df_fake['used_multi_expert'].sum()) if 'used_multi_expert' in df_fake.columns else 0,\n",
    "        'multi_expert_usage_pct': float(df_fake['used_multi_expert'].mean() * 100) if 'used_multi_expert' in df_fake.columns else 0.0\n",
    "    }\n",
    "    \n",
    "    # Domain bazÄ±nda metrikler\n",
    "    domain_metrics = {}\n",
    "    for domain in DOMAIN_NAMES:\n",
    "        df_dom = df_fake[df_fake['true_domain_name'] == domain]\n",
    "        if len(df_dom) > 0:\n",
    "            domain_metrics[domain] = {\n",
    "                'count': int(len(df_dom)),\n",
    "                'accuracy': float(df_dom['domain_correct'].mean()),\n",
    "                'avg_confidence': float(df_dom['domain_confidence'].mean())\n",
    "            }\n",
    "    summary_metrics['stage1_metrics']['by_domain'] = domain_metrics\n",
    "else:\n",
    "    summary_metrics['stage1_metrics'] = {\n",
    "        'accuracy': 0.0,\n",
    "        'total_samples': 0,\n",
    "        'avg_confidence': 0.0\n",
    "    }\n",
    "\n",
    "# Stage 2 metrikleri\n",
    "# df_stage1_correct zaten yukarÄ±da tanÄ±mlandÄ±\n",
    "if len(df_stage1_correct) > 0:\n",
    "    summary_metrics['stage2_metrics'] = {\n",
    "        'accuracy': float(mask_accuracy_on_correct),\n",
    "        'total_samples': int(len(df_stage1_correct)),\n",
    "        'avg_confidence': float(df_stage1_correct['mask_confidence'].mean())\n",
    "    }\n",
    "    \n",
    "    # Domain bazÄ±nda Stage 2 metrikleri\n",
    "    domain_s2_metrics = {}\n",
    "    for domain in DOMAIN_NAMES:\n",
    "        df_dom = df_stage1_correct[df_stage1_correct['true_domain_name'] == domain]\n",
    "        if len(df_dom) > 0:\n",
    "            domain_s2_metrics[domain] = {\n",
    "                'count': int(len(df_dom)),\n",
    "                'accuracy': float(df_dom['mask_correct'].mean()),\n",
    "                'avg_confidence': float(df_dom['mask_confidence'].mean())\n",
    "            }\n",
    "    summary_metrics['stage2_metrics']['by_domain'] = domain_s2_metrics\n",
    "else:\n",
    "    summary_metrics['stage2_metrics'] = {\n",
    "        'accuracy': 0.0,\n",
    "        'total_samples': 0,\n",
    "        'avg_confidence': 0.0\n",
    "    }\n",
    "\n",
    "# Full pipeline metrikleri\n",
    "summary_metrics['pipeline_metrics'] = {\n",
    "    'full_pipeline_accuracy': float(pipeline_accuracy),\n",
    "    'stage0_accuracy': float(stage0_accuracy),\n",
    "    'stage1_accuracy': float(domain_accuracy) if len(df_fake) > 0 else 0.0,\n",
    "    'stage2_accuracy': float(mask_accuracy_on_correct) if len(df_stage1_correct) > 0 else 0.0\n",
    "}\n",
    "\n",
    "# Hata analizi\n",
    "error_breakdown = {\n",
    "    'stage0_errors': int(len(df_results[df_results['stage0_correct'] == False])),\n",
    "    'stage1_errors': int(len(df_fake[df_fake['domain_correct'] == False])) if len(df_fake) > 0 else 0,\n",
    "    'stage2_errors': int(len(df_stage1_correct[df_stage1_correct['mask_correct'] == False])) if len(df_stage1_correct) > 0 else 0,\n",
    "    'full_success': int(len(df_stage1_correct[df_stage1_correct['mask_correct'] == True])) if len(df_stage1_correct) > 0 else 0\n",
    "}\n",
    "summary_metrics['error_breakdown'] = error_breakdown\n",
    "\n",
    "# JSON'a kaydet (timestamp ile)\n",
    "json_filename = f'summary_metrics_{TIMESTAMP}.json'\n",
    "json_path = os.path.join(OUTPUT_FOLDER, json_filename)\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"   âœ… Ã–zet metrikler kaydedildi: {json_path}\")\n",
    "print(f\"      OluÅŸturulma zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# =============================================\n",
    "# 3. Domain BazÄ±nda Ã–zet CSV\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Domain bazÄ±nda Ã¶zet CSV'ye kaydediliyor...\")\n",
    "\n",
    "if len(df_fake) > 0:\n",
    "    domain_summary = []\n",
    "    for domain in DOMAIN_NAMES:\n",
    "        df_dom = df_fake[df_fake['true_domain_name'] == domain]\n",
    "        if len(df_dom) == 0:\n",
    "            continue\n",
    "        \n",
    "        df_dom_correct = df_dom[df_dom['domain_correct'] == True]\n",
    "        \n",
    "        domain_summary.append({\n",
    "            'domain': domain,\n",
    "            'total_samples': len(df_dom),\n",
    "            'stage1_accuracy': df_dom['domain_correct'].mean(),\n",
    "            'stage1_avg_confidence': df_dom['domain_confidence'].mean(),\n",
    "            'stage2_samples': len(df_dom_correct),\n",
    "            'stage2_accuracy': df_dom_correct['mask_correct'].mean() if len(df_dom_correct) > 0 else 0.0,\n",
    "            'stage2_avg_confidence': df_dom_correct['mask_confidence'].mean() if len(df_dom_correct) > 0 else 0.0,\n",
    "            'pipeline_accuracy': df_dom['pipeline_correct'].mean() if 'pipeline_correct' in df_dom.columns else 0.0\n",
    "        })\n",
    "    \n",
    "    if domain_summary:\n",
    "        df_domain_summary = pd.DataFrame(domain_summary)\n",
    "        domain_csv_filename = f'domain_summary_{TIMESTAMP}.csv'\n",
    "        domain_csv_path = os.path.join(OUTPUT_FOLDER, domain_csv_filename)\n",
    "        df_domain_summary.to_csv(domain_csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   âœ… Domain Ã¶zeti kaydedildi: {domain_csv_path}\")\n",
    "        print(f\"      OluÅŸturulma zamanÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Domain Ã¶zeti oluÅŸturulamadÄ±\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Fake gÃ¶rÃ¼ntÃ¼ yok, domain Ã¶zeti atlandÄ±\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Export Ã–zeti\n",
    "# =============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š EXPORT Ã–ZETÄ°\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ… DetaylÄ± sonuÃ§lar: {csv_filename}\")\n",
    "print(f\"âœ… Ã–zet metrikler: {json_filename}\")\n",
    "if len(df_fake) > 0 and 'domain_summary' in locals():\n",
    "    print(f\"âœ… Domain Ã¶zeti: {domain_csv_filename}\")\n",
    "print(f\"âœ… GÃ¶rselleÅŸtirmeler: {VIZ_FOLDER}\")\n",
    "print(f\"\\\\nğŸ“ TÃ¼m Ã§Ä±ktÄ±lar: {OUTPUT_FOLDER}\")\n",
    "print(f\"ğŸ“… Test zamanÄ±: {TIMESTAMP}\")\n",
    "\n",
    "print(f\"\\nâœ… Export tamamlandÄ±!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Test TamamlandÄ±!\n",
    "\n",
    "TÃ¼m analizler, gÃ¶rselleÅŸtirmeler ve export iÅŸlemleri tamamlandÄ±.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "âœ… TEST TAMAMLANDI - Ã–ZET\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… COMBINED PIPELINE FULL TEST TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“Š Ã–ZET METRÄ°KLER:\")\n",
    "print(f\"   Toplam Test Ã–rneÄŸi: {len(df_results):,}\")\n",
    "print(f\"   Stage 0 (Real/Fake) Accuracy: {stage0_accuracy*100:.2f}%\")\n",
    "if len(df_results[df_results['pred_rf_name'] == 'fake']) > 0:\n",
    "    print(f\"   Stage 1 (Domain) Accuracy: {domain_accuracy*100:.2f}%\")\n",
    "    df_stage1_correct = df_results[(df_results['pred_rf_name'] == 'fake') & \n",
    "                                   (df_results['domain_correct'] == True)]\n",
    "    if len(df_stage1_correct) > 0:\n",
    "        print(f\"   Stage 2 (Mask) Accuracy: {mask_accuracy_on_correct*100:.2f}%\")\n",
    "print(f\"   Full Pipeline Accuracy: {pipeline_accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ“ Ã‡IKTILAR:\")\n",
    "print(f\"   ğŸ“‚ Ana KlasÃ¶r: {OUTPUT_FOLDER}\")\n",
    "print(f\"   ğŸ“… Test ZamanÄ±: {TIMESTAMP}\")\n",
    "print(f\"   ğŸ“Š DetaylÄ± SonuÃ§lar: detailed_results_{TIMESTAMP}.csv\")\n",
    "print(f\"   ğŸ“ˆ Ã–zet Metrikler: summary_metrics_{TIMESTAMP}.json\")\n",
    "print(f\"   ğŸ¨ GÃ¶rselleÅŸtirmeler: {len(os.listdir(VIZ_FOLDER)) if os.path.exists(VIZ_FOLDER) else 0} dosya\")\n",
    "\n",
    "print(\"\\nğŸ¯ Ã–ZELLÄ°KLER:\")\n",
    "print(\"   âœ… Stage 0: Real/Fake sÄ±nÄ±flandÄ±rmasÄ±\")\n",
    "print(\"   âœ… Stage 1: Domain tespiti (Multi-Expert Routing ile)\")\n",
    "print(\"   âœ… Stage 2: Mask tespiti (Domain-specific)\")\n",
    "print(\"   âœ… KapsamlÄ± gÃ¶rselleÅŸtirmeler ve analizler\")\n",
    "print(\"   âœ… ROC/PR curves, F1-scores, confusion matrices\")\n",
    "print(\"   âœ… Confidence calibration, error analysis\")\n",
    "print(\"   âœ… CSV ve JSON export\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ TÃœM Ä°ÅLEMLER BAÅARIYLA TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
