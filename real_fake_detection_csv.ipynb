{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Fake Detection Training - CSV-Based Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook trains a deep learning model to detect real vs fake (inpainted) images using:\n",
    "- **CSV-based data splits** (no random splitting)\n",
    "- **ResNet50** architecture with ImageNet pretrained weights\n",
    "- **Comprehensive error analysis** on fake images (domain, mask, quality metrics, generative model)\n",
    "- **Real-time GPU monitoring** during training\n",
    "- **10+ detailed visualizations** for model performance analysis\n",
    "\n",
    "## Key Features\n",
    "- Enhanced progress bar with GPU stats, learning rate, and ETA\n",
    "- Best model tracking (saves best_model.pth)\n",
    "- Exhaustive misclassified fake images analysis\n",
    "- Quality metrics correlation (SSIM, LPIPS, MSE)\n",
    "- Domain/Mask/Generative Model accuracy breakdowns\n",
    "\n",
    "## Data\n",
    "- **Fake images**: ADE20K, CelebAHQ, CityScapes, HumanParsing, OpenImages\n",
    "- **Generative models**: StableDiffusion v5, StableDiffusion XL, Kandinsky 2.2, OpenJourney\n",
    "- **Training**: 75K fake + 16K real images (balanced)\n",
    "- **Validation**: 16K fake + 3K real images (balanced)\n",
    "- **Test**: 16K fake + 3K real images (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU monitoring not available (install nvidia-ml-py3 for GPU stats)\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Import utils from current directory\n",
    "from utils import setup_device, create_training_folder, calculate_metrics, export_metrics_to_csv\n",
    "\n",
    "# Try to import pynvml for GPU monitoring\n",
    "try:\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    GPU_MONITORING = True\n",
    "    print(\"GPU monitoring enabled (pynvml loaded successfully)\")\n",
    "except:\n",
    "    GPU_MONITORING = False\n",
    "    print(\"GPU monitoring not available (install nvidia-ml-py3 for GPU stats)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Hyperparameters (All Customizable)",
    "",
    "# Training Parameters",
    "BATCH_SIZE = 8",
    "LEARNING_RATE = 0.001",
    "NUM_EPOCHS = 20",
    "WEIGHT_DECAY = 1e-4",
    "",
    "# Scheduler Parameters",
    "SCHEDULER_PATIENCE = 2",
    "SCHEDULER_FACTOR = 0.5",
    "",
    "# Data Parameters",
    "IMAGE_SIZE = 224",
    "USE_AUGMENTATION = True",
    "NUM_WORKERS = 0",
    "",
    "# DATA USAGE RATIO (0.0 to 1.0)",
    "# Control what percentage of CSV data to use for training/val/test",
    "TRAIN_DATA_RATIO = 1.0  # Use 100% of training data (1.0 = all, 0.1 = 10%, etc.)",
    "VAL_DATA_RATIO = 1.0    # Use 100% of validation data",
    "TEST_DATA_RATIO = 1.0   # Use 100% of test data",
    "",
    "# CLASS BALANCING STRATEGY",
    "# Choose how to handle class imbalance (more fakes than reals):",
    "#   'undersampling': Drop excess fake samples to match real count (loses data)",
    "#   'loss_weighting': Use all data but weight loss by inverse class frequency (no data loss, RECOMMENDED)",
    "#   'none': Use all data without balancing (model may be biased)",
    "CLASS_BALANCE_METHOD = 'loss_weighting'  # Recommended: 'loss_weighting'",
    "",
    "# Loss Function Type",
    "LOSS_TYPE = 'crossentropy'  # Options: 'crossentropy', 'focal'",
    "FOCAL_ALPHA = 0.25  # Weight for focal loss (only used if LOSS_TYPE=\"focal\")",
    "FOCAL_GAMMA = 2.0   # Focusing parameter (only used if LOSS_TYPE=\"focal\")",
    "",
    "# RESUME TRAINING FROM CHECKPOINT",
    "# Set to True to continue training from a previous best_model.pth",
    "# Set to False to start fresh training from ImageNet pretrained weights",
    "RESUME_FROM_CHECKPOINT = False",
    "",
    "# If RESUME_FROM_CHECKPOINT is True, specify which version to resume from",
    "# Leave as None to auto-detect the latest version, or specify version number (e.g., 1, 2, 3)",
    "RESUME_VERSION = None  # None = auto-detect latest, or specify: 1, 2, 3, etc.",
    "",
    "# Paths",
    "FAKE_TRAIN_CSV = 'dataset_splits/fake_only_split/fake_train.csv'",
    "FAKE_VAL_CSV = 'dataset_splits/fake_only_split/fake_val.csv'",
    "FAKE_TEST_CSV = 'dataset_splits/fake_only_split/fake_test.csv'",
    "",
    "REAL_TRAIN_CSV = 'dataset_splits/real_only_split/real_train.csv'",
    "REAL_VAL_CSV = 'dataset_splits/real_only_split/real_val.csv'",
    "REAL_TEST_CSV = 'dataset_splits/real_only_split/real_test.csv'",
    "",
    "# Output folder",
    "NOTEBOOK_NAME = 'real_fake_detection_csv'",
    "",
    "# Random seed for reproducibility",
    "RANDOM_SEED = 42",
    "",
    "# Set seeds",
    "torch.manual_seed(RANDOM_SEED)",
    "np.random.seed(RANDOM_SEED)",
    "if torch.cuda.is_available():",
    "    torch.cuda.manual_seed(RANDOM_SEED)",
    "",
    "print(\"=\"*60)",
    "print(\"HYPERPARAMETERS\")",
    "print(\"=\"*60)",
    "print(f\"Batch Size: {BATCH_SIZE}\")",
    "print(f\"Learning Rate: {LEARNING_RATE}\")",
    "print(f\"Epochs: {NUM_EPOCHS}\")",
    "print(f\"Weight Decay: {WEIGHT_DECAY}\")",
    "print(f\"Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")",
    "print(f\"Augmentation: {USE_AUGMENTATION}\")",
    "print(f\"Scheduler Patience: {SCHEDULER_PATIENCE}\")",
    "print(f\"Scheduler Factor: {SCHEDULER_FACTOR}\")",
    "print(\"\\n\" + \"-\"*60)",
    "print(\"DATA USAGE RATIOS\")",
    "print(\"-\"*60)",
    "print(f\"Train Data Ratio: {TRAIN_DATA_RATIO*100:.1f}% of available data\")",
    "print(f\"Val Data Ratio: {VAL_DATA_RATIO*100:.1f}% of available data\")",
    "print(f\"Test Data Ratio: {TEST_DATA_RATIO*100:.1f}% of available data\")",
    "print(\"\\n\" + \"-\"*60)",
    "print(\"CLASS BALANCING STRATEGY\")",
    "print(\"-\"*60)",
    "print(f\"Method: {CLASS_BALANCE_METHOD}\")",
    "if CLASS_BALANCE_METHOD == 'loss_weighting':",
    "",
    "# Loss Function Type",
    "LOSS_TYPE = 'crossentropy'  # Options: 'crossentropy', 'focal'",
    "FOCAL_ALPHA = 0.25  # Weight for focal loss (only used if LOSS_TYPE=\"focal\")",
    "FOCAL_GAMMA = 2.0   # Focusing parameter (only used if LOSS_TYPE=\"focal\")",
    "    print(\"  ‚úÖ Using loss weighting (no data loss, RECOMMENDED)\")",
    "elif CLASS_BALANCE_METHOD == 'undersampling':",
    "",
    "# Loss Function Type",
    "LOSS_TYPE = 'crossentropy'  # Options: 'crossentropy', 'focal'",
    "FOCAL_ALPHA = 0.25  # Weight for focal loss (only used if LOSS_TYPE=\"focal\")",
    "FOCAL_GAMMA = 2.0   # Focusing parameter (only used if LOSS_TYPE=\"focal\")",
    "    print(\"  ‚ö†Ô∏è  Using undersampling (data loss)\")",
    "else:",
    "    print(\"  ‚ö†Ô∏è  No balancing (model may be biased)\")",
    "print(\"\\n\" + \"-\"*60)",
    "print(\"CHECKPOINT RESUME\")",
    "print(\"-\"*60)",
    "print(f\"Resume from checkpoint: {RESUME_FROM_CHECKPOINT}\")",
    "if RESUME_FROM_CHECKPOINT:",
    "    if RESUME_VERSION is None:",
    "        print(\"  üìÇ Will auto-detect latest version\")",
    "    else:",
    "        print(f\"  üìÇ Will resume from version: v{RESUME_VERSION}\")",
    "else:",
    "    print(\"  üÜï Starting fresh training from ImageNet weights\")",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: GPU Setup and Folder Creation\n\n# Setup device\ndevice = setup_device()\n\n# CRITICAL: Verify CUDA is actually being used\nif not torch.cuda.is_available():\n    raise RuntimeError(\"‚ùå CUDA is not available! GPU training is required.\")\n\nprint(f\"‚úÖ Using device: {device}\")\nprint(f\"‚úÖ CUDA is available: {torch.cuda.is_available()}\")\nprint(f\"‚úÖ Current CUDA device: {torch.cuda.current_device()}\")\nprint(f\"‚úÖ Device count: {torch.cuda.device_count()}\")\n\n# Create training folder structure with versioning\nbase_dir, models_dir, data_dir, viz_dir, version = create_training_folder(NOTEBOOK_NAME)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TRAINING SESSION: {NOTEBOOK_NAME} - Version {version}\")\nprint(f\"{'='*60}\")\nprint(f\"Base directory: {base_dir}\")\nprint(f\"Models directory: {models_dir}\")\nprint(f\"Data directory: {data_dir}\")\nprint(f\"Visualizations directory: {viz_dir}\")\nprint(f\"{'='*60}\\n\")\n\n# Verify directories exist\nprint(\"DIRECTORY VERIFICATION:\")\nprint(f\"  Models dir exists: {os.path.exists(models_dir)} ‚úÖ\" if os.path.exists(models_dir) else f\"  Models dir exists: {os.path.exists(models_dir)} ‚ùå\")\nprint(f\"  Data dir exists: {os.path.exists(data_dir)} ‚úÖ\" if os.path.exists(data_dir) else f\"  Data dir exists: {os.path.exists(data_dir)} ‚ùå\")\nprint(f\"  Viz dir exists: {os.path.exists(viz_dir)} ‚úÖ\" if os.path.exists(viz_dir) else f\"  Viz dir exists: {os.path.exists(viz_dir)} ‚ùå\")\n\n# Display detailed GPU info\nprint(\"\\nGPU INFORMATION:\")\nprint(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\nprint(f\"  GPU Memory Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\nprint(f\"  GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\nprint(f\"  GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n\n# GPU monitoring setup\nif GPU_MONITORING:\n    try:\n        gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n        gpu_name = pynvml.nvmlDeviceGetName(gpu_handle)\n        print(f\"  pynvml GPU Device: {gpu_name}\")\n        \n        mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n        print(f\"  GPU Memory Used: {mem_info.used / 1024**3:.2f} GB / {mem_info.total / 1024**3:.2f} GB\")\n        print(f\"  GPU Memory Free: {mem_info.free / 1024**3:.2f} GB\")\n    except Exception as e:\n        print(f\"  pynvml monitoring failed: {e}\")\n        GPU_MONITORING = False\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üöÄ GPU VERIFICATION COMPLETE - TRAINING WILL USE GPU\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: RealFakeCSVDataset Class",
    "",
    "class RealFakeCSVDataset(Dataset):",
    "    \"\"\"",
    "    Dataset that loads real and fake images from CSV files.",
    "    Returns: (image, label, metadata_dict)",
    "    ",
    "    Labels:",
    "        0 = Real",
    "        1 = Fake",
    "    ",
    "    Metadata:",
    "        - Fake: perturbed_img_id, mask_name, domain, ssim, lpips_score, mse,",
    "                model_name, dataset, area_ratio, sem_magnitude",
    "        - Real: real_img_id, parent_dataset",
    "    \"\"\"",
    "    ",
    "    def __init__(self, fake_csv_path, real_csv_path, transform=None, ",
    "                 balance_method='loss_weighting', data_ratio=1.0, seed=42):",
    "        \"\"\"",
    "        Args:",
    "            fake_csv_path: Path to fake images CSV",
    "            real_csv_path: Path to real images CSV",
    "            transform: Image transformations",
    "            balance_method: 'undersampling', 'loss_weighting', or 'none'",
    "            data_ratio: Percentage of data to use (0.0 to 1.0)",
    "            seed: Random seed for reproducibility",
    "        \"\"\"",
    "        self.transform = transform",
    "        self.balance_method = balance_method",
    "        ",
    "        # Load CSV files",
    "        print(f\"Loading fake CSV: {fake_csv_path}\")",
    "        fake_df = pd.read_csv(fake_csv_path)",
    "        print(f\"  Loaded {len(fake_df):,} fake images from CSV\")",
    "        ",
    "        print(f\"Loading real CSV: {real_csv_path}\")",
    "        real_df = pd.read_csv(real_csv_path)",
    "        print(f\"  Loaded {len(real_df):,} real images from CSV\")",
    "        ",
    "        # Apply data ratio if < 1.0",
    "        if data_ratio < 1.0:",
    "            print(f\"\\n‚öôÔ∏è  Applying data ratio: {data_ratio*100:.1f}%\")",
    "            fake_sample_size = int(len(fake_df) * data_ratio)",
    "            real_sample_size = int(len(real_df) * data_ratio)",
    "            ",
    "            fake_df = fake_df.sample(n=fake_sample_size, random_state=seed).reset_index(drop=True)",
    "            real_df = real_df.sample(n=real_sample_size, random_state=seed).reset_index(drop=True)",
    "            ",
    "            print(f\"  Fake images after ratio: {len(fake_df):,} ({data_ratio*100:.1f}%)\")",
    "            print(f\"  Real images after ratio: {len(real_df):,} ({data_ratio*100:.1f}%)\")",
    "        ",
    "        # Store original counts for class weighting (BEFORE balancing)",
    "        self.num_real = len(real_df)",
    "        self.num_fake = len(fake_df)",
    "        ",
    "        # Apply balancing strategy",
    "        if balance_method == 'undersampling':",
    "            min_count = min(len(fake_df), len(real_df))",
    "            print(f\"\\n‚öñÔ∏è  Undersampling to {min_count:,} samples each...\")",
    "            ",
    "            fake_df = fake_df.sample(n=min_count, random_state=seed).reset_index(drop=True)",
    "            real_df = real_df.sample(n=min_count, random_state=seed).reset_index(drop=True)",
    "            ",
    "            print(f\"  Fake images after undersampling: {len(fake_df):,}\")",
    "            print(f\"  Real images after undersampling: {len(real_df):,}\")",
    "            print(f\"  ‚ö†Ô∏è  Data loss: {self.num_fake - len(fake_df):,} fake images dropped\")",
    "            ",
    "        elif balance_method == 'loss_weighting':",
    "            print(f\"\\n‚öñÔ∏è  Using loss weighting (keeping all data)\")",
    "            print(f\"  Real: {len(real_df):,} | Fake: {len(fake_df):,}\")",
    "            print(f\"  ‚úÖ No data loss - imbalance handled by loss weights\")",
    "            ",
    "        else:  # 'none'",
    "            print(f\"\\n‚ö†Ô∏è  No balancing applied\")",
    "            print(f\"  Real: {len(real_df):,} | Fake: {len(fake_df):,}\")",
    "            print(f\"  Imbalance ratio: {len(fake_df)/len(real_df):.2f}x more fakes\")",
    "        ",
    "        # Prepare data list",
    "        self.data = []",
    "        ",
    "        # Add fake images (label = 1)",
    "        for idx, row in fake_df.iterrows():",
    "            metadata = {",
    "                'label_name': 'fake',",
    "                'perturbed_img_id': row.get('perturbed_img_id', ''),",
    "                'real_img_id': row.get('real_img_id', ''),",
    "                'mask_name': row.get('mask_name', ''),",
    "                'domain': row.get('domain', ''),",
    "                'ssim': row.get('ssim', 0.0),",
    "                'lpips_score': row.get('lpips_score', 0.0),",
    "                'mse': row.get('mse', 0.0),",
    "                'model_name': row.get('model_name', ''),",
    "                'dataset': row.get('dataset', ''),",
    "                'area_ratio': row.get('area_ratio', 0.0),",
    "                'sem_magnitude': row.get('sem_magnitude', 0.0)",
    "            }",
    "            self.data.append((row['fake_img_path'], 1, metadata))",
    "        ",
    "        # Add real images (label = 0)",
    "        for idx, row in real_df.iterrows():",
    "            metadata = {",
    "                'label_name': 'real',",
    "                'real_img_id': row.get('real_img_id', ''),",
    "                'parent_dataset': row.get('parent_dataset', '')",
    "            }",
    "            self.data.append((row['real_img_path'], 0, metadata))",
    "        ",
    "        print(f\"\\nüìä Final dataset size: {len(self.data):,} images\")",
    "        print(f\"  Real: {len(real_df):,} ({len(real_df)/len(self.data)*100:.1f}%)\")",
    "        print(f\"  Fake: {len(fake_df):,} ({len(fake_df)/len(self.data)*100:.1f}%)\")",
    "    ",
    "    def __len__(self):",
    "        return len(self.data)",
    "    ",
    "    def __getitem__(self, idx):",
    "        img_path, label, metadata = self.data[idx]",
    "        ",
    "        # Load image",
    "        try:",
    "            image = Image.open(img_path).convert('RGB')",
    "        except Exception as e:",
    "            print(f\"Error loading image {img_path}: {e}\")",
    "            # Return a black image as fallback",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))",
    "        ",
    "        # Apply transforms",
    "        if self.transform:",
    "            image = self.transform(image)",
    "        ",
    "        return image, label, metadata",
    "    ",
    "    def get_class_weights(self):",
    "    \"\"\"",
    "    Calculate class weights using direct ratio (most aggressive).",
    "    Returns: torch.FloatTensor: Weights for [real, fake] classes",
    "    \"\"\"",
    "    imbalance_ratio = self.num_fake / self.num_real",
    "    print(f\"\\n‚öñÔ∏è  Class Imbalance: {imbalance_ratio:.2f}x more fakes\")",
    "    ",
    "    # Calculate weights",
    "    total = self.num_real + self.num_fake",
    "    weight_real_v1 = total / (2.0 * self.num_real)  # Standard",
    "    weight_fake_v1 = total / (2.0 * self.num_fake)",
    "    weight_real_v3 = imbalance_ratio  # Direct ratio (USED)",
    "    weight_fake_v3 = 1.0",
    "    ",
    "    print(f\"  Standard: Real={weight_real_v1:.4f}, Fake={weight_fake_v1:.4f}\")",
    "    print(f\"  Direct:   Real={weight_real_v3:.4f}, Fake={weight_fake_v3:.4f} ‚Üê USING THIS\")",
    "    return torch.FloatTensor([weight_real_v3, weight_fake_v3])",
    "",
    "print(\"RealFakeCSVDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training transforms: WITH augmentation\n",
      "Validation/Test transforms: Basic resize + normalize\n",
      "Image size: 224x224\n",
      "Normalization: ImageNet statistics\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Data Transforms\n",
    "\n",
    "# Training transforms with augmentation\n",
    "if USE_AUGMENTATION:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    print(\"Training transforms: WITH augmentation\")\n",
    "else:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    print(\"Training transforms: WITHOUT augmentation\")\n",
    "\n",
    "# Validation and test transforms (no augmentation)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Validation/Test transforms: Basic resize + normalize\")\n",
    "print(f\"Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"Normalization: ImageNet statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load Datasets and Create DataLoaders",
    "",
    "print(\"=\"*60)",
    "print(\"LOADING DATASETS\")",
    "print(\"=\"*60)",
    "",
    "# Training dataset",
    "print(\"\\nTRAINING SET:\")",
    "train_dataset = RealFakeCSVDataset(",
    "    fake_csv_path=FAKE_TRAIN_CSV,",
    "    real_csv_path=REAL_TRAIN_CSV,",
    "    transform=train_transform,",
    "    balance_method=CLASS_BALANCE_METHOD,",
    "",
    "# Loss Function Type",
    "LOSS_TYPE = 'crossentropy'  # Options: 'crossentropy', 'focal'",
    "FOCAL_ALPHA = 0.25  # Weight for focal loss (only used if LOSS_TYPE=\"focal\")",
    "FOCAL_GAMMA = 2.0   # Focusing parameter (only used if LOSS_TYPE=\"focal\")",
    "    data_ratio=TRAIN_DATA_RATIO,",
    "    seed=RANDOM_SEED",
    ")",
    "",
    "# Validation dataset",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"VALIDATION SET:\")",
    "val_dataset = RealFakeCSVDataset(",
    "    fake_csv_path=FAKE_VAL_CSV,",
    "    real_csv_path=REAL_VAL_CSV,",
    "    transform=eval_transform,",
    "    balance_method=CLASS_BALANCE_METHOD,",
    "",
    "# Loss Function Type",
    "LOSS_TYPE = 'crossentropy'  # Options: 'crossentropy', 'focal'",
    "FOCAL_ALPHA = 0.25  # Weight for focal loss (only used if LOSS_TYPE=\"focal\")",
    "FOCAL_GAMMA = 2.0   # Focusing parameter (only used if LOSS_TYPE=\"focal\")",
    "    data_ratio=VAL_DATA_RATIO,",
    "    seed=RANDOM_SEED",
    ")",
    "",
    "# Test dataset",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"TEST SET:\")",
    "test_dataset = RealFakeCSVDataset(",
    "    fake_csv_path=FAKE_TEST_CSV,",
    "    real_csv_path=REAL_TEST_CSV,",
    "    transform=eval_transform,",
    "    balance_method=CLASS_BALANCE_METHOD,",
    "",
    "# Loss Function Type",
    "LOSS_TYPE = 'crossentropy'  # Options: 'crossentropy', 'focal'",
    "FOCAL_ALPHA = 0.25  # Weight for focal loss (only used if LOSS_TYPE=\"focal\")",
    "FOCAL_GAMMA = 2.0   # Focusing parameter (only used if LOSS_TYPE=\"focal\")",
    "    data_ratio=TEST_DATA_RATIO,",
    "    seed=RANDOM_SEED",
    ")",
    "",
    "# Calculate class weights if using loss weighting",
    "if CLASS_BALANCE_METHOD == 'loss_weighting':",
    "",
    "# Loss Function Type",
    "LOSS_TYPE = 'crossentropy'  # Options: 'crossentropy', 'focal'",
    "FOCAL_ALPHA = 0.25  # Weight for focal loss (only used if LOSS_TYPE=\"focal\")",
    "FOCAL_GAMMA = 2.0   # Focusing parameter (only used if LOSS_TYPE=\"focal\")",
    "    class_weights = train_dataset.get_class_weights()",
    "    print(\"\\n\" + \"=\"*60)",
    "    print(\"CLASS WEIGHTS FOR LOSS FUNCTION\")",
    "    print(\"=\"*60)",
    "    print(f\"Real (class 0) weight: {class_weights[0]:.4f}\")",
    "    print(f\"Fake (class 1) weight: {class_weights[1]:.4f}\")",
    "    print(f\"\\nInterpretation:\")",
    "    print(f\"  - Real images get {class_weights[0]:.2f}x weight in loss\")",
    "    print(f\"  - Fake images get {class_weights[1]:.2f}x weight in loss\")",
    "    print(f\"  - Higher weight for minority class compensates for imbalance\")",
    "    print(\"=\"*60)",
    "else:",
    "    class_weights = None",
    "",
    "# Custom collate function to handle metadata as list instead of dict batching",
    "def custom_collate_fn(batch):",
    "    \"\"\"",
    "    Custom collate function that keeps metadata as a list of dicts",
    "    instead of trying to batch them into a single dict.",
    "    ",
    "    Args:",
    "        batch: List of (image, label, metadata_dict) tuples",
    "    ",
    "    Returns:",
    "        images: Batched tensor of images",
    "        labels: Batched tensor of labels  ",
    "        metadata: List of metadata dicts (NOT batched)",
    "    \"\"\"",
    "    images = torch.stack([item[0] for item in batch])",
    "    labels = torch.tensor([item[1] for item in batch])",
    "    metadata = [item[2] for item in batch]  # Keep as list!",
    "    ",
    "    return images, labels, metadata",
    "",
    "# Create DataLoaders with custom collate function",
    "train_loader = DataLoader(",
    "    train_dataset,",
    "    batch_size=BATCH_SIZE,",
    "    shuffle=True,",
    "    num_workers=NUM_WORKERS,",
    "    pin_memory=True if torch.cuda.is_available() else False,",
    "    collate_fn=custom_collate_fn",
    ")",
    "",
    "val_loader = DataLoader(",
    "    val_dataset,",
    "    batch_size=BATCH_SIZE,",
    "    shuffle=False,",
    "    num_workers=NUM_WORKERS,",
    "    pin_memory=True if torch.cuda.is_available() else False,",
    "    collate_fn=custom_collate_fn",
    ")",
    "",
    "test_loader = DataLoader(",
    "    test_dataset,",
    "    batch_size=BATCH_SIZE,",
    "    shuffle=False,",
    "    num_workers=NUM_WORKERS,",
    "    pin_memory=True if torch.cuda.is_available() else False,",
    "    collate_fn=custom_collate_fn",
    ")",
    "",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"DATALOADER SUMMARY\")",
    "print(\"=\"*60)",
    "print(f\"Train batches: {len(train_loader)}\")",
    "print(f\"Val batches: {len(val_loader)}\")",
    "print(f\"Test batches: {len(test_loader)}\")",
    "print(f\"Batch size: {BATCH_SIZE}\")",
    "print(f\"Num workers: {NUM_WORKERS}\")",
    "print(f\"Custom collate: Enabled (metadata as list)\")",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Model Definition\n\nclass RealFakeModel(nn.Module):\n    \"\"\"\n    ResNet50-based binary classifier for real vs fake image detection.\n    \n    Architecture:\n        - Backbone: ResNet50 pretrained on ImageNet\n        - Classifier: Single FC layer (2048 -> 2 classes)\n    \"\"\"\n    \n    def __init__(self, pretrained=True):\n        super(RealFakeModel, self).__init__()\n        \n        # Load pretrained ResNet50\n        self.resnet = models.resnet50(pretrained=pretrained)\n        \n        # Get number of features from the last layer\n        num_features = self.resnet.fc.in_features\n        \n        # Replace final FC layer for binary classification\n        self.resnet.fc = nn.Linear(num_features, 2)\n    \n    def forward(self, x):\n        return self.resnet(x)\n\n# Initialize model\nmodel = RealFakeModel(pretrained=True).to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"=\"*60)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\"*60)\nprint(f\"Model: ResNet50 (pretrained on ImageNet)\")\nprint(f\"Output classes: 2 (Real, Fake)\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Device: {device}\")\nprint(\"=\"*60)\n\n# Variables to store resume info\nresume_checkpoint_path = None\nstart_epoch = 0\nresume_best_val_acc = 0.0\n\n# Check if we should resume from checkpoint\nif RESUME_FROM_CHECKPOINT:\n    print(\"\\n\" + \"=\"*60)\n    print(\"LOADING CHECKPOINT\")\n    print(\"=\"*60)\n    \n    # Determine which version to load from\n    base_results_dir = f\"eƒüitim_sonu√ßlarƒ±/{NOTEBOOK_NAME}\"\n    \n    if RESUME_VERSION is None:\n        # Auto-detect latest version\n        if os.path.exists(base_results_dir):\n            versions = []\n            for item in os.listdir(os.path.join(base_results_dir, \"models\")):\n                if item.startswith(\"v\") and os.path.isdir(os.path.join(base_results_dir, \"models\", item)):\n                    try:\n                        versions.append(int(item[1:]))\n                    except:\n                        pass\n            \n            if versions:\n                latest_version = max(versions)\n                resume_checkpoint_path = os.path.join(base_results_dir, f\"models/v{latest_version}/best_model.pth\")\n                print(f\"üîç Auto-detected latest version: v{latest_version}\")\n            else:\n                print(\"‚ùå No previous versions found. Starting fresh training.\")\n                RESUME_FROM_CHECKPOINT = False\n        else:\n            print(\"‚ùå No previous training directory found. Starting fresh training.\")\n            RESUME_FROM_CHECKPOINT = False\n    else:\n        # Use specified version\n        resume_checkpoint_path = os.path.join(base_results_dir, f\"models/v{RESUME_VERSION}/best_model.pth\")\n        print(f\"üìÇ Using specified version: v{RESUME_VERSION}\")\n    \n    # Load checkpoint if path exists\n    if RESUME_FROM_CHECKPOINT and resume_checkpoint_path and os.path.exists(resume_checkpoint_path):\n        print(f\"üì• Loading checkpoint from: {resume_checkpoint_path}\")\n        \n        try:\n            checkpoint = torch.load(resume_checkpoint_path, map_location=device)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            \n            start_epoch = checkpoint.get('epoch', 0) + 1\n            resume_best_val_acc = checkpoint.get('val_acc', 0.0)\n            \n            print(f\"‚úÖ Checkpoint loaded successfully!\")\n            print(f\"   Previous epoch: {checkpoint.get('epoch', 0)}\")\n            print(f\"   Previous val_acc: {resume_best_val_acc:.4f}\")\n            print(f\"   Previous val_loss: {checkpoint.get('val_loss', 0.0):.4f}\")\n            print(f\"   Will continue from epoch: {start_epoch + 1}\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error loading checkpoint: {e}\")\n            print(\"   Starting fresh training instead.\")\n            RESUME_FROM_CHECKPOINT = False\n            start_epoch = 0\n            resume_best_val_acc = 0.0\n    \n    elif RESUME_FROM_CHECKPOINT:\n        print(f\"‚ùå Checkpoint not found at: {resume_checkpoint_path}\")\n        print(\"   Starting fresh training instead.\")\n        RESUME_FROM_CHECKPOINT = False\n        start_epoch = 0\n        resume_best_val_acc = 0.0\n    \n    print(\"=\"*60)\n\nif not RESUME_FROM_CHECKPOINT:\n    print(f\"\\nüÜï Starting fresh training from ImageNet pretrained weights\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.5: Focal Loss Implementation (Optional - Better for Class Imbalance)",
    "",
    "class FocalLoss(nn.Module):",
    "    \"\"\"",
    "    Focal Loss for addressing class imbalance.",
    "",
    "    Formula: FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)",
    "",
    "    where:",
    "        - p_t is the model's estimated probability for the correct class",
    "        - alpha_t is the class weight (like in weighted CE)",
    "        - gamma is the focusing parameter (default=2.0)",
    "            - gamma=0: Focal Loss = CrossEntropyLoss",
    "            - gamma>0: Reduces loss for well-classified examples",
    "",
    "    Key Advantage over Weighted CrossEntropy:",
    "        - Weighted CE treats all examples equally regardless of difficulty",
    "        - Focal Loss down-weights easy examples and focuses on hard negatives",
    "        - For severe imbalance, this prevents the model from being",
    "          overwhelmed by easy majority-class examples",
    "",
    "    Reference: https://arxiv.org/abs/1708.02002",
    "    \"\"\"",
    "",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):",
    "        \"\"\"",
    "        Args:",
    "            alpha: Class weights tensor [weight_class0, weight_class1, ...]",
    "                   If None, all classes weighted equally",
    "            gamma: Focusing parameter. Higher = more focus on hard examples",
    "            reduction: 'mean', 'sum', or 'none'",
    "        \"\"\"",
    "        super(FocalLoss, self).__init__()",
    "        self.alpha = alpha",
    "        self.gamma = gamma",
    "        self.reduction = reduction",
    "",
    "    def forward(self, inputs, targets):",
    "        \"\"\"",
    "        Args:",
    "            inputs: Model logits [batch_size, num_classes]",
    "            targets: Ground truth labels [batch_size]",
    "        \"\"\"",
    "        # Get probabilities using softmax",
    "        p = torch.softmax(inputs, dim=1)",
    "",
    "        # Get cross entropy loss (without reduction)",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')",
    "",
    "        # Get probability of the correct class for each sample",
    "        p_t = p.gather(1, targets.view(-1, 1)).squeeze(1)",
    "",
    "        # Calculate focal term: (1 - p_t)^gamma",
    "        focal_term = (1 - p_t) ** self.gamma",
    "",
    "        # Calculate focal loss",
    "        focal_loss = focal_term * ce_loss",
    "",
    "        # Apply class weights if provided",
    "        if self.alpha is not None:",
    "            alpha_t = self.alpha.gather(0, targets)",
    "            focal_loss = alpha_t * focal_loss",
    "",
    "        # Apply reduction",
    "        if self.reduction == 'mean':",
    "            return focal_loss.mean()",
    "        elif self.reduction == 'sum':",
    "            return focal_loss.sum()",
    "        else:",
    "            return focal_loss",
    "",
    "print(\"FocalLoss class defined successfully!\")",
    "print(\"\\nFocal Loss advantages for class imbalance:\")",
    "print(\"  1. Down-weights easy examples (high confidence)\")",
    "print(\"  2. Focuses training on hard examples (low confidence)\")",
    "print(\"  3. Prevents majority class from dominating training\")",
    "print(\"  4. Combines class weighting (alpha) with difficulty weighting (gamma)\")",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Loss, Optimizer, Scheduler, and History",
    "",
    "# Loss function selection: CrossEntropy vs Focal Loss",
    "print(\"=\"*60)",
    "print(\"LOSS FUNCTION CONFIGURATION\")",
    "print(\"=\"*60)",
    "print(f\"Loss type: {LOSS_TYPE}\")",
    "print(f\"Balance method: {CLASS_BALANCE_METHOD}\")",
    "",
    "if LOSS_TYPE == 'focal':",
    "    # Use Focal Loss (better for severe class imbalance)",
    "    if CLASS_BALANCE_METHOD == 'loss_weighting' and class_weights is not None:",
    "        class_weights_gpu = class_weights.to(device)",
    "        criterion = FocalLoss(alpha=class_weights_gpu, gamma=FOCAL_GAMMA)",
    "        print(f\"\\n‚úÖ Using FOCAL LOSS with class weights\")",
    "        print(f\"  Alpha (class weights): Real={class_weights[0]:.4f}, Fake={class_weights[1]:.4f}\")",
    "        print(f\"  Gamma (focusing param): {FOCAL_GAMMA}\")",
    "        print(f\"\\n  How Focal Loss works:\")",
    "        print(f\"    1. Easy examples (high confidence) ‚Üí low loss\")",
    "        print(f\"    2. Hard examples (low confidence) ‚Üí high loss\")",
    "        print(f\"    3. Prevents majority class easy examples from dominating\")",
    "        print(f\"    4. Ideal for severe class imbalance (4-5x or more)\")",
    "    else:",
    "        criterion = FocalLoss(alpha=None, gamma=FOCAL_GAMMA)",
    "        print(f\"\\n‚úÖ Using FOCAL LOSS without class weights\")",
    "        print(f\"  Gamma: {FOCAL_GAMMA}\")",
    "else:",
    "    # Use standard CrossEntropy Loss",
    "    if CLASS_BALANCE_METHOD == 'loss_weighting' and class_weights is not None:",
    "        class_weights_gpu = class_weights.to(device)",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights_gpu)",
    "        print(f\"\\n‚úÖ Using CrossEntropyLoss WITH class weights\")",
    "        print(f\"  Real (class 0) weight: {class_weights[0]:.4f}\")",
    "        print(f\"  Fake (class 1) weight: {class_weights[1]:.4f}\")",
    "        print(f\"\\n  How it works:\")",
    "        print(f\"    - Loss for each sample multiplied by its class weight\")",
    "        print(f\"    - Minority class (real) errors penalized more heavily\")",
    "        print(f\"    - Prevents model bias toward majority class\")",
    "    else:",
    "        criterion = nn.CrossEntropyLoss()",
    "        print(f\"\\n‚úÖ Using CrossEntropyLoss (no class weights)\")",
    "        print(f\"  ‚ö†Ô∏è  No imbalance handling - may bias toward majority class\")",
    "",
    "print(\"=\"*60)",
    "",
    "# Optimizer",
    "optimizer = optim.Adam(",
    "    model.parameters(),",
    "    lr=LEARNING_RATE,",
    "    weight_decay=WEIGHT_DECAY",
    ")",
    "",
    "# Load optimizer state if resuming from checkpoint",
    "if RESUME_FROM_CHECKPOINT and resume_checkpoint_path and os.path.exists(resume_checkpoint_path):",
    "    try:",
    "        checkpoint = torch.load(resume_checkpoint_path, map_location=device)",
    "        if 'optimizer_state_dict' in checkpoint:",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])",
    "            print(f\"\\n‚úÖ Optimizer state loaded from checkpoint\")",
    "    except Exception as e:",
    "        print(f\"\\n‚ö†Ô∏è  Could not load optimizer state: {e}\")",
    "        print(\"   Optimizer will start with fresh state\")",
    "",
    "# Learning rate scheduler",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(",
    "    optimizer,",
    "    mode='max',",
    "    patience=SCHEDULER_PATIENCE,",
    "    factor=SCHEDULER_FACTOR,",
    "    verbose=True",
    ")",
    "",
    "# Training history",
    "history = {",
    "    'train_loss': [],",
    "    'train_acc': [],",
    "    'val_loss': [],",
    "    'val_acc': [],",
    "    'lr': []",
    "}",
    "",
    "print(\"\\n\" + \"=\"*60)",
    "print(\"TRAINING CONFIGURATION\")",
    "print(\"=\"*60)",
    "print(f\"Optimizer: Adam\")",
    "print(f\"  Learning rate: {LEARNING_RATE}\")",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")",
    "print(f\"Scheduler: ReduceLROnPlateau\")",
    "print(f\"  Mode: max (validation accuracy)\")",
    "print(f\"  Patience: {SCHEDULER_PATIENCE} epochs\")",
    "print(f\"  Factor: {SCHEDULER_FACTOR}\")",
    "print(f\"Class balancing: {CLASS_BALANCE_METHOD}\")",
    "if RESUME_FROM_CHECKPOINT:",
    "    print(f\"Resume training: YES (starting from epoch {start_epoch + 1})\")",
    "    print(f\"Previous best val_acc: {resume_best_val_acc:.4f}\")",
    "else:",
    "    print(f\"Resume training: NO (fresh start)\")",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: GPU Monitoring Helper Functions\n",
    "\n",
    "def get_gpu_stats():\n",
    "    \"\"\"\n",
    "    Get GPU memory and utilization statistics.\n",
    "    Returns: (memory_utilization%, gpu_utilization%)\n",
    "    \"\"\"\n",
    "    if not GPU_MONITORING:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    try:\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "        utilization = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle)\n",
    "        \n",
    "        mem_util = (mem_info.used / mem_info.total) * 100\n",
    "        gpu_util = utilization.gpu\n",
    "        \n",
    "        return mem_util, gpu_util\n",
    "    except:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    Format seconds to HH:MM:SS.\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "print(\"GPU monitoring helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: Training Loop with Enhanced Progress Bar\n\nprint(\"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\n# CRITICAL: Verify model and data are on GPU\nprint(f\"\\nüîç GPU VERIFICATION BEFORE TRAINING:\")\nprint(f\"  Model device: {next(model.parameters()).device}\")\nprint(f\"  Expected device: {device}\")\nassert next(model.parameters()).device.type == 'cuda', \"‚ùå Model is NOT on GPU!\"\nprint(f\"  ‚úÖ Model confirmed on GPU\")\n\n# Test a batch to verify GPU is being used\nprint(f\"\\nüîç Testing GPU with first batch...\")\nfirst_batch = next(iter(train_loader))\ntest_images, test_labels, _ = first_batch\ntest_images = test_images.to(device)\ntest_labels = test_labels.to(device)\nprint(f\"  Batch images device: {test_images.device}\")\nprint(f\"  Batch labels device: {test_labels.device}\")\nassert test_images.device.type == 'cuda', \"‚ùå Data is NOT on GPU!\"\nprint(f\"  ‚úÖ Data confirmed on GPU\")\nprint(f\"  ‚úÖ GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\nprint(f\"\\n{'='*60}\")\n\n# Best model tracking - use resume value if resuming\nif RESUME_FROM_CHECKPOINT and resume_best_val_acc > 0:\n    best_val_acc = resume_best_val_acc\n    best_epoch = start_epoch\n    print(f\"\\nüìÇ Resuming from checkpoint:\")\n    print(f\"   Starting best_val_acc: {best_val_acc:.4f}\")\n    print(f\"   Starting from epoch: {start_epoch + 1}\")\nelse:\n    best_val_acc = 0.0\n    best_epoch = 0\n    print(f\"\\nüÜï Starting fresh training\")\n\n# Training start time\ntraining_start_time = time.time()\n\nfor epoch in range(start_epoch, NUM_EPOCHS):\n    epoch_start_time = time.time()\n    \n    # Get current learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    history['lr'].append(current_lr)\n    \n    print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}] - LR: {current_lr:.6f}\")\n    print(\"-\" * 60)\n    \n    # ==================== TRAINING PHASE ====================\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    \n    # Create progress bar for training\n    train_pbar = tqdm(train_loader, desc=f\"Training\", unit=\"batch\")\n    \n    for batch_idx, (images, labels, metadata) in enumerate(train_pbar):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate accuracy\n        _, predicted = torch.max(outputs.data, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n        train_loss += loss.item()\n        \n        # Calculate running metrics\n        avg_loss = train_loss / (batch_idx + 1)\n        current_acc = train_correct / train_total\n        \n        # Get GPU stats\n        mem_util, gpu_util = get_gpu_stats()\n        \n        # Estimate time remaining\n        batches_done = batch_idx + 1\n        batches_left = len(train_loader) - batches_done\n        batch_time = (time.time() - epoch_start_time) / batches_done\n        eta_seconds = batch_time * batches_left\n        \n        # Update progress bar\n        train_pbar.set_postfix({\n            'Loss': f'{avg_loss:.4f}',\n            'Acc': f'{current_acc:.3f}',\n            'LR': f'{current_lr:.6f}',\n            'GPU_Mem': f'{mem_util:.0f}%',\n            'GPU_Use': f'{gpu_util:.0f}%',\n            'ETA': format_time(eta_seconds)\n        })\n    \n    # Calculate epoch training metrics\n    epoch_train_loss = train_loss / len(train_loader)\n    epoch_train_acc = train_correct / train_total\n    \n    history['train_loss'].append(epoch_train_loss)\n    history['train_acc'].append(epoch_train_acc)\n    \n    # ==================== VALIDATION PHASE ====================\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    \n    val_pbar = tqdm(val_loader, desc=f\"Validation\", unit=\"batch\")\n    \n    with torch.no_grad():\n        for batch_idx, (images, labels, metadata) in enumerate(val_pbar):\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            # Calculate accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n            val_loss += loss.item()\n            \n            # Calculate running metrics\n            avg_loss = val_loss / (batch_idx + 1)\n            current_acc = val_correct / val_total\n            \n            # Update progress bar\n            val_pbar.set_postfix({\n                'Loss': f'{avg_loss:.4f}',\n                'Acc': f'{current_acc:.3f}'\n            })\n    \n    # Calculate epoch validation metrics\n    epoch_val_loss = val_loss / len(val_loader)\n    epoch_val_acc = val_correct / val_total\n    \n    history['val_loss'].append(epoch_val_loss)\n    history['val_acc'].append(epoch_val_acc)\n    \n    # Print epoch summary\n    epoch_time = time.time() - epoch_start_time\n    print(f\"\\nEpoch [{epoch+1}/{NUM_EPOCHS}] Summary:\")\n    print(f\"  Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}\")\n    print(f\"  Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n    print(f\"  Time: {format_time(epoch_time)}\")\n    print(f\"  GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n    \n    # Save best model\n    if epoch_val_acc > best_val_acc:\n        best_val_acc = epoch_val_acc\n        best_epoch = epoch + 1\n        \n        # Save best model checkpoint\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_acc': epoch_val_acc,\n            'val_loss': epoch_val_loss\n        }, os.path.join(models_dir, 'best_model.pth'))\n        \n        print(f\"  ‚úÖ NEW BEST MODEL SAVED! Val Acc: {epoch_val_acc:.4f}\")\n    \n    # Always save epoch checkpoint\n    torch.save(\n        model.state_dict(),\n        os.path.join(models_dir, f'model_epoch_{epoch+1}.pth')\n    )\n    \n    # Update learning rate scheduler\n    scheduler.step(epoch_val_acc)\n    \n    print(\"-\" * 60)\n\n# Training complete\ntotal_training_time = time.time() - training_start_time\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"Total training time: {format_time(total_training_time)}\")\nprint(f\"Best validation accuracy: {best_val_acc:.4f} (Epoch {best_epoch})\")\nprint(f\"Best model saved at: {os.path.join(models_dir, 'best_model.pth')}\")\nprint(f\"Final GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12: Save Training History\n\n# Create history DataFrame using actual number of epochs trained\nactual_epochs_trained = len(history['train_loss'])\n\n# Calculate starting epoch number for DataFrame\nif RESUME_FROM_CHECKPOINT:\n    epoch_start_num = start_epoch + 1\nelse:\n    epoch_start_num = 1\n\nhistory_df = pd.DataFrame({\n    'epoch': list(range(epoch_start_num, epoch_start_num + actual_epochs_trained)),\n    'train_loss': history['train_loss'],\n    'train_acc': history['train_acc'],\n    'val_loss': history['val_loss'],\n    'val_acc': history['val_acc'],\n    'learning_rate': history['lr']\n})\n\n# Save to CSV\nhistory_path = os.path.join(data_dir, 'training_history.csv')\nhistory_df.to_csv(history_path, index=False)\n\nprint(f\"Training history saved to: {history_path}\")\nprint(f\"\\nHistory summary:\")\nprint(f\"Epochs trained in this session: {actual_epochs_trained}\")\nif RESUME_FROM_CHECKPOINT:\n    print(f\"Resumed from epoch: {start_epoch + 1}\")\nprint(f\"\\nHistory data:\")\nprint(history_df)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 13: Plot Training Curves\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Use actual epochs from history\nepochs = list(range(epoch_start_num, epoch_start_num + actual_epochs_trained))\n\n# Plot 1: Loss curves\naxes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\naxes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\naxes[0].axvline(x=best_epoch, color='red', linestyle='--', linewidth=2, alpha=0.7, \n                label=f'Best Epoch ({best_epoch})')\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Loss', fontsize=12)\naxes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=10)\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Accuracy curves\naxes[1].plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\naxes[1].plot(epochs, history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\naxes[1].axvline(x=best_epoch, color='red', linestyle='--', linewidth=2, alpha=0.7, \n                label=f'Best Epoch ({best_epoch})')\naxes[1].axhline(y=best_val_acc, color='green', linestyle=':', linewidth=1.5, alpha=0.5)\naxes[1].text(best_epoch + 0.5, best_val_acc, f'Best: {best_val_acc:.4f}', \n             fontsize=10, color='green', fontweight='bold')\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Accuracy', fontsize=12)\naxes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=10)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\n\n# Save plot\nplot_path = os.path.join(viz_dir, 'training_curves.png')\nplt.savefig(plot_path, dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"Training curves saved to: {plot_path}\")\nif RESUME_FROM_CHECKPOINT:\n    print(f\"Note: Plot shows epochs {epoch_start_num} to {epoch_start_num + actual_epochs_trained - 1} (resumed training)\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 13.5: Comprehensive Training Visualization\n\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Loss Curves (Top Left)\nax1 = fig.add_subplot(gs[0, 0])\nepochs_plot = list(range(epoch_start_num, epoch_start_num + actual_epochs_trained))\nax1.plot(epochs_plot, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)\nax1.plot(epochs_plot, history['val_loss'], 'r-s', label='Val Loss', linewidth=2, markersize=6)\nax1.axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.5, label=f'Best Epoch ({best_epoch})')\nax1.set_xlabel('Epoch', fontsize=11, fontweight='bold')\nax1.set_ylabel('Loss', fontsize=11, fontweight='bold')\nax1.set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\nax1.legend(fontsize=9)\nax1.grid(True, alpha=0.3)\n\n# 2. Accuracy Curves (Top Middle)\nax2 = fig.add_subplot(gs[0, 1])\nax2.plot(epochs_plot, history['train_acc'], 'b-o', label='Train Acc', linewidth=2, markersize=6)\nax2.plot(epochs_plot, history['val_acc'], 'r-s', label='Val Acc', linewidth=2, markersize=6)\nax2.axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.5, label=f'Best Epoch ({best_epoch})')\nax2.axhline(y=best_val_acc, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\nax2.set_xlabel('Epoch', fontsize=11, fontweight='bold')\nax2.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\nax2.set_title('Training & Validation Accuracy', fontsize=13, fontweight='bold')\nax2.legend(fontsize=9)\nax2.grid(True, alpha=0.3)\n\n# 3. Learning Rate Schedule (Top Right)\nax3 = fig.add_subplot(gs[0, 2])\nax3.plot(epochs_plot, history['lr'], 'g-o', linewidth=2, markersize=6)\nax3.set_xlabel('Epoch', fontsize=11, fontweight='bold')\nax3.set_ylabel('Learning Rate', fontsize=11, fontweight='bold')\nax3.set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\nax3.set_yscale('log')\nax3.grid(True, alpha=0.3)\n\n# 4. Loss Gap (Train - Val) (Middle Left)\nax4 = fig.add_subplot(gs[1, 0])\nloss_gap = [t - v for t, v in zip(history['train_loss'], history['val_loss'])]\ncolors = ['green' if gap < 0 else 'red' for gap in loss_gap]\nax4.bar(epochs_plot, loss_gap, color=colors, alpha=0.6, edgecolor='black')\nax4.axhline(y=0, color='black', linestyle='-', linewidth=1)\nax4.set_xlabel('Epoch', fontsize=11, fontweight='bold')\nax4.set_ylabel('Train Loss - Val Loss', fontsize=11, fontweight='bold')\nax4.set_title('Overfitting Monitor (Loss Gap)', fontsize=13, fontweight='bold')\nax4.grid(True, alpha=0.3, axis='y')\n\n# 5. Accuracy Gap (Val - Train) (Middle Middle)\nax5 = fig.add_subplot(gs[1, 1])\nacc_gap = [v - t for t, v in zip(history['train_acc'], history['val_acc'])]\ncolors = ['green' if gap > 0 else 'red' for gap in acc_gap]\nax5.bar(epochs_plot, acc_gap, color=colors, alpha=0.6, edgecolor='black')\nax5.axhline(y=0, color='black', linestyle='-', linewidth=1)\nax5.set_xlabel('Epoch', fontsize=11, fontweight='bold')\nax5.set_ylabel('Val Acc - Train Acc', fontsize=11, fontweight='bold')\nax5.set_title('Generalization Gap', fontsize=13, fontweight='bold')\nax5.grid(True, alpha=0.3, axis='y')\n\n# 6. Epoch Improvement (Middle Right)\nax6 = fig.add_subplot(gs[1, 2])\nval_improvements = [0] + [history['val_acc'][i] - history['val_acc'][i-1] for i in range(1, len(history['val_acc']))]\ncolors = ['green' if imp > 0 else 'red' for imp in val_improvements]\nax6.bar(epochs_plot, val_improvements, color=colors, alpha=0.6, edgecolor='black')\nax6.axhline(y=0, color='black', linestyle='-', linewidth=1)\nax6.set_xlabel('Epoch', fontsize=11, fontweight='bold')\nax6.set_ylabel('Validation Acc Improvement', fontsize=11, fontweight='bold')\nax6.set_title('Per-Epoch Validation Improvement', fontsize=13, fontweight='bold')\nax6.grid(True, alpha=0.3, axis='y')\n\n# 7. Training Progress Summary Table (Bottom Left)\nax7 = fig.add_subplot(gs[2, :2])\nax7.axis('off')\n\nsummary_data = []\nfor i, epoch in enumerate(epochs_plot):\n    summary_data.append([\n        epoch,\n        f\"{history['train_loss'][i]:.4f}\",\n        f\"{history['train_acc'][i]:.4f}\",\n        f\"{history['val_loss'][i]:.4f}\",\n        f\"{history['val_acc'][i]:.4f}\",\n        f\"{history['lr'][i]:.2e}\",\n        \"‚úÖ BEST\" if epoch == best_epoch else \"\"\n    ])\n\ntable = ax7.table(cellText=summary_data,\n                  colLabels=['Epoch', 'Train Loss', 'Train Acc', 'Val Loss', 'Val Acc', 'LR', 'Best'],\n                  cellLoc='center',\n                  loc='center',\n                  bbox=[0, 0, 1, 1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(9)\ntable.scale(1, 2)\n\n# Style header\nfor i in range(7):\n    table[(0, i)].set_facecolor('#4CAF50')\n    table[(0, i)].set_text_props(weight='bold', color='white')\n\n# Highlight best epoch row\nfor i, epoch in enumerate(epochs_plot):\n    if epoch == best_epoch:\n        for j in range(7):\n            table[(i+1, j)].set_facecolor('#FFE082')\n\nax7.set_title('Detailed Training History', fontsize=13, fontweight='bold', pad=20)\n\n# 8. Best Metrics Summary (Bottom Right)\nax8 = fig.add_subplot(gs[2, 2])\nax8.axis('off')\n\nbest_metrics = [\n    ['Metric', 'Value'],\n    ['', ''],\n    ['Best Epoch', f'{best_epoch}'],\n    ['Best Val Acc', f'{best_val_acc:.4f}'],\n    ['Final Train Acc', f'{history[\"train_acc\"][-1]:.4f}'],\n    ['Final Val Acc', f'{history[\"val_acc\"][-1]:.4f}'],\n    ['Final Train Loss', f'{history[\"train_loss\"][-1]:.4f}'],\n    ['Final Val Loss', f'{history[\"val_loss\"][-1]:.4f}'],\n    ['Final LR', f'{history[\"lr\"][-1]:.2e}'],\n    ['Total Epochs', f'{actual_epochs_trained}']\n]\n\ntable2 = ax8.table(cellText=best_metrics,\n                   cellLoc='left',\n                   loc='center',\n                   bbox=[0, 0, 1, 1])\ntable2.auto_set_font_size(False)\ntable2.set_fontsize(10)\ntable2.scale(1, 2.5)\n\n# Style header\ntable2[(0, 0)].set_facecolor('#2196F3')\ntable2[(0, 0)].set_text_props(weight='bold', color='white')\ntable2[(0, 1)].set_facecolor('#2196F3')\ntable2[(0, 1)].set_text_props(weight='bold', color='white')\n\n# Bold metric names\nfor i in range(2, len(best_metrics)):\n    table2[(i, 0)].set_text_props(weight='bold')\n\nax8.set_title('Best Model Metrics', fontsize=13, fontweight='bold', pad=20)\n\nplt.suptitle(f'Comprehensive Training Analysis - Version {version}', \n             fontsize=16, fontweight='bold', y=0.995)\n\n# Save\ncomprehensive_path = os.path.join(viz_dir, 'comprehensive_training_analysis.png')\nplt.savefig(comprehensive_path, dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"Comprehensive training analysis saved to: {comprehensive_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Load Best Model and Run Test Inference\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "best_model_path = os.path.join(models_dir, 'best_model.pth')\n",
    "print(f\"\\nLoading best model from epoch {best_epoch}\")\n",
    "print(f\"Path: {best_model_path}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"\\nRunning test inference...\")\n",
    "\n",
    "# Storage for predictions and metadata\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_confidences = []\n",
    "all_metadata = []\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    test_pbar = tqdm(test_loader, desc=\"Test Inference\", unit=\"batch\")\n",
    "    \n",
    "    for images, labels, metadata_batch in test_pbar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        confidences, predictions = torch.max(probs, dim=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_confidences.extend(confidences.cpu().numpy())\n",
    "        all_metadata.extend(metadata_batch)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_confidences = np.array(all_confidences)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEST RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Total test samples: {len(all_labels)}\")\n",
    "print(f\"Correct predictions: {(all_predictions == all_labels).sum()}\")\n",
    "print(f\"Incorrect predictions: {(all_predictions != all_labels).sum()}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 14.5: Train/Val/Test Comprehensive Comparison\n\n# First, we need to get train and val predictions for comparison\nprint(\"Running inference on Train and Val sets for comprehensive comparison...\")\n\n# Train set inference\nmodel.eval()\ntrain_preds = []\ntrain_labels_list = []\ntrain_confs = []\n\nwith torch.no_grad():\n    for images, labels, _ in tqdm(train_loader, desc=\"Train Inference\"):\n        images = images.to(device)\n        outputs = model(images)\n        probs = torch.softmax(outputs, dim=1)\n        confidences, predictions = torch.max(probs, dim=1)\n        \n        train_preds.extend(predictions.cpu().numpy())\n        train_labels_list.extend(labels.numpy())\n        train_confs.extend(confidences.cpu().numpy())\n\ntrain_preds = np.array(train_preds)\ntrain_labels_list = np.array(train_labels_list)\ntrain_confs = np.array(train_confs)\n\n# Val set inference  \nval_preds = []\nval_labels_list = []\nval_confs = []\n\nwith torch.no_grad():\n    for images, labels, _ in tqdm(val_loader, desc=\"Val Inference\"):\n        images = images.to(device)\n        outputs = model(images)\n        probs = torch.softmax(outputs, dim=1)\n        confidences, predictions = torch.max(probs, dim=1)\n        \n        val_preds.extend(predictions.cpu().numpy())\n        val_labels_list.extend(labels.numpy())\n        val_confs.extend(confidences.cpu().numpy())\n\nval_preds = np.array(val_preds)\nval_labels_list = np.array(val_labels_list)\nval_confs = np.array(val_confs)\n\n# Calculate metrics\ntrain_acc = accuracy_score(train_labels_list, train_preds)\nval_acc = accuracy_score(val_labels_list, val_preds)\n\n# Real/Fake breakdown\ntrain_real_acc = (train_preds[train_labels_list == 0] == 0).mean()\ntrain_fake_acc = (train_preds[train_labels_list == 1] == 1).mean()\n\nval_real_acc = (val_preds[val_labels_list == 0] == 0).mean()\nval_fake_acc = (val_preds[val_labels_list == 1] == 1).mean()\n\ntest_real_acc = (all_predictions[all_labels == 0] == 0).mean()\ntest_fake_acc = (all_predictions[all_labels == 1] == 1).mean()\n\n# Create comprehensive comparison plot\nfig = plt.figure(figsize=(20, 10))\ngs = fig.add_gridspec(2, 4, hspace=0.3, wspace=0.3)\n\n# 1. Overall Accuracy Comparison (Top Left)\nax1 = fig.add_subplot(gs[0, 0])\nsets = ['Train', 'Val', 'Test']\naccuracies = [train_acc, val_acc, test_accuracy]\ncolors = ['#2196F3', '#FF9800', '#4CAF50']\nbars = ax1.bar(sets, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n\nfor i, (bar, acc) in enumerate(zip(bars, accuracies)):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n             f'{acc:.4f}',\n             ha='center', va='bottom', fontsize=12, fontweight='bold')\n\nax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\nax1.set_title('Overall Accuracy: Train vs Val vs Test', fontsize=13, fontweight='bold')\nax1.set_ylim([0, 1.1])\nax1.grid(True, alpha=0.3, axis='y')\n\n# 2. Real vs Fake Accuracy by Set (Top Middle-Left)\nax2 = fig.add_subplot(gs[0, 1])\nx = np.arange(3)\nwidth = 0.35\n\nreal_accs = [train_real_acc, val_real_acc, test_real_acc]\nfake_accs = [train_fake_acc, val_fake_acc, test_fake_acc]\n\nbars1 = ax2.bar(x - width/2, real_accs, width, label='Real', color='#03A9F4', alpha=0.8, edgecolor='black')\nbars2 = ax2.bar(x + width/2, fake_accs, width, label='Fake', color='#FF5722', alpha=0.8, edgecolor='black')\n\n# Add value labels\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                 f'{height:.3f}',\n                 ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\nax2.set_title('Real vs Fake Detection Accuracy', fontsize=13, fontweight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(sets)\nax2.legend(fontsize=10)\nax2.set_ylim([0, 1.15])\nax2.grid(True, alpha=0.3, axis='y')\n\n# 3. Sample Count Comparison (Top Middle-Right)\nax3 = fig.add_subplot(gs[0, 2])\nsample_counts = [len(train_labels_list), len(val_labels_list), len(all_labels)]\nbars = ax3.bar(sets, sample_counts, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n\nfor bar, count in zip(bars, sample_counts):\n    height = bar.get_height()\n    ax3.text(bar.get_x() + bar.get_width()/2., height + 100,\n             f'{count:,}',\n             ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nax3.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')\nax3.set_title('Dataset Size Comparison', fontsize=13, fontweight='bold')\nax3.grid(True, alpha=0.3, axis='y')\n\n# 4. Confidence Distribution Comparison (Top Right)\nax4 = fig.add_subplot(gs[0, 3])\nax4.hist(train_confs, bins=30, alpha=0.5, label='Train', color='#2196F3', edgecolor='black')\nax4.hist(val_confs, bins=30, alpha=0.5, label='Val', color='#FF9800', edgecolor='black')\nax4.hist(all_confidences, bins=30, alpha=0.5, label='Test', color='#4CAF50', edgecolor='black')\n\nax4.axvline(train_confs.mean(), color='#2196F3', linestyle='--', linewidth=2, label=f'Train Œº={train_confs.mean():.3f}')\nax4.axvline(val_confs.mean(), color='#FF9800', linestyle='--', linewidth=2, label=f'Val Œº={val_confs.mean():.3f}')\nax4.axvline(all_confidences.mean(), color='#4CAF50', linestyle='--', linewidth=2, label=f'Test Œº={all_confidences.mean():.3f}')\n\nax4.set_xlabel('Confidence Score', fontsize=12, fontweight='bold')\nax4.set_ylabel('Frequency', fontsize=12, fontweight='bold')\nax4.set_title('Confidence Distribution Comparison', fontsize=13, fontweight='bold')\nax4.legend(fontsize=8)\nax4.grid(True, alpha=0.3)\n\n# 5. Confusion Matrices Side by Side (Bottom)\n# Train CM\nax5 = fig.add_subplot(gs[1, 0])\ntrain_cm = confusion_matrix(train_labels_list, train_preds)\nsns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=ax5,\n            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n            cbar_kws={'label': 'Count'}, square=True)\nax5.set_title(f'Train Confusion Matrix\\n(Acc: {train_acc:.4f})', fontsize=12, fontweight='bold')\nax5.set_xlabel('Predicted', fontsize=11, fontweight='bold')\nax5.set_ylabel('True', fontsize=11, fontweight='bold')\n\n# Val CM\nax6 = fig.add_subplot(gs[1, 1])\nval_cm = confusion_matrix(val_labels_list, val_preds)\nsns.heatmap(val_cm, annot=True, fmt='d', cmap='Oranges', ax=ax6,\n            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n            cbar_kws={'label': 'Count'}, square=True)\nax6.set_title(f'Val Confusion Matrix\\n(Acc: {val_acc:.4f})', fontsize=12, fontweight='bold')\nax6.set_xlabel('Predicted', fontsize=11, fontweight='bold')\nax6.set_ylabel('True', fontsize=11, fontweight='bold')\n\n# Test CM\nax7 = fig.add_subplot(gs[1, 2])\ntest_cm = confusion_matrix(all_labels, all_predictions)\nsns.heatmap(test_cm, annot=True, fmt='d', cmap='Greens', ax=ax7,\n            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n            cbar_kws={'label': 'Count'}, square=True)\nax7.set_title(f'Test Confusion Matrix\\n(Acc: {test_accuracy:.4f})', fontsize=12, fontweight='bold')\nax7.set_xlabel('Predicted', fontsize=11, fontweight='bold')\nax7.set_ylabel('True', fontsize=11, fontweight='bold')\n\n# 6. Summary Statistics Table (Bottom Right)\nax8 = fig.add_subplot(gs[1, 3])\nax8.axis('off')\n\nsummary_stats = [\n    ['Metric', 'Train', 'Val', 'Test'],\n    ['', '', '', ''],\n    ['Overall Acc', f'{train_acc:.4f}', f'{val_acc:.4f}', f'{test_accuracy:.4f}'],\n    ['Real Acc', f'{train_real_acc:.4f}', f'{val_real_acc:.4f}', f'{test_real_acc:.4f}'],\n    ['Fake Acc', f'{train_fake_acc:.4f}', f'{val_fake_acc:.4f}', f'{test_fake_acc:.4f}'],\n    ['Samples', f'{len(train_labels_list):,}', f'{len(val_labels_list):,}', f'{len(all_labels):,}'],\n    ['Mean Conf', f'{train_confs.mean():.4f}', f'{val_confs.mean():.4f}', f'{all_confidences.mean():.4f}'],\n    ['Std Conf', f'{train_confs.std():.4f}', f'{val_confs.std():.4f}', f'{all_confidences.std():.4f}']\n]\n\ntable = ax8.table(cellText=summary_stats,\n                  cellLoc='center',\n                  loc='center',\n                  bbox=[0, 0, 1, 1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1, 2.5)\n\n# Style header row\nfor i in range(4):\n    table[(0, i)].set_facecolor('#455A64')\n    table[(0, i)].set_text_props(weight='bold', color='white')\n\n# Style metric names\nfor i in range(2, len(summary_stats)):\n    table[(i, 0)].set_facecolor('#ECEFF1')\n    table[(i, 0)].set_text_props(weight='bold')\n\n# Highlight best values in each row\nfor i in range(2, len(summary_stats)):\n    values = [float(summary_stats[i][j].replace(',', '')) for j in range(1, 4)]\n    best_idx = values.index(max(values))\n    table[(i, best_idx + 1)].set_facecolor('#C8E6C9')\n    table[(i, best_idx + 1)].set_text_props(weight='bold')\n\nax8.set_title('Performance Summary', fontsize=13, fontweight='bold', pad=20)\n\nplt.suptitle(f'Train / Validation / Test Comprehensive Comparison - Version {version}', \n             fontsize=16, fontweight='bold', y=0.995)\n\n# Save\ncomparison_path = os.path.join(viz_dir, 'train_val_test_comparison.png')\nplt.savefig(comparison_path, dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nTrain/Val/Test comparison saved to: {comparison_path}\")\nprint(f\"\\n{'='*60}\")\nprint(\"COMPREHENSIVE PERFORMANCE SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"Train Accuracy: {train_acc:.4f} (Real: {train_real_acc:.4f}, Fake: {train_fake_acc:.4f})\")\nprint(f\"Val Accuracy:   {val_acc:.4f} (Real: {val_real_acc:.4f}, Fake: {val_fake_acc:.4f})\")\nprint(f\"Test Accuracy:  {test_accuracy:.4f} (Real: {test_real_acc:.4f}, Fake: {test_fake_acc:.4f})\")\nprint(f\"{'='*60}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Build Comprehensive Results DataFrame\n",
    "\n",
    "print(\"Building comprehensive results DataFrame...\")\n",
    "\n",
    "# Create base results\n",
    "results_data = {\n",
    "    'true_label': ['real' if label == 0 else 'fake' for label in all_labels],\n",
    "    'predicted_label': ['real' if pred == 0 else 'fake' for pred in all_predictions],\n",
    "    'confidence': all_confidences,\n",
    "    'correct': (all_predictions == all_labels).astype(int)\n",
    "}\n",
    "\n",
    "# Add metadata fields\n",
    "# We need to extract metadata from the list of dicts\n",
    "for key in ['label_name', 'perturbed_img_id', 'real_img_id', 'mask_name', 'domain', \n",
    "            'ssim', 'lpips_score', 'mse', 'model_name', 'dataset', 'area_ratio', \n",
    "            'sem_magnitude', 'parent_dataset']:\n",
    "    results_data[key] = [metadata.get(key, None) for metadata in all_metadata]\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(f\"Results DataFrame shape: {results_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(results_df['true_label'].value_counts())\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(results_df['predicted_label'].value_counts())\n",
    "\n",
    "print(f\"\\nAccuracy by true label:\")\n",
    "print(results_df.groupby('true_label')['correct'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Export Misclassified Fake Images\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MISCLASSIFIED FAKE IMAGES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter for misclassified fake images (true=fake, predicted=real)\n",
    "misclassified_fake = results_df[\n",
    "    (results_df['true_label'] == 'fake') & \n",
    "    (results_df['predicted_label'] == 'real')\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nTotal fake images: {(results_df['true_label'] == 'fake').sum()}\")\n",
    "print(f\"Correctly classified fake: {((results_df['true_label'] == 'fake') & (results_df['correct'] == 1)).sum()}\")\n",
    "print(f\"Misclassified fake (predicted as real): {len(misclassified_fake)}\")\n",
    "print(f\"Fake detection accuracy: {((results_df['true_label'] == 'fake') & (results_df['correct'] == 1)).sum() / (results_df['true_label'] == 'fake').sum():.4f}\")\n",
    "\n",
    "# Select relevant columns for export\n",
    "export_columns = ['perturbed_img_id', 'real_img_id', 'confidence', 'mask_name', 'domain', \n",
    "                  'ssim', 'lpips_score', 'mse', 'model_name', 'dataset', \n",
    "                  'area_ratio', 'sem_magnitude']\n",
    "\n",
    "misclassified_export = misclassified_fake[export_columns].copy()\n",
    "\n",
    "# Save to CSV\n",
    "misclassified_path = os.path.join(data_dir, 'misclassified_fake_images.csv')\n",
    "misclassified_export.to_csv(misclassified_path, index=False)\n",
    "\n",
    "print(f\"\\nMisclassified fake images saved to: {misclassified_path}\")\n",
    "print(f\"Total rows exported: {len(misclassified_export)}\")\n",
    "\n",
    "# Show sample of misclassified images\n",
    "if len(misclassified_fake) > 0:\n",
    "    print(f\"\\nSample of misclassified fake images:\")\n",
    "    print(misclassified_export.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cell 17.5: Comprehensive Error Analysis Visualization\n\nfig = plt.figure(figsize=(20, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n\n# 1. Error Distribution by Class (Top Left)\nax1 = fig.add_subplot(gs[0, 0])\nerror_data = results_df.groupby('true_label')['correct'].agg(['count', 'sum'])\nerror_data['errors'] = error_data['count'] - error_data['sum']\nerror_data['error_rate'] = error_data['errors'] / error_data['count']\n\nx = np.arange(len(error_data))\nwidth = 0.35\n\nbars1 = ax1.bar(x - width/2, error_data['sum'], width, label='Correct', color='#4CAF50', alpha=0.8, edgecolor='black')\nbars2 = ax1.bar(x + width/2, error_data['errors'], width, label='Errors', color='#F44336', alpha=0.8, edgecolor='black')\n\n# Add values on bars\nfor bar in bars1:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n             f'{int(height):,}',\n             ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nfor bar, error_rate in zip(bars2, error_data['error_rate']):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n             f'{int(height):,}\\n({error_rate*100:.1f}%)',\n             ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nax1.set_ylabel('Count', fontsize=12, fontweight='bold')\nax1.set_title('Error Distribution by Class', fontsize=13, fontweight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(error_data.index)\nax1.legend(fontsize=10)\nax1.grid(True, alpha=0.3, axis='y')\n\n# 2. Confidence vs Correctness (Top Middle)\nax2 = fig.add_subplot(gs[0, 1])\ncorrect_mask = results_df['correct'] == 1\nincorrect_mask = results_df['correct'] == 0\n\nax2.scatter(results_df[correct_mask]['confidence'], [1]*correct_mask.sum(), \n           alpha=0.3, s=10, color='green', label=f'Correct ({correct_mask.sum():,})')\nax2.scatter(results_df[incorrect_mask]['confidence'], [0]*incorrect_mask.sum(), \n           alpha=0.3, s=10, color='red', label=f'Incorrect ({incorrect_mask.sum():,})')\n\nax2.axvline(results_df[correct_mask]['confidence'].mean(), color='green', \n           linestyle='--', linewidth=2, alpha=0.7, label=f'Correct Œº={results_df[correct_mask][\"confidence\"].mean():.3f}')\nax2.axvline(results_df[incorrect_mask]['confidence'].mean(), color='red', \n           linestyle='--', linewidth=2, alpha=0.7, label=f'Incorrect Œº={results_df[incorrect_mask][\"confidence\"].mean():.3f}')\n\nax2.set_xlabel('Prediction Confidence', fontsize=12, fontweight='bold')\nax2.set_ylabel('Correctness', fontsize=12, fontweight='bold')\nax2.set_yticks([0, 1])\nax2.set_yticklabels(['Incorrect', 'Correct'])\nax2.set_title('Confidence vs Correctness', fontsize=13, fontweight='bold')\nax2.legend(fontsize=8, loc='center left')\nax2.grid(True, alpha=0.3)\n\n# 3. Error Rate by Confidence Bins (Top Right)\nax3 = fig.add_subplot(gs[0, 2])\nconf_bins = pd.cut(results_df['confidence'], bins=10)\nerror_by_conf = results_df.groupby(conf_bins)['correct'].agg(['count', 'sum'])\nerror_by_conf['error_rate'] = 1 - (error_by_conf['sum'] / error_by_conf['count'])\nerror_by_conf = error_by_conf.dropna()\n\nbin_centers = [interval.mid for interval in error_by_conf.index]\nax3.plot(bin_centers, error_by_conf['error_rate'] * 100, 'o-', linewidth=2, \n        markersize=8, color='#E91E63', markeredgecolor='black', markeredgewidth=1)\nax3.fill_between(bin_centers, 0, error_by_conf['error_rate'] * 100, alpha=0.3, color='#E91E63')\n\nax3.set_xlabel('Confidence Score', fontsize=12, fontweight='bold')\nax3.set_ylabel('Error Rate (%)', fontsize=12, fontweight='bold')\nax3.set_title('Error Rate by Confidence Level', fontsize=13, fontweight='bold')\nax3.grid(True, alpha=0.3)\n\n# 4. Domain Error Analysis (Middle Left)\nax4 = fig.add_subplot(gs[1, 0])\ndomain_errors = fake_results.groupby('domain')['correct'].agg(['count', 'sum'])\ndomain_errors['errors'] = domain_errors['count'] - domain_errors['sum']\ndomain_errors['error_rate'] = domain_errors['errors'] / domain_errors['count']\ndomain_errors = domain_errors.sort_values('error_rate', ascending=False)\n\ncolors_domain = ['#F44336' if rate > 0.3 else '#FF9800' if rate > 0.15 else '#4CAF50' \n                for rate in domain_errors['error_rate']]\n\nbars = ax4.barh(range(len(domain_errors)), domain_errors['error_rate'] * 100, \n               color=colors_domain, alpha=0.7, edgecolor='black')\n\nfor i, (idx, row) in enumerate(domain_errors.iterrows()):\n    ax4.text(row['error_rate'] * 100 + 1, i, \n            f\"{row['error_rate']*100:.1f}% ({int(row['errors'])}/{int(row['count'])})\",\n            va='center', fontsize=9, fontweight='bold')\n\nax4.set_yticks(range(len(domain_errors)))\nax4.set_yticklabels(domain_errors.index)\nax4.set_xlabel('Error Rate (%)', fontsize=12, fontweight='bold')\nax4.set_title('Error Rate by Domain (Fake Images)', fontsize=13, fontweight='bold')\nax4.grid(True, alpha=0.3, axis='x')\n\n# 5. Generative Model Error Analysis (Middle Middle)\nax5 = fig.add_subplot(gs[1, 1])\nmodel_errors = fake_results.groupby('dataset')['correct'].agg(['count', 'sum'])\nmodel_errors['errors'] = model_errors['count'] - model_errors['sum']\nmodel_errors['error_rate'] = model_errors['errors'] / model_errors['count']\nmodel_errors = model_errors.sort_values('error_rate', ascending=False)\n\ncolors_model = ['#F44336' if rate > 0.3 else '#FF9800' if rate > 0.15 else '#4CAF50' \n               for rate in model_errors['error_rate']]\n\nbars = ax5.barh(range(len(model_errors)), model_errors['error_rate'] * 100, \n               color=colors_model, alpha=0.7, edgecolor='black')\n\nfor i, (idx, row) in enumerate(model_errors.iterrows()):\n    ax5.text(row['error_rate'] * 100 + 1, i, \n            f\"{row['error_rate']*100:.1f}% ({int(row['errors'])}/{int(row['count'])})\",\n            va='center', fontsize=9, fontweight='bold')\n\nax5.set_yticks(range(len(model_errors)))\nax5.set_yticklabels(model_errors.index)\nax5.set_xlabel('Error Rate (%)', fontsize=12, fontweight='bold')\nax5.set_title('Error Rate by Generative Model', fontsize=13, fontweight='bold')\nax5.grid(True, alpha=0.3, axis='x')\n\n# 6. Quality Metrics for Misclassified vs Correct (Middle Right)\nax6 = fig.add_subplot(gs[1, 2])\n\nmetrics_to_plot = ['ssim', 'lpips_score', 'mse']\nmetric_labels = ['SSIM', 'LPIPS', 'MSE']\n\nmisclass_fake = fake_results[fake_results['correct'] == 0]\ncorrect_fake = fake_results[fake_results['correct'] == 1]\n\nx_pos = np.arange(len(metrics_to_plot))\nwidth = 0.35\n\n# Normalize metrics to 0-1 range for comparison\ndef normalize(series):\n    return (series - series.min()) / (series.max() - series.min())\n\nmisclass_means = [normalize(misclass_fake[m].dropna()).mean() for m in metrics_to_plot]\ncorrect_means = [normalize(correct_fake[m].dropna()).mean() for m in metrics_to_plot]\n\nbars1 = ax6.bar(x_pos - width/2, misclass_means, width, label='Misclassified', \n               color='#F44336', alpha=0.7, edgecolor='black')\nbars2 = ax6.bar(x_pos + width/2, correct_means, width, label='Correct', \n               color='#4CAF50', alpha=0.7, edgecolor='black')\n\n# Add values\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax6.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}',\n                ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nax6.set_ylabel('Normalized Value', fontsize=12, fontweight='bold')\nax6.set_title('Quality Metrics: Misclassified vs Correct', fontsize=13, fontweight='bold')\nax6.set_xticks(x_pos)\nax6.set_xticklabels(metric_labels)\nax6.legend(fontsize=10)\nax6.grid(True, alpha=0.3, axis='y')\n\n# 7. Top 10 Hardest Masks (Bottom Left)\nax7 = fig.add_subplot(gs[2, :2])\nmask_errors = fake_results.groupby('mask_name')['correct'].agg(['count', 'sum'])\nmask_errors = mask_errors[mask_errors['count'] >= 10]  # At least 10 samples\nmask_errors['errors'] = mask_errors['count'] - mask_errors['sum']\nmask_errors['error_rate'] = mask_errors['errors'] / mask_errors['count']\nhardest_masks = mask_errors.nlargest(10, 'error_rate')\n\ncolors_mask = ['#F44336' if rate > 0.5 else '#FF9800' if rate > 0.3 else '#FFC107' \n              for rate in hardest_masks['error_rate']]\n\nbars = ax7.barh(range(len(hardest_masks)), hardest_masks['error_rate'] * 100,\n               color=colors_mask, alpha=0.7, edgecolor='black')\n\nfor i, (idx, row) in enumerate(hardest_masks.iterrows()):\n    ax7.text(row['error_rate'] * 100 + 1, i, \n            f\"{row['error_rate']*100:.1f}% ({int(row['errors'])}/{int(row['count'])})\",\n            va='center', fontsize=9, fontweight='bold')\n\nax7.set_yticks(range(len(hardest_masks)))\nax7.set_yticklabels([idx[:30] for idx in hardest_masks.index], fontsize=9)\nax7.set_xlabel('Error Rate (%)', fontsize=12, fontweight='bold')\nax7.set_title('Top 10 Hardest Masks (Min 10 samples)', fontsize=13, fontweight='bold')\nax7.grid(True, alpha=0.3, axis='x')\n\n# 8. Error Summary Statistics (Bottom Right)\nax8 = fig.add_subplot(gs[2, 2])\nax8.axis('off')\n\ntotal_errors = (results_df['correct'] == 0).sum()\nreal_errors = ((results_df['true_label'] == 'real') & (results_df['correct'] == 0)).sum()\nfake_errors = ((results_df['true_label'] == 'fake') & (results_df['correct'] == 0)).sum()\n\nerror_stats = [\n    ['Error Type', 'Count', '%'],\n    ['', '', ''],\n    ['Total Errors', f'{total_errors:,}', f'{total_errors/len(results_df)*100:.2f}%'],\n    ['Real ‚Üí Fake', f'{real_errors:,}', f'{real_errors/len(results_df)*100:.2f}%'],\n    ['Fake ‚Üí Real', f'{fake_errors:,}', f'{fake_errors/len(results_df)*100:.2f}%'],\n    ['', '', ''],\n    ['Hardest Domain', hardest_masks.index[0][:15], f'{hardest_masks.iloc[0][\"error_rate\"]*100:.1f}%'],\n    ['Hardest Model', model_errors.index[0][:15], f'{model_errors.iloc[0][\"error_rate\"]*100:.1f}%'],\n    ['', '', ''],\n    ['Avg Confidence', '', ''],\n    ['  Correct', f'{results_df[correct_mask][\"confidence\"].mean():.4f}', ''],\n    ['  Incorrect', f'{results_df[incorrect_mask][\"confidence\"].mean():.4f}', ''],\n]\n\ntable = ax8.table(cellText=error_stats,\n                  cellLoc='left',\n                  loc='center',\n                  bbox=[0, 0, 1, 1])\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1, 2)\n\n# Style header\nfor i in range(3):\n    table[(0, i)].set_facecolor('#D32F2F')\n    table[(0, i)].set_text_props(weight='bold', color='white')\n\n# Highlight important rows\nfor i in [2, 3, 4]:\n    table[(i, 0)].set_facecolor('#FFEBEE')\n    table[(i, 0)].set_text_props(weight='bold')\n\nax8.set_title('Error Summary', fontsize=13, fontweight='bold', pad=20)\n\nplt.suptitle(f'Comprehensive Error Analysis - Version {version}', \n             fontsize=16, fontweight='bold', y=0.995)\n\n# Save\nerror_analysis_path = os.path.join(viz_dir, 'comprehensive_error_analysis.png')\nplt.savefig(error_analysis_path, dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"Comprehensive error analysis saved to: {error_analysis_path}\")\nprint(f\"\\n{'='*60}\")\nprint(\"ERROR ANALYSIS SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"Total Errors: {total_errors:,} ({total_errors/len(results_df)*100:.2f}%)\")\nprint(f\"  Real misclassified as Fake: {real_errors:,}\")\nprint(f\"  Fake misclassified as Real: {fake_errors:,}\")\nprint(f\"\\nHardest Domain: {domain_errors.index[0]} (Error Rate: {domain_errors.iloc[0]['error_rate']*100:.2f}%)\")\nprint(f\"Hardest Model: {model_errors.index[0]} (Error Rate: {model_errors.iloc[0]['error_rate']*100:.2f}%)\")\nprint(f\"{'='*60}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Domain/Mask/Model Accuracy Analysis\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DETAILED ACCURACY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter for fake images only\n",
    "fake_results = results_df[results_df['true_label'] == 'fake'].copy()\n",
    "\n",
    "# 1. Domain-wise accuracy\n",
    "print(\"\\n1. DOMAIN-WISE ACCURACY:\")\n",
    "print(\"-\" * 60)\n",
    "domain_accuracy = fake_results.groupby('domain')['correct'].agg([\n",
    "    ('total_images', 'count'),\n",
    "    ('correct_predictions', 'sum'),\n",
    "    ('accuracy', 'mean')\n",
    "]).sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(domain_accuracy)\n",
    "\n",
    "# Save to CSV\n",
    "domain_path = os.path.join(data_dir, 'domain_accuracy.csv')\n",
    "domain_accuracy.to_csv(domain_path)\n",
    "print(f\"\\nDomain accuracy saved to: {domain_path}\")\n",
    "\n",
    "# 2. Mask-wise accuracy\n",
    "print(\"\\n2. MASK-WISE ACCURACY (Top 20):\")\n",
    "print(\"-\" * 60)\n",
    "mask_accuracy = fake_results.groupby('mask_name')['correct'].agg([\n",
    "    ('total_images', 'count'),\n",
    "    ('correct_predictions', 'sum'),\n",
    "    ('accuracy', 'mean')\n",
    "]).sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(mask_accuracy.head(20))\n",
    "\n",
    "# Save to CSV\n",
    "mask_path = os.path.join(data_dir, 'mask_accuracy.csv')\n",
    "mask_accuracy.to_csv(mask_path)\n",
    "print(f\"\\nMask accuracy saved to: {mask_path}\")\n",
    "\n",
    "# 3. Generative model accuracy\n",
    "print(\"\\n3. GENERATIVE MODEL ACCURACY:\")\n",
    "print(\"-\" * 60)\n",
    "model_accuracy = fake_results.groupby('dataset')['correct'].agg([\n",
    "    ('total_images', 'count'),\n",
    "    ('correct_predictions', 'sum'),\n",
    "    ('accuracy', 'mean')\n",
    "]).sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(model_accuracy)\n",
    "\n",
    "# Save to CSV\n",
    "model_path = os.path.join(data_dir, 'generative_model_accuracy.csv')\n",
    "model_accuracy.to_csv(model_path)\n",
    "print(f\"\\nGenerative model accuracy saved to: {model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Standard Metrics (Confusion Matrix, Classification Report)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STANDARD METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Save confusion matrix as CSV\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=['True Real', 'True Fake'],\n",
    "    columns=['Pred Real', 'Pred Fake']\n",
    ")\n",
    "cm_path = os.path.join(data_dir, 'confusion_matrix_counts.csv')\n",
    "cm_df.to_csv(cm_path)\n",
    "print(f\"\\nConfusion matrix saved to: {cm_path}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"-\"*60)\n",
    "class_names = ['Real', 'Fake']\n",
    "report = classification_report(\n",
    "    all_labels, \n",
    "    all_predictions, \n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Save classification report as CSV\n",
    "report_dict = classification_report(\n",
    "    all_labels,\n",
    "    all_predictions,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_path = os.path.join(data_dir, 'classification_report.csv')\n",
    "report_df.to_csv(report_path)\n",
    "print(f\"\\nClassification report saved to: {report_path}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Confusion Matrix Plot\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues', \n",
    "    xticklabels=['Real', 'Fake'],\n",
    "    yticklabels=['Real', 'Fake'],\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    linecolor='gray'\n",
    ")\n",
    "\n",
    "plt.title('Confusion Matrix - Real vs Fake Detection', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add accuracy annotation\n",
    "accuracy_text = f'Overall Accuracy: {test_accuracy:.4f}'\n",
    "plt.text(1.0, -0.15, accuracy_text, ha='center', va='top', \n",
    "         fontsize=11, fontweight='bold', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "cm_plot_path = os.path.join(viz_dir, 'confusion_matrix.png')\n",
    "plt.savefig(cm_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion matrix plot saved to: {cm_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Domain & Mask Accuracy Bar Charts\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Domain accuracy\n",
    "domain_plot_data = domain_accuracy.sort_values('accuracy', ascending=True)\n",
    "axes[0].barh(range(len(domain_plot_data)), domain_plot_data['accuracy'], color='steelblue')\n",
    "axes[0].set_yticks(range(len(domain_plot_data)))\n",
    "axes[0].set_yticklabels(domain_plot_data.index)\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Detection Accuracy by Domain', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(domain_plot_data.iterrows()):\n",
    "    axes[0].text(row['accuracy'] + 0.01, i, f\"{row['accuracy']:.3f} ({int(row['total_images'])})\", \n",
    "                 va='center', fontsize=9)\n",
    "\n",
    "# Plot 2: Top 15 masks by accuracy\n",
    "mask_plot_data = mask_accuracy.nlargest(15, 'accuracy').sort_values('accuracy', ascending=True)\n",
    "axes[1].barh(range(len(mask_plot_data)), mask_plot_data['accuracy'], color='coral')\n",
    "axes[1].set_yticks(range(len(mask_plot_data)))\n",
    "axes[1].set_yticklabels(mask_plot_data.index, fontsize=9)\n",
    "axes[1].set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Detection Accuracy by Mask (Top 15)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(mask_plot_data.iterrows()):\n",
    "    axes[1].text(row['accuracy'] + 0.01, i, f\"{row['accuracy']:.3f}\", \n",
    "                 va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "domain_mask_path = os.path.join(viz_dir, 'domain_mask_accuracy.png')\n",
    "plt.savefig(domain_mask_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Domain & Mask accuracy charts saved to: {domain_mask_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Quality Metrics Scatter Plots\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Prepare data - bin quality metrics and calculate accuracy\n",
    "def bin_and_calculate_accuracy(df, metric_col, bins=10):\n",
    "    \"\"\"Bin a metric and calculate accuracy per bin.\"\"\"\n",
    "    df_clean = df.dropna(subset=[metric_col])\n",
    "    df_clean['bin'] = pd.cut(df_clean[metric_col], bins=bins)\n",
    "    \n",
    "    result = df_clean.groupby('bin').agg({\n",
    "        'correct': ['mean', 'count'],\n",
    "        metric_col: 'mean'\n",
    "    })\n",
    "    \n",
    "    result.columns = ['accuracy', 'count', 'metric_value']\n",
    "    return result\n",
    "\n",
    "# Plot 1: SSIM vs Accuracy\n",
    "ssim_data = bin_and_calculate_accuracy(fake_results, 'ssim', bins=10)\n",
    "axes[0].scatter(ssim_data['metric_value'], ssim_data['accuracy'], \n",
    "                s=ssim_data['count']*2, alpha=0.6, color='blue')\n",
    "axes[0].set_xlabel('SSIM (Structural Similarity)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Detection Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Detection Accuracy vs SSIM', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0.05, 0.95, 'Bubble size = sample count', \n",
    "             transform=axes[0].transAxes, fontsize=9, va='top')\n",
    "\n",
    "# Plot 2: LPIPS vs Accuracy\n",
    "lpips_data = bin_and_calculate_accuracy(fake_results, 'lpips_score', bins=10)\n",
    "axes[1].scatter(lpips_data['metric_value'], lpips_data['accuracy'], \n",
    "                s=lpips_data['count']*2, alpha=0.6, color='green')\n",
    "axes[1].set_xlabel('LPIPS (Perceptual Distance)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Detection Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Detection Accuracy vs LPIPS', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(0.05, 0.95, 'Bubble size = sample count', \n",
    "             transform=axes[1].transAxes, fontsize=9, va='top')\n",
    "\n",
    "# Plot 3: MSE vs Accuracy\n",
    "mse_data = bin_and_calculate_accuracy(fake_results, 'mse', bins=10)\n",
    "axes[2].scatter(mse_data['metric_value'], mse_data['accuracy'], \n",
    "                s=mse_data['count']*2, alpha=0.6, color='red')\n",
    "axes[2].set_xlabel('MSE (Mean Squared Error)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Detection Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Detection Accuracy vs MSE', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].text(0.05, 0.95, 'Bubble size = sample count', \n",
    "             transform=axes[2].transAxes, fontsize=9, va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "quality_path = os.path.join(viz_dir, 'quality_metrics_scatter.png')\n",
    "plt.savefig(quality_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Quality metrics scatter plots saved to: {quality_path}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- SSIM: Higher values = more similar to original (harder to detect?)\")\n",
    "print(\"- LPIPS: Lower values = more perceptually similar (harder to detect?)\")\n",
    "print(\"- MSE: Lower values = closer to original pixel values (harder to detect?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Confidence Histogram\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Split data by correctness\n",
    "correct_confidences = results_df[results_df['correct'] == 1]['confidence']\n",
    "incorrect_confidences = results_df[results_df['correct'] == 0]['confidence']\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(correct_confidences, bins=50, alpha=0.6, color='green', \n",
    "         label=f'Correct Predictions (n={len(correct_confidences)})', edgecolor='black')\n",
    "plt.hist(incorrect_confidences, bins=50, alpha=0.6, color='red', \n",
    "         label=f'Incorrect Predictions (n={len(incorrect_confidences)})', edgecolor='black')\n",
    "\n",
    "# Add mean lines\n",
    "mean_correct = correct_confidences.mean()\n",
    "mean_incorrect = incorrect_confidences.mean()\n",
    "\n",
    "plt.axvline(mean_correct, color='darkgreen', linestyle='--', linewidth=2,\n",
    "            label=f'Mean Correct: {mean_correct:.3f}')\n",
    "plt.axvline(mean_incorrect, color='darkred', linestyle='--', linewidth=2,\n",
    "            label=f'Mean Incorrect: {mean_incorrect:.3f}')\n",
    "\n",
    "plt.xlabel('Confidence Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "plt.title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "confidence_path = os.path.join(viz_dir, 'confidence_histogram.png')\n",
    "plt.savefig(confidence_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confidence histogram saved to: {confidence_path}\")\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"  Correct predictions - Mean: {mean_correct:.4f}, Std: {correct_confidences.std():.4f}\")\n",
    "print(f\"  Incorrect predictions - Mean: {mean_incorrect:.4f}, Std: {incorrect_confidences.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Generative Model Comparison Bar Chart\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Sort by accuracy\n",
    "model_plot_data = model_accuracy.sort_values('accuracy', ascending=True)\n",
    "\n",
    "# Create bar chart\n",
    "bars = plt.barh(range(len(model_plot_data)), model_plot_data['accuracy'], color='teal')\n",
    "\n",
    "# Customize axes\n",
    "plt.yticks(range(len(model_plot_data)), model_plot_data.index)\n",
    "plt.xlabel('Detection Accuracy', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Generative Model', fontsize=12, fontweight='bold')\n",
    "plt.title('Fake Detection Accuracy by Generative Model', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(model_plot_data.iterrows()):\n",
    "    plt.text(row['accuracy'] + 0.01, i, \n",
    "             f\"{row['accuracy']:.3f} (n={int(row['total_images'])})\", \n",
    "             va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xlim(0, 1.1)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "model_comparison_path = os.path.join(viz_dir, 'generative_model_comparison.png')\n",
    "plt.savefig(model_comparison_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generative model comparison saved to: {model_comparison_path}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher accuracy = fakes from this model are easier to detect\")\n",
    "print(\"- Lower accuracy = fakes from this model are more convincing/harder to detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Misclassified Examples Grid (4x4 Images)\n",
    "\n",
    "# Note: This requires loading actual images from disk\n",
    "# We'll create a grid showing misclassified fake images with their metadata\n",
    "\n",
    "if len(misclassified_fake) > 0:\n",
    "    # Select up to 16 random misclassified examples\n",
    "    num_examples = min(16, len(misclassified_fake))\n",
    "    sample_misclassified = misclassified_fake.sample(n=num_examples, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Get image paths from the dataset\n",
    "    # We need to reconstruct paths from perturbed_img_id\n",
    "    # This assumes we can access the CSV to get the image path\n",
    "    fake_test_df = pd.read_csv(FAKE_TEST_CSV)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    plotted = 0\n",
    "    for idx, (_, row) in enumerate(sample_misclassified.iterrows()):\n",
    "        if plotted >= 16:\n",
    "            break\n",
    "            \n",
    "        # Find the image path\n",
    "        img_row = fake_test_df[fake_test_df['perturbed_img_id'] == row['perturbed_img_id']]\n",
    "        \n",
    "        if len(img_row) > 0:\n",
    "            img_path = img_row.iloc[0]['fake_img_path']\n",
    "            \n",
    "            try:\n",
    "                # Load and display image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                axes[plotted].imshow(img)\n",
    "                axes[plotted].axis('off')\n",
    "                \n",
    "                # Create title with metadata\n",
    "                title = f\"{row['mask_name'][:15]}\\n\"\n",
    "                title += f\"Domain: {row['domain']}\\n\"\n",
    "                title += f\"Model: {row['dataset'][:20]}\\n\"\n",
    "                title += f\"Conf: {row['confidence']:.3f} | SSIM: {row['ssim']:.3f}\"\n",
    "                \n",
    "                axes[plotted].set_title(title, fontsize=8, color='red')\n",
    "                plotted += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(plotted, 16):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Misclassified Fake Images (Predicted as Real)', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    misclass_grid_path = os.path.join(viz_dir, 'misclassified_examples_grid.png')\n",
    "    plt.savefig(misclass_grid_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Misclassified examples grid saved to: {misclass_grid_path}\")\n",
    "    print(f\"Displayed {plotted} misclassified fake images\")\n",
    "else:\n",
    "    print(\"No misclassified fake images to display!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Final Summary Report\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \" * 25 + \"TRAINING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä MODEL INFORMATION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Architecture: ResNet50 (pretrained on ImageNet)\")\n",
    "print(f\"Training Device: {device}\")\n",
    "\n",
    "print(\"\\nüìÅ DATASET STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_dataset):,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è TRAINING CONFIGURATION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE}, factor={SCHEDULER_FACTOR})\")\n",
    "print(f\"Augmentation: {'Enabled' if USE_AUGMENTATION else 'Disabled'}\")\n",
    "\n",
    "print(\"\\nüèÜ TRAINING RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Best epoch: {best_epoch}/{NUM_EPOCHS}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Final training accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final training loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Total training time: {format_time(total_training_time)}\")\n",
    "\n",
    "print(\"\\nüéØ TEST PERFORMANCE\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Overall test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Total test samples: {len(all_labels):,}\")\n",
    "print(f\"Correct predictions: {(all_predictions == all_labels).sum():,}\")\n",
    "print(f\"Incorrect predictions: {(all_predictions != all_labels).sum():,}\")\n",
    "\n",
    "# Real vs Fake breakdown\n",
    "real_acc = results_df[results_df['true_label'] == 'real']['correct'].mean()\n",
    "fake_acc = results_df[results_df['true_label'] == 'fake']['correct'].mean()\n",
    "print(f\"\\nReal image detection accuracy: {real_acc:.4f}\")\n",
    "print(f\"Fake image detection accuracy: {fake_acc:.4f}\")\n",
    "\n",
    "print(\"\\nüîç ERROR ANALYSIS (FAKE IMAGES)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total fake images in test: {(results_df['true_label'] == 'fake').sum():,}\")\n",
    "print(f\"Correctly detected fakes: {((results_df['true_label'] == 'fake') & (results_df['correct'] == 1)).sum():,}\")\n",
    "print(f\"Misclassified fakes: {len(misclassified_fake):,}\")\n",
    "\n",
    "if len(domain_accuracy) > 0:\n",
    "    print(f\"\\nHardest domain: {domain_accuracy.idxmin()['accuracy']} (Acc: {domain_accuracy['accuracy'].min():.4f})\")\n",
    "    print(f\"Easiest domain: {domain_accuracy.idxmax()['accuracy']} (Acc: {domain_accuracy['accuracy'].max():.4f})\")\n",
    "\n",
    "if len(model_accuracy) > 0:\n",
    "    print(f\"\\nMost detectable generative model: {model_accuracy.idxmax()['accuracy']} (Acc: {model_accuracy['accuracy'].max():.4f})\")\n",
    "    print(f\"Least detectable generative model: {model_accuracy.idxmin()['accuracy']} (Acc: {model_accuracy['accuracy'].min():.4f})\")\n",
    "\n",
    "print(\"\\nüíæ OUTPUT FILES\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(f\"\\nModels ({len([f for f in os.listdir(models_dir) if f.endswith('.pth')])} files):\")\n",
    "print(f\"  - {models_dir}\")\n",
    "print(f\"\\nData CSVs:\")\n",
    "print(f\"  - {data_dir}\")\n",
    "for csv_file in sorted(os.listdir(data_dir)):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        print(f\"    ‚Ä¢ {csv_file}\")\n",
    "\n",
    "print(f\"\\nVisualizations:\")\n",
    "print(f\"  - {viz_dir}\")\n",
    "for img_file in sorted(os.listdir(viz_dir)):\n",
    "    if img_file.endswith('.png'):\n",
    "        print(f\"    ‚Ä¢ {img_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \" * 30 + \"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}